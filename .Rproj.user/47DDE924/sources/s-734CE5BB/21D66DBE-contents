library(ISLR)

data("Default")
head(Default)

class(Default)
summary(Default)
# estamos en presencia de una muestra bien desbalanceada!

library(ggplot2)
library(gridExtra)

## Scatter plot con densidades ----
plot.2d <- ggplot(Default, aes(x = balance, y = income, group = default)) +
  geom_point(aes(shape = default, color = default), alpha = 0.5) +
  theme_light()

# histos.balance <- ggplot(Default, aes(x = balance, group = default)) +
#   geom_histogram(bins = round((sqrt(333) + sqrt(9000))/2), aes(color = default, fill = default), alpha = 0.2) +
#   theme_light()
# histos.balance
#
# histos.income <- ggplot(Default, aes(x = income, group = default)) +
#   geom_histogram(bins = round((sqrt(333) + sqrt(9000))/2), aes(color = default, fill = default), alpha = 0.2) +
#   theme_light()
# histos.income

# Empty plot
empty <- ggplot()+geom_point(aes(1,1), color="white") +
  theme(
    plot.background = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank()
  )
# arriba
dens.balance <- ggplot(Default, aes(x = balance, group = default)) +
  geom_density(aes(color = default, fill = default), alpha = 0.2) +
  theme_light()+
  theme(legend.position = "none")
# derecha
dens.income <- ggplot(Default, aes(x = income, group = default)) +
  geom_density(aes(color = default, fill = default), alpha = 0.2) +
  theme_light() + coord_flip() +
  theme(legend.position = "none")

grid.arrange(dens.balance, empty, plot.2d, dens.income, ncol=2, nrow=2, widths=c(4, 1), heights=c(1, 4))

## *** LDA -----
library(caret)
df <- Default[, c("income", "balance", "default")]
set.seed(123)
train.ID <- createDataPartition(df$default, p = 0.8, list = FALSE)

train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]

# damos nuestros primeros pasos en la validacion cruzada...
# (podemos probar varias opciones)
# en este caso no hay parametros para tunear
fit_control <- trainControl(method='cv', number = 10)

model_lda_def <- train(default ~.,
                       data = train_df,
                       method = "lda",
                       trControl = fit_control)
model_lda_def
model_lda_def$finalModel

varImp(model_lda_def)

# hagamos las predicciones del conjunto de prueba
prediction_lda_def <- predict(model_lda_def, newdata = test_df)
confusionMatrix(prediction_lda_def, reference = test_df$default)

# extraemos el Accuracy o Precision
confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.lda <- 1-confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1]
names(tasa.error.lda) <- "Error LDA"
tasa.error.lda

# Pintar la frontera de decision ----
# Paso 1: crear un grid de valores desde min a max de ambos predictores
pl = seq(min(train_df$balance), max(train_df$balance), by=20)
pw = seq(min(train_df$income), max(train_df$income), by=800)

lgrid <- expand.grid(balance=pl, income=pw)

# Paso 2: obtener las predicciones tanto para el grid como para el test
ldaPredGrid <- predict(model_lda_def, newdata=lgrid)
train_df$Pred.Class <- predict(model_lda_def, newdata = train_df)

# Paso 3: ggplot con la funcion contour
frontera.LDA <- ggplot(data=lgrid) +
  stat_contour(aes(x=balance, y=income, z=as.numeric(ldaPredGrid)), bins=2) +
  geom_point(aes(x=balance, y=income, colour=ldaPredGrid),alpha=0.1) +
  labs(colour = "Fallo en pago") +
  geom_point(data=train_df,
             aes(x=balance, y=income,
                 colour=Pred.Class), size=5, shape=1) +
  theme_light()
frontera.LDA

## Curva ROC - LDA ----
# hagamos las predicciones del conjunto de prueba
pred_prob <- predict(model_lda_def, newdata = test_df, type = "prob")

library(ROCR)
library(dplyr)

prob.pred <- prediction(pred_prob[,2], test_df$default)

# sin mucho estilo:
# ROC
prob.pred %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

prob.pred %>%
  performance("err") %>%
  plot()

prob.pred %>%
  performance("fnr") %>%
  plot()

prob.pred %>%
  performance("fpr") %>%
  plot()

# AUC: mientras mas cercano a 1, mejor predicciones
auc.lda <- performance(prob.pred, measure = "auc")@y.values[[1]]

# Podemos combinar las 3 ultimas en un mismo grafico
df_perfor <- data.frame(Error.Rate = performance(prob.pred, "err")@y.values[[1]],
                        FNR = performance(prob.pred, "fnr")@y.values[[1]],
                        FPR = performance(prob.pred, "fpr")@y.values[[1]],
                        TPR = performance(prob.pred, "tpr")@y.values[[1]],
                        CutOffs = performance(prob.pred, "err")@x.values[[1]])

# plot tasas de error
errores.lda <- ggplot(df_perfor, aes(x = CutOffs)) +
  geom_line(aes(y = Error.Rate, colour = "Tasa Error General")) +
  geom_line(aes(y = FNR, colour = "FNR")) +
  geom_line(aes(y = FPR, colour = "FPR")) +
  scale_colour_discrete(name = "Medidas" ) +
  xlab("Puntos de corte") + ylab("Tasas de Error") +
  theme_light()
errores.lda

# plot de la curva ROC
roc.lda <- ggplot(df_perfor, aes(x = FPR, y = TPR)) +
  geom_line() +
  xlab("FPR: 1- especificidad") + ylab("TPR: sensibilidad") +
  ggtitle(paste0("Curva ROC - LDA (Area Under Curve = ", round(auc.lda, digits = 3),")")) +
  theme_light()
roc.lda

## *** QDA ----

train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]

model_qda_def <- train(default ~.,
                       data = train_df,
                       method = "qda",
                       trControl = fit_control)
model_qda_def
model_qda_def$finalModel

# hagamos las predicciones del conjunto de prueba
prediction_qda_def <- predict(model_qda_def, newdata = test_df)
confusionMatrix(prediction_qda_def, reference = test_df$default)

# extraemos el Accuracy o Precision
confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.qda <- 1-confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
names(tasa.error.qda) <- "Error QDA"
tasa.error.qda

## Pintar frontera QDA ----
# Paso 2: obtener las predicciones tanto para el grid como para el test
qdaPredGrid <- predict(model_qda_def, newdata=lgrid)
train_df$Pred.Class.QDA <- predict(model_qda_def, newdata = train_df)

# Paso 3: ggplot con la funcion contour
frontera.QDA <- ggplot(data=lgrid) +
  stat_contour(aes(x=balance, y=income, z=as.numeric(qdaPredGrid)), bins=2) +
  geom_point(aes(x=balance, y=income, colour=qdaPredGrid),alpha=0.1) +
  labs(colour = "Fallo en pago") +
  geom_point(data=train_df,
             aes(x=balance, y=income,
                 colour=Pred.Class.QDA), size=5, shape=1) +
  theme_light()
frontera.QDA

grid.arrange(frontera.LDA, frontera.QDA, ncol=1, nrow=2)


## Curva ROC - QDA ----
# hagamos las predicciones del conjunto de prueba
pred_prob <- predict(model_qda_def, newdata = test_df, type = "prob")

prob.pred <- prediction(pred_prob[,2], test_df$default)

# AUC: mientras mas cercano a 1, mejor predicciones
auc.qda <- performance(prob.pred, measure = "auc")@y.values[[1]]

# Podemos combinar las 3 ultimas en un mismo grafico
df_perfor <- data.frame(Error.Rate = performance(prob.pred, "err")@y.values[[1]],
                        FNR = performance(prob.pred, "fnr")@y.values[[1]],
                        FPR = performance(prob.pred, "fpr")@y.values[[1]],
                        TPR = performance(prob.pred, "tpr")@y.values[[1]],
                        CutOffs = performance(prob.pred, "err")@x.values[[1]])

# plot tasas de error
errores.lda <- ggplot(df_perfor, aes(x = CutOffs)) +
  geom_line(aes(y = Error.Rate, colour = "Tasa Error General")) +
  geom_line(aes(y = FNR, colour = "FNR")) +
  geom_line(aes(y = FPR, colour = "FPR")) +
  scale_colour_discrete(name = "Medidas" ) +
  xlab("Puntos de corte") + ylab("Tasas de Error") +
  theme_light()
errores.lda

# plot de la curva ROC
roc.lda <- ggplot(df_perfor, aes(x = FPR, y = TPR)) +
  geom_line() +
  xlab("FPR: 1- especificidad") + ylab("TPR: sensibilidad") +
  ggtitle(paste0("Curva ROC - LDA (Area Under Curve = ", round(auc.qda, digits = 3),")")) +
  theme_light()
roc.lda

## *** RDA ----
# To use random search, another option is available in trainControl called search. Possible values of this argument are "grid"
# and "random". The built-in models contained in caret contain code to generate random tuning parameter combinations. The total
# number of unique combinations is specified by the tuneLength option to train.


train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]

# By default, if TuneLength = p is the number of tuning parameters, the grid size is 3^p. As another example, regularized
# discriminant analysis (RDA) models have two parameters (gamma and lambda), both of which lie between zero and one. The
# default training grid would produce nine combinations in this two-dimensional space.

fitControl <- trainControl(method = "cv",
                           number = 10)

model_rda_def <- train(default ~.,
                       data = train_df,
                       method = "rda",
                       tuneLength = 2,
                       trControl = fitControl)
model_rda_def
model_rda_def$finalModel

## en este caso el ggplot nos da información sobre los hiperparametros y su correspondiente accuracy
ggplot(model_rda_def) + theme_light()

# El grid se puede definir tambien "a mano"
lambda <- c(0.1, 0.2, 0.3, 0.8, 0.9)
gamma = c(0)
mi.grid <- data.frame(lambda , gamma)

fitControl <- trainControl(method = "cv",
                           number = 10)

model_rda_def <- train(default ~.,
                       data = train_df,
                       method = "rda",
                       tuneGrid = mi.grid,
                       trControl = fitControl)
model_rda_def
model_rda_def$finalModel

## en este caso el ggplot nos da información sobre los hiperparametros y su correspondiente accuracy
ggplot(model_rda_def) + theme_light()

# hagamos las predicciones del conjunto de prueba
prediction_rda_def <- predict(model_rda_def, newdata = test_df)
confusionMatrix(prediction_rda_def, reference = test_df$default)

# extraemos el Accuracy o Precision
confusionMatrix(prediction_rda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.rda <- 1-confusionMatrix(prediction_rda_def, reference = test_df$default)$overall[1]
names(tasa.error.rda) <- "Error RDA"
tasa.error.rda

## *** KNN comparacion ----
# Este va a demorar bastante, porque el n es mayor!
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]

#opcion 1
fit_control1 <- trainControl(method='cv', number = 10)
#opcion 2: usando la curva ROC, en lugar del Accuracy
fit_control2 <- trainControl(method='cv', number = 10,
                             classProbs=TRUE, summaryFunction = twoClassSummary)

model_knn_def <- train(default ~.,
                       data = train_df,
                       method = "knn",
                       trControl = fit_control1,
                       preProcess = c("center", "scale"),
                       tuneLength = 40)

model_knn_def <- train(default ~.,
                        data = train_df,
                        method = "knn",
                        trControl = fit_control2,
                        metric = "ROC",
                        preProcess = c("center", "scale"),
                        tuneLength = 40)
model_knn_def

plot(model_knn_def)

varImp(model_knn_def)

# hagamos las predicciones del conjunto de prueba
prediction_knn_def <- predict(model_knn_def, newdata = test_df)
confusionMatrix(prediction_knn_def, reference = test_df$default)
# la tasa de error
tasa.error.knn <- 1-confusionMatrix(prediction_knn_def, reference = test_df$default)$overall[1]
names(tasa.error.knn) <- "Error KNN"
tasa.error.knn

## Comparar modelos según su performance! ----

resamps <- resamples(list(LDA = model_lda_def,
                          QDA = model_qda_def,
                          RDA = model_rda_def,
                          KNN = model_knn_def))
resamps

summary(resamps)

# box plots
bwplot(resamps, layout = c(2, 1))
