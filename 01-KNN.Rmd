# K - Vecinos más Próximos

Comenzamos con uno de los algoritmos más sencillos e intuitivos para regresión y clasificación: los *K - Vecinos Más Próximos* (*KNN: K-Nearest Neighbors*). Nuestro bautizo será con un problema de clasificación, empleando los paquetes `class` y `caret`. Luego, pasaremos a un problema sencillo de regresión, esta vez usando solo el paquete `caret`. Trataremos los problemas de *ajuste y validación del modelo*, usando técnicas de *remuestreo*.

## Clasificación con el paquete `class`

El problema inicial está relacionado con la clasificación de la especie de flor Iris---*setosa*, *virginica* y *versicolor*---a partir de mediciones sus pétalos y sépalos. Estos datos fueron recogidos por Ronald Fisher con el objetivo de cuantificar la variación morfológica de la flor. Actualmente están disponibles en diversas plataformas. En R es uno de los datos que vienen de base (`iris`).

En la Tabla \@ref(tab:iris-tab) representamos una muestra del dataset, que en su totalidad consiste de 50 observaciones de cada una de las 3 especies. Como todo estudio, debemos comenzar por un análisis descriptivo de la muestra.

```{r iris-tab, tidy=FALSE, echo=FALSE}
knitr::kable(
  head(iris, 10), caption = 'Estructura del dataset Iris',
  booktabs = TRUE
)
```

Cargamos las librerías necesarias para llevar a cabo el estudio. Luego inspeccionamos los datos.

```{r, out.width='100%', fig.asp=.75, fig.align='center', message=FALSE}
# ya conocemos esta:
library(tidyverse)
# para usar knn:
library(class)
# esta es nueva para nosotros:
library(GGally)

df <- data(iris) # cargar datos
summary(iris) # un breve descriptivo

# ver el balance de la muestra, según las clases
prop.table(table(iris$Species))

# visualización
p1 <- ggpairs(iris, 
        aes(colour = Species, alpha = 0.2), 
        columns = c("Sepal.Length",  "Sepal.Width", 
                    "Petal.Length", "Petal.Width")) + 
  theme_bw()
p1

p2 <- ggpairs(iris, 
        aes(colour = Species, alpha = 0.2), 
        lower=list(combo=wrap("facethist",
                              bins=round(sqrt(50))))) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
p2
```

De estos análisis observamos, por ejemplo, que 2 de las 4 variables---*Petal.Width* y *Petal.Length*---parecen separar bastante bien las 3 especies, y que la muestra está muy bien balanceada.

Antes de pasar a ajustar nuestro modelo, debemos *preprocesar* la muestra. El algoritmo *KNN* es muy sensible a la escala de los datos, por ejemplo, podría favorecer distancias entre elementos con valores más grandes. Una forma sencilla de *estandarizar* o *escalar* los datos es usando:

```{r}
iris.scl <- scale(iris[,1:4])
```

Otro elemento importante es la validación del modelo que ajustemos: ¿cómo y con qué muestra medir la precisión? Por lo pronto, fijaremos aleatoriamente un 20% de los datos para calcular la *tasa de error*.

```{r}
# set de índices para entrenar-validar (80% - 20%)
set.seed(123)
train.ID <- sample(1:nrow(iris), 0.8 * nrow(iris)) 

# matriz de diseño para entrenar
X.train <- iris.scl[train.ID,1:4]
# matriz de diseño para testear
X.test <- iris.scl[-train.ID,1:4]
# respuesta (categórica) entrenamiento
Y <- iris[train.ID,5]
# respuesta (categórica) test
Y.test <- iris[-train.ID,5]
```

Usando la función `knn` podemos predecir las clases de los datos en `X.test`. Otro problema es cómo seleccionar la cantidad de vecinos `k` apropiada. Una *regla de pulgar* (*thumb rule*) es fijar $k = \sqrt{n_{train}}$. Para analizar la precisión del modelo creamos la *matriz de confusión* y calculamos la tasa de error correspondiente para el conjunto de datos test.

```{r}
# KNN 
pr <- knn(X.train, X.test, cl=Y, k = round(sqrt(nrow(X.train))))

# matriz de confusión
tab <- table(pr,Y.test)

```

```{r confIris-tab, tidy=FALSE, echo=FALSE}
knitr::kable(
  tab, caption = 'Matriz de Confusión - KNN Iris',
  booktabs = TRUE)

```

```{r}
# tasa de error test
test.error <- sum(pr != Y.test)/sum(tab)
test.error
```

La tasa de error test es bastante baja, además indica que se clasifican bien el $\approx 97\%$ de las observaciones. Veamos ahora qué pasa al variar el número de vecinos `K`.

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
test.error <- data.frame()
for (K in seq(1, 120, by = 5)) {
  # KNN
  pr <- knn(X.train,X.test,cl=Y,k=K)
  # matriz de confusion
  tab <- table(pr,Y.test)
  # tasa de error test
  test.error <- rbind(test.error, 
                        data.frame(Tasa.Error = sum(pr != Y.test)/sum(tab), K))
}

ggplot(test.error, aes(x = K, y = Tasa.Error)) + 
  geom_point() + 
  geom_line() + 
  ylab("Tasa de Error (test)") +  xlab("K: número de vecinos") + 
  theme_light()
```

Se representa la tasa de error al aumentar el número de vecinos. Los *saltos* de la curva son resultado del pequeño tamaño de la muestra test. Como cualquier otro modelo de machine learning, el interés está en seleccionar el nivel de flexibilidad (número de vecinos) que mejore la clasificación... inténtalo!


## El paquete `caret`

El paquete `caret` (_**C**lassification **A**nd **RE**gression **T**raining_) es uno de los más populares para entrenar modelos de *machine learning*. Contiene una interfaz uniforme para la mayoría de los algoritmos que se tratan en este curso y, en particular, los 3 que veremos en estas sesiones. Las ventajas del paquete son que permite hacer:

* partición de los datos
* pre-procesado de los datos
* selección de variables
* ajuste del modelo usando remuestreo
* estimación de la importancia/relevancia de las variables

Más información disponible en [topepo.github.io/caret](http://topepo.github.io/caret/index.html).

### Visualización

Seguiremos con los datos `iris`. El paso inicial: análisis descriptivo y visualización de los datos podríamos obviarlo... pero a modo didáctico reproducimos el mismo análisis, esta vez usando la función `featurePlot` de `caret`.

```{r}
library(caret)
str(iris)
```

Diagramas de dispersión:

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "pairs",
            ## Add a key at the top
            auto.key = list(columns = 3))
```

Densidades estimadas:

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
featurePlot(x = iris[, 1:4], 
            y = iris$Species,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|", 
            layout = c(4, 1), 
            auto.key = list(columns = 3))
```

Diagramas de cajas:

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
featurePlot(x = iris[, 1:4], 
            y = iris$Species, 
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(4,1 ), 
            auto.key = list(columns = 2))
```

### Clasificación con KNN

Necesitamos extraer una muestra independiente (*test*) para probar el modelo, una vez ajustado. Ahora usaremos la función `createDataPartition`, que permite hacer la partición teniendo en cuenta la variable respuesta. Esto es esencial para mantener el balance de la muestra.

```{r}
# creamos una partición test
df <- iris
set.seed(123)
train.ID <- createDataPartition(df$Species, p = 0.8, list = FALSE)

train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
```

Para ajustar el modelo usaremos la función `train`, que permite: 

* evaluar, usando remuestreo, el efecto de distintos parámetros en la precisión del modelo;
* escoger el modelo óptimo, de acuerdo a los parámetros probados; 
* estimar la precisión del modelo, de acuerdo a diferentes medidas.

Actualmente hay unos $\approx 238$ modelos disponibles. Nosotros empezaremos probando el `knn`, pero antes tenemos que especificar el método de remuestreo, usando la función `trainControl`. Con esta función, podemos fijar una validación cruzada *k-Fold* o *leave-one-out (LOOCV)*. También están disponibles las opciones *bootstrap* y *k-Fold repetitivo*. 

En este ejemplo, hemos fijado un *k-Fold* con 10 hojas. Además, hacemos el escalado de las variables dentro del propio algoritmo, usando la opción `preProcess`. Finalmente, le decimos al algoritmo que intente 10 valores diferentes para escoger el número de vecinos óptimo, usando la opción `tuneLength`

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
# primeros pasos con la validación cruzada...
fit_control <- trainControl(method='cv', number = 10)  

model_knn_iris <- train(Species ~., 
                       data = train_df, 
                       method = "knn", 
                       trControl = fit_control, 
                       preProcess = c("center", "scale"),  
                       tuneLength = 10)
model_knn_iris

plot(model_knn_iris)
```

Podemos ver en el resumen el número óptimo de vecinos (entre los valores probados) del modelo final. En el gráfico, vemos cómo varía el *accuracy* en función del número de vecinos. La tabla de confusión y medidas de precisión para los datos test:

```{r}
# hagamos las predicciones del conjunto de prueba
prediction_knn_iris <- predict(model_knn_iris, newdata = test_df)
confusionMatrix(prediction_knn_iris, reference = test_df$Species)
```

Intentemos ahora fijar las cantidades de vecinos a probar. También cambiamos el método de remuestreo...

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
# definimos el grid:
some_k <- expand.grid(k = 1:15) 

# k-fold CV pero con repeticiones
fit_control1 <- trainControl(
  method = "repeatedcv", 
  number = 10, # número de folds
  repeats = 5 ) # repeticiones

# bootstrap
fit_control2 <- trainControl(
  method = "boot",  
  number = 10) # número de muestras bootstrap

# LOOCV
fit_control3 <- trainControl(
  method = "LOOCV") 

model2_knn_iris <- train(Species ~., 
                        data = train_df, 
                        method = "knn", 
                        trControl = fit_control2, 
                        preProcess = c("center", "scale"),  
                        tuneGrid = some_k)
model2_knn_iris

plot(model2_knn_iris)

```

```{r}
# hagamos las predicciones del conjunto de prueba
prediction_knn_iris2 <- predict(model2_knn_iris, newdata = test_df)
confusionMatrix(prediction_knn_iris2, reference = test_df$Species)
```

### Importancia de las variables

Para KNN no tenemos un método que permita determinar la relevancia de cada predictor. Por ejemplo, en mínimos cuadrados, sí se puede conducir un test para determinar si cada coeficiente $\beta_i$ del modelo es significativamente distinto de cero. Aún así, `caret` incorpora la función `varImp` que da una medida de *importancia* de cada predictor del problema de clasificación o regresión.

```{r}
varImp(model_knn_iris)
```

Aunque esto no debe usarse como método de selección de variables, sí motiva el estudio del problema al disminuir la dimensión $p = 4$. Por ejemplo, veamos la precisión del modelo al dejar solo `Petal.Length` y `Petal.Width`.

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
# seleccionamos los predictores que queremos y la respuesta
df_petal <- iris[,c("Petal.Length", "Petal.Width", "Species")]
train_df_petal <- df_petal[train.ID, ]
test_df_petal <- df_petal[-train.ID, ]

# el modelo...
fit_control <- trainControl(method='cv', number = 10)  

model_knn_petal <- train(Species ~., 
                        data = train_df_petal, 
                        method = "knn", 
                        trControl = fit_control, 
                        preProcess = c("center", "scale"),  
                        tuneLength = 20)
model_knn_petal

plot(model_knn_petal)

```

```{r}
# hagamos las predicciones del conjunto de prueba
prediction_knn_petal <- predict(model_knn_petal, newdata = test_df_petal)
confusionMatrix(prediction_knn_petal, reference = test_df_petal$Species)
```

Pero, ¿cómo visualizar las fronteras de decisión del método? Ahora que $p = 2$, podemos representar esto en el plano usando la siguiente función:
```{r}
decision_bound = function(train_df_in, test_df_in, model_in){
  # plot decision boundary  for iris[,c("Petal.Length", "Petal.Width", "Species")]

  require(MASS)
  require(caret)
  require(ggplot2)
  require(gridExtra)

  # Paso 1: crear un grid de valores desde min a max de ambos predictores
  pl = seq(min(train_df_in$Petal.Length), max(train_df_in$Petal.Length), length.out = 80)
  pw = seq(min(train_df_in$Petal.Width), max(train_df_in$Petal.Width), length.out = 80)

  lgrid <- expand.grid(Petal.Length=pl, Petal.Width=pw)

  # Paso 2: obtener las predicciones tanto para el grid como para el test
  modelPredGrid <- predict(model_in, newdata=lgrid)
  train_df_in$Pred.Class <- predict(model_in, newdata = train_df_in)
  test_df_in$Pred.Class <- predict(model_in, newdata = test_df_in)

  # Paso 3: ggplot con la funcion contour
  gg1 <- ggplot(data=lgrid) +
    stat_contour(aes(x=Petal.Length, y=Petal.Width, z=as.numeric(modelPredGrid)), bins=2) +
    geom_point(aes(x=Petal.Length, y=Petal.Width, colour=modelPredGrid), alpha=0.1) +
    labs(colour = "Clases") + ggtitle("Train") +
    geom_point(data=train_df_in,
               aes(x=Petal.Length, y=Petal.Width,
                   colour=Species), size=5, shape=1) +
    theme_light()

  gg2 <- ggplot(data=lgrid) +
    stat_contour(aes(x=Petal.Length, y=Petal.Width, z=as.numeric(modelPredGrid)), bins=2) +
    geom_point(aes(x=Petal.Length, y=Petal.Width, colour=modelPredGrid), alpha=0.1) +
    labs(colour = "Clases") + ggtitle("Test") +
    geom_point(data=test_df_in,
               aes(x=Petal.Length, y=Petal.Width,
                   colour=Species), size=5, shape=1) +
    theme_light()
  grid.arrange(gg1, gg2, ncol=1, nrow=2)
}

```

Así que aplicando esto a nuestros datos de entrenamiento (o los del test) obtenemos las fronteras de decisión:

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
# fronteras de decisión, usando la nueva función
decision_bound(train_df_petal, test_df_petal, model_knn_petal)

```



## Regresión

Abordamos ahora el problema de regresión con KNN, o sea, la respuesta es cuantitativa-continua. Seguimos usando el paquete `caret` que tiene implementado el algoritmo y ofrece facilidades para el preprocesado de los datos y la validación del modelo. 

Particularmente, atacaremos el problema de predecir el precio medio de las viviendas (`medv`) en los suburbios de Boston, usando otras 13 variables predictoras. 

```{r}
library(MASS)
library(caret)
library(ggplot2)

# cargar e inspeccionar los datos
# para detalles sobre las variables predictoras:
# ?Boston
data(Boston)
str(Boston)
summary(Boston)
```

Veamos las relaciones entre predictores y la variable respuesta (en este ejemplo, solo hemos representado algunas).

```{r, message=F,  out.width='80%', fig.asp=.75, fig.align='center'}
# ver correlaciones y posibles relaciones:

# todos los predictores:
# ggpairs(Boston, ggplot2::aes(y = medv, alpha = 0.2)) + theme_light()
# algunos predictores:
ggpairs(Boston[, c("lstat", "age", "rad", "rm", "ptratio", "medv")]) + theme_light()

```

Por ejemplo, es notable la relación lineal entre `medv` y las variables predictoras `rm` y `lstat`. Estas dos corresponden al número medio de habitaciones por vivienda y al ínfimo estatus poblacional, respectivamente.

Ajustemos un modelo de regresión, usando todas las variables y el algoritmo KNN.

```{r,  out.width='80%', fig.asp=.75, fig.align='center'}
# Split the data into training and test set
set.seed(123)
train.ID <- createDataPartition(Boston$medv, p = 0.8, list = FALSE)
train.data  <- Boston[train.ID, ]
test.data <- Boston[-train.ID, ]

# Fit the model on the training set
set.seed(123)
knn_reg_model <- train(
  medv~.,
  data = train.data,
  method = "knn",
  trControl = trainControl("cv", number = 10),
  preProcess = c("center","scale"),
  tuneLength = 20
)

knn_reg_model
# Plot model error RMSE vs different values of k
plot(knn_reg_model)
```

Ahora, lo que nos interesa es disminuir el Error Cuadrático Medio:
```{r}
# predicciones
predictions <- predict(knn_reg_model, test.data)
# RMSE: raíz del error cuadrático medio
RMSE(predictions, test.data$medv)
# MAE: error absoluto medio
MAE(predictions, test.data$medv)
```

Si representamos las predicciones y los valores reales de la variable `medv`, esperamos que los puntos estén muy cercanos a la recta $Y = X$.

```{r , out.width='80%', fig.asp=.75, fig.align='center'}
df_plot <- data.frame(pred = predictions, real = test.data$medv)
ggplot(df_plot, aes(x = pred, y = real)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  xlab(expression(hat( y))) + ylab("y") +
  theme_light()
```


¿Será posible mejorar esto? ¿Son todas las variables realmente necesarias? ¿Un grid con valores más pequeños a $K = 5$ podría resultar mejor? Veamos qué tal es el ajuste y las predicciones si nos limitamos a unas pocas variables predictoras.

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
# importancia de las variables según impacto en la predicción
varImp(knn_reg_model)

# seleccionemos solo algunas variables:
boston <- Boston[, c("lstat", "rm", "ptratio", "medv")]
```

```{r, out.width='80%', fig.asp=.75, fig.align='center'}
# ajustamos el modelo en el nuevo diseño
train.data  <- boston[train.ID, ]
test.data <- boston[-train.ID, ]

set.seed(123)
knn_reg_model <- train(
  medv~.,
  data = train.data,
  method = "knn",
  trControl = trainControl("cv", number = 10),
  preProcess = c("center","scale"),
  tuneGrid = expand.grid(k = 1:15)
)

# Veamos si el modelo ha mejorado algo:

# predicciones
predictions <- predict(knn_reg_model, test.data)
# RMSE: raíz del error cuadrático medio
RMSE(predictions, test.data$medv)
# MAE: error absoluto medio
MAE(predictions, test.data$medv)
```
```{r , out.width='80%', fig.asp=.75, fig.align='center'}
df_plot <- data.frame(pred = predictions, real = test.data$medv)
ggplot(df_plot, aes(x = pred, y = real)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  xlab(expression(hat( y))) + ylab("y") +
  theme_light()
```

## Weighted KNN

El método de K-Vecinos Más Próximos Ponderados (WKNN: *Weighted K-Nearest Neighbors*) es una variante del KNN. El principio básico es el mismo: predecir una respuesta en función de los puntos más cercanos de la muestra. La diferencia es que WKNN da más importancia a los más próximos, dentro de los K prefijados. Esto se logra ponderando o dando pesos a los vecinos.

En `caret` podemos fijar el método `kknn` que implementa WKNN, tanto para regresión como para clasificación. Ahora debemos optimizar 3 hiperparámetros:

```{r}
getModelInfo("kknn")$kknn$parameters
```
El número de vecinos `K` se corresponde al campo`kmax`. El campo `distance` se refiere al order del parámetro $p$ en la *Distancia de Minkowski*. El `kernel` es la transformación de los ejes de coordenadas, las opciones son:

```{r, eval=F}
kerns <- c("rectangular", "triangular", "epanechnikov", "biweight", "triweight", 
                                 "cos", "inv", "gaussian")
```

Veamos un ejemplo con los datos `iris`. Empezamos fijando un grid o malla de posibles valores de los hiperparámetros a optimizar:
```{r}
# muestra, por eso es necesario una particion balanceada con createDataPartition
df <- iris
set.seed(123)
train.ID <- createDataPartition(df$Species, p = 0.8, list = FALSE)

train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]

# hacemos una validación cruzada con 10-folds 10 veces
fit_control <- trainControl(method='repeatedcv', number = 10, repeats = 10)

# fijamos el grid de valores de los hiperparámetros:
buscar_mejor <- expand.grid( kmax =  3:9,
                             distance = 1:2,
                             kernel = c("rectangular", #standard knn
                                        "triangular",
                                        "gaussian"))
```

El modelo se ajusta igual a como ya hemos estudiado:
```{r}
set.seed(321)
model.w.knn <- train(Species ~.,
                     data = train_df,
                     method = "kknn",
                     trControl = fit_control,
                     preProcess = c("center", "scale"),
                     tuneGrid = buscar_mejor)
model.w.knn

model.w.knn$finalModel

```
```{r, out.width='80%', fig.asp=.75, fig.align='center'}
plot(model.w.knn)
```
Observamos que el modelo final---el mejor de acuerdo al `Accuracy`---es aquel con 8 vecinos, donde el parámetro $p=2$ en la Distancia de Minkowski (equivalente a la Distancia Euclídea) y el kernel es triangular. Esto se obtuvo al probar 7 valores de `kmax` $\times$ 2 valores de `distance` $\times$ 3 posibles `kernel`.  

Por otro lado, en lugar de escribir explícitamente el grid de valores a probar, en `caret` tenemos la opción de realizar una búsqueda aleatoria. Esto podría ser un primer paso para detectar rangos de valores de los hiperparámetros donde luego afinar la búsqueda. Como ejemplo, lo haremos para solo 8 combinaciones de posibles hiperparámetros (en la práctica debemos fijar un mayor número de combinaciones, lo que conlleva un mayor coste computacional):

```{r , out.width='80%', fig.asp=.75, fig.align='center'}
# random search WKNN
fit_control <- trainControl(method='repeatedcv', number = 10, 
                            repeats = 10,
                            search = "random")
set.seed(321)
model.w.knn <- train(Species ~.,
                     data = train_df,
                     method = "kknn",
                     trControl = fit_control,
                     preProcess = c("center", "scale"),
                     tuneLength = 8)
model.w.knn
plot(model.w.knn)
```

## Procesamiento en paralelo

No cambia la estructura de entrenamiento, solo es necesario fijar el número de núcleos. En este caso tenemos un ejemplo donde comparamos una ejecución sin paralelizar y otra que emplea 8 núcleos en paralelo con el paquete `doParallel`. El número de hiperparámetros es 16.

```{r, eval = FALSE}
# random search WKNN
fit_control <- trainControl(method='repeatedcv', number = 16, 
                            repeats = 10,
                            search = "random")
## No-Parallel ----
tic("wknn-train")

set.seed(321)
model.w.knn <- train(Species ~.,
                     data = train_df,
                     method = "kknn",
                     trControl = fit_control,
                     preProcess = c("center", "scale"),
                     tuneLength = 16)
time_wknn <- toc()

## Parallel ----

library(doParallel)

# Número de núcleos:
cl <- makePSOCKcluster(8)

# Registrar:
registerDoParallel(cl)

# Misma estructura que hasta ahora:
tic("wknn-train_para")
set.seed(321)
model.w.knn <- train(Species ~.,
                     data = train_df,
                     method = "kknn",
                     trControl = fit_control,
                     preProcess = c("center", "scale"),
                     tuneLength = 16)
time_wknn_para <- toc()


stopImplicitCluster()
```


