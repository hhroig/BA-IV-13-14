<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 K - Vecinos más Próximos | Introducción al Aprendizaje Supervisado</title>
  <meta name="description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="2 K - Vecinos más Próximos | Introducción al Aprendizaje Supervisado" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 K - Vecinos más Próximos | Introducción al Aprendizaje Supervisado" />
  
  <meta name="twitter:description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento." />
  

<meta name="author" content="Harold A. Hernández-Roig (hahernan@est-econ.uc3m.es)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="DA.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html"><i class="fa fa-check"></i><b>2</b> K - Vecinos más Próximos</a><ul>
<li class="chapter" data-level="2.1" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#clasificación-con-el-paquete-class"><i class="fa fa-check"></i><b>2.1</b> Clasificación con el paquete <code>class</code></a></li>
<li class="chapter" data-level="2.2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#el-paquete-caret"><i class="fa fa-check"></i><b>2.2</b> El paquete <code>caret</code></a><ul>
<li class="chapter" data-level="2.2.1" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#visualización"><i class="fa fa-check"></i><b>2.2.1</b> Visualización</a></li>
<li class="chapter" data-level="2.2.2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#clasificación-con-knn"><i class="fa fa-check"></i><b>2.2.2</b> Clasificación con KNN</a></li>
<li class="chapter" data-level="2.2.3" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#importancia-de-las-variables"><i class="fa fa-check"></i><b>2.2.3</b> Importancia de las variables</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#regresión"><i class="fa fa-check"></i><b>2.3</b> Regresión</a></li>
<li class="chapter" data-level="2.4" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#weighted-knn"><i class="fa fa-check"></i><b>2.4</b> Weighted KNN</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="DA.html"><a href="DA.html"><i class="fa fa-check"></i><b>3</b> Análisis Discriminante</a><ul>
<li class="chapter" data-level="3.1" data-path="DA.html"><a href="DA.html#LDA"><i class="fa fa-check"></i><b>3.1</b> Análisis Discriminante Lineal</a></li>
<li class="chapter" data-level="3.2" data-path="DA.html"><a href="DA.html#QDA"><i class="fa fa-check"></i><b>3.2</b> Análisis Discriminante Cuadrático</a></li>
<li class="chapter" data-level="3.3" data-path="DA.html"><a href="DA.html#RDA"><i class="fa fa-check"></i><b>3.3</b> Análisis Discriminante Regularizado</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="compara.html"><a href="compara.html"><i class="fa fa-check"></i><b>4</b> Comparación entre Modelos</a><ul>
<li class="chapter" data-level="4.1" data-path="compara.html"><a href="compara.html#comparando-según-accuracy"><i class="fa fa-check"></i><b>4.1</b> Comparando según <code>Accuracy</code></a></li>
<li class="chapter" data-level="4.2" data-path="compara.html"><a href="compara.html#curva-roc"><i class="fa fa-check"></i><b>4.2</b> Curva <em>ROC</em></a><ul>
<li class="chapter" data-level="4.2.1" data-path="compara.html"><a href="compara.html#análisis-en-la-muestra-test"><i class="fa fa-check"></i><b>4.2.1</b> Análisis en la muestra test</a></li>
<li class="chapter" data-level="4.2.2" data-path="compara.html"><a href="compara.html#análisis-en-la-muestra-de-entrenamiento"><i class="fa fa-check"></i><b>4.2.2</b> Análisis en la muestra de entrenamiento</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="wiscon.html"><a href="wiscon.html"><i class="fa fa-check"></i><b>5</b> Wisconsin Breast-Cancer Data</a><ul>
<li class="chapter" data-level="5.1" data-path="wiscon.html"><a href="wiscon.html#comentarios-finales-reducción-de-la-dimensión"><i class="fa fa-check"></i><b>5.1</b> Comentarios finales: reducción de la dimensión</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción al Aprendizaje Supervisado</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="k---vecinos-más-próximos" class="section level1">
<h1><span class="header-section-number">2</span> K - Vecinos más Próximos</h1>
<p>Comenzamos con uno de los algoritmos más sencillos e intuitivos para regresión y clasificación: los <em>K - Vecinos Más Próximos</em> (<em>KNN: K-Nearest Neighbors</em>). Nuestro bautizo será con un problema de clasificación, empleando los paquetes <code>class</code> y <code>caret</code>. Luego, pasaremos a un problema sencillo de regresión, esta vez usando solo el paquete <code>caret</code>. Trataremos los problemas de <em>ajuste y validación del modelo</em>, usando técnicas de <em>remuestreo</em>.</p>
<div id="clasificación-con-el-paquete-class" class="section level2">
<h2><span class="header-section-number">2.1</span> Clasificación con el paquete <code>class</code></h2>
<p>El problema inicial está relacionado con la clasificación de la especie de flor Iris—<em>setosa</em>, <em>virginica</em> y <em>versicolor</em>—a partir de mediciones sus pétalos y sépalos. Estos datos fueron recogidos por Ronald Fisher con el objetivo de cuantificar la variación morfológica de la flor. Actualmente están disponibles en diversas plataformas. En R es uno de los datos que vienen de base (<code>iris</code>).</p>
<p>En la Tabla <a href="k-vecinos-más-próximos.html#tab:iris-tab">2.1</a> representamos una muestra del dataset, que en su totalidad consiste de 50 observaciones de cada una de las 3 especies. Como todo estudio, debemos comenzar por un análisis descriptivo de la muestra.</p>
<table>
<caption><span id="tab:iris-tab">Table 2.1: </span>Estructura del dataset Iris</caption>
<thead>
<tr class="header">
<th align="right">Sepal.Length</th>
<th align="right">Sepal.Width</th>
<th align="right">Petal.Length</th>
<th align="right">Petal.Width</th>
<th align="left">Species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5.1</td>
<td align="right">3.5</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">4.9</td>
<td align="right">3.0</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">4.6</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">5.0</td>
<td align="right">3.6</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.7</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">4.6</td>
<td align="right">3.4</td>
<td align="right">1.4</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">5.0</td>
<td align="right">3.4</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">4.4</td>
<td align="right">2.9</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">4.9</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.1</td>
<td align="left">setosa</td>
</tr>
</tbody>
</table>
<p>Cargamos las librerías necesarias para llevar a cabo el estudio. Luego inspeccionamos los datos.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1"><span class="kw">library</span>(class)</a>
<a class="sourceLine" id="cb1-2" title="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb1-3" title="3"><span class="kw">library</span>(GGally)</a>
<a class="sourceLine" id="cb1-4" title="4"></a>
<a class="sourceLine" id="cb1-5" title="5">df &lt;-<span class="st"> </span><span class="kw">data</span>(iris) <span class="co"># cargar datos</span></a>
<a class="sourceLine" id="cb1-6" title="6"><span class="kw">summary</span>(iris) <span class="co"># un breve descriptivo</span></a></code></pre></div>
<pre><code>##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  
##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
##        Species  
##  setosa    :50  
##  versicolor:50  
##  virginica :50  
##                 
##                 
## </code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" title="1"><span class="co"># ver el balance de la muestra, según las clases</span></a>
<a class="sourceLine" id="cb3-2" title="2"><span class="kw">prop.table</span>(<span class="kw">table</span>(iris<span class="op">$</span>Species))</a></code></pre></div>
<pre><code>## 
##     setosa versicolor  virginica 
##  0.3333333  0.3333333  0.3333333</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># visualización</span></a>
<a class="sourceLine" id="cb5-2" title="2"><span class="kw">ggpairs</span>(iris, ggplot2<span class="op">::</span><span class="kw">aes</span>(<span class="dt">colour =</span> Species, <span class="dt">alpha =</span> <span class="fl">0.2</span>), <span class="dt">lower=</span><span class="kw">list</span>(<span class="dt">combo=</span><span class="kw">wrap</span>(<span class="st">&quot;facethist&quot;</span>,  </a>
<a class="sourceLine" id="cb5-3" title="3"><span class="dt">bins=</span><span class="kw">round</span>(<span class="kw">sqrt</span>(<span class="dv">50</span>))))) <span class="op">+</span><span class="st"> </span><span class="kw">theme_light</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>De estos análisis observamos, por ejemplo, que 2 de las 4 variables—<em>Petal.Width</em> y <em>Petal.Length</em>—parecen separar bastante bien las 3 especies, y que la muestra está muy bien balanceada.</p>
<p>Antes de pasar a ajustar nuestro modelo, debemos <em>preprocesar</em> la muestra. El algoritmo <em>KNN</em> es muy sensible a la escala de los datos, por ejemplo, podría favorecer distancias entre elementos con valores más grandes. Una forma sencilla de <em>estandarizar</em> o <em>escalar</em> los datos es usando:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" title="1">iris.scl &lt;-<span class="st"> </span><span class="kw">scale</span>(iris[,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>])</a></code></pre></div>
<p>Otro elemento importante es la validación del modelo que ajustemos: ¿cómo y con qué muestra medir la precisión? Por lo pronto, fijaremos aleatoriamente un 20% de los datos para calcular la <em>tasa de error</em>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" title="1"><span class="co"># set de índices para entrenar-validar (80% - 20%)</span></a>
<a class="sourceLine" id="cb7-2" title="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb7-3" title="3">train.ID &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(iris), <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(iris)) </a>
<a class="sourceLine" id="cb7-4" title="4"></a>
<a class="sourceLine" id="cb7-5" title="5"><span class="co"># matriz de diseño para entrenar</span></a>
<a class="sourceLine" id="cb7-6" title="6">X.train &lt;-<span class="st"> </span>iris.scl[train.ID,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]</a>
<a class="sourceLine" id="cb7-7" title="7"><span class="co"># matriz de diseño para testear</span></a>
<a class="sourceLine" id="cb7-8" title="8">X.test &lt;-<span class="st"> </span>iris.scl[<span class="op">-</span>train.ID,<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]</a>
<a class="sourceLine" id="cb7-9" title="9"><span class="co"># respuesta (categórica) entrenamiento</span></a>
<a class="sourceLine" id="cb7-10" title="10">Y &lt;-<span class="st"> </span>iris[train.ID,<span class="dv">5</span>]</a>
<a class="sourceLine" id="cb7-11" title="11"><span class="co"># respuesta (categórica) test</span></a>
<a class="sourceLine" id="cb7-12" title="12">Y.test &lt;-<span class="st"> </span>iris[<span class="op">-</span>train.ID,<span class="dv">5</span>]</a></code></pre></div>
<p>Usando la función <code>knn</code> podemos predecir las clases de los datos en <code>X.test</code>. Otro problema es cómo seleccionar la cantidad de vecinos <code>k</code> apropiada. Una <em>regla de pulgar</em> (<em>thumb rule</em>) es fijar <span class="math inline">\(k = \sqrt{n_{train}}\)</span>. Para analizar la precisión del modelo creamos la <em>matriz de confusión</em> y calculamos la tasa de error correspondiente para el conjunto de datos test.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" title="1"><span class="co"># KNN </span></a>
<a class="sourceLine" id="cb8-2" title="2">pr &lt;-<span class="st"> </span><span class="kw">knn</span>(X.train, X.test, <span class="dt">cl=</span>Y, <span class="dt">k =</span> <span class="kw">round</span>(<span class="kw">sqrt</span>(<span class="kw">nrow</span>(X.train))))</a>
<a class="sourceLine" id="cb8-3" title="3"></a>
<a class="sourceLine" id="cb8-4" title="4"><span class="co"># matriz de confusión</span></a>
<a class="sourceLine" id="cb8-5" title="5">tab &lt;-<span class="st"> </span><span class="kw">table</span>(pr,Y.test)</a></code></pre></div>
<table>
<caption><span id="tab:confIris-tab">Table 2.2: </span>Matriz de Confusión - KNN Iris</caption>
<thead>
<tr class="header">
<th></th>
<th align="right">setosa</th>
<th align="right">versicolor</th>
<th align="right">virginica</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>setosa</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td>versicolor</td>
<td align="right">0</td>
<td align="right">14</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td>virginica</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" title="1"><span class="co"># tasa de error test</span></a>
<a class="sourceLine" id="cb9-2" title="2">test.error &lt;-<span class="st"> </span><span class="kw">sum</span>(pr <span class="op">!=</span><span class="st"> </span>Y.test)<span class="op">/</span><span class="kw">sum</span>(tab)</a>
<a class="sourceLine" id="cb9-3" title="3">test.error</a></code></pre></div>
<pre><code>## [1] 0.03333333</code></pre>
<p>La tasa de error test es bastante baja, además indica que se clasifican bien el <span class="math inline">\(\approx 97\%\)</span> de las observaciones. Veamos ahora qué pasa al variar el número de vecinos <code>K</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" title="1">test.error &lt;-<span class="st"> </span><span class="kw">data.frame</span>()</a>
<a class="sourceLine" id="cb11-2" title="2"><span class="cf">for</span> (K <span class="cf">in</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">120</span>, <span class="dt">by =</span> <span class="dv">5</span>)) {</a>
<a class="sourceLine" id="cb11-3" title="3">  <span class="co"># KNN</span></a>
<a class="sourceLine" id="cb11-4" title="4">  pr &lt;-<span class="st"> </span><span class="kw">knn</span>(X.train,X.test,<span class="dt">cl=</span>Y,<span class="dt">k=</span>K)</a>
<a class="sourceLine" id="cb11-5" title="5">  <span class="co"># matriz de confusion</span></a>
<a class="sourceLine" id="cb11-6" title="6">  tab &lt;-<span class="st"> </span><span class="kw">table</span>(pr,Y.test)</a>
<a class="sourceLine" id="cb11-7" title="7">  <span class="co"># tasa de error test</span></a>
<a class="sourceLine" id="cb11-8" title="8">  test.error &lt;-<span class="st"> </span><span class="kw">rbind</span>(test.error, </a>
<a class="sourceLine" id="cb11-9" title="9">                        <span class="kw">data.frame</span>(<span class="dt">Tasa.Error =</span> <span class="kw">sum</span>(pr <span class="op">!=</span><span class="st"> </span>Y.test)<span class="op">/</span><span class="kw">sum</span>(tab), K))</a>
<a class="sourceLine" id="cb11-10" title="10">}</a>
<a class="sourceLine" id="cb11-11" title="11"></a>
<a class="sourceLine" id="cb11-12" title="12"><span class="kw">ggplot</span>(test.error, <span class="kw">aes</span>(<span class="dt">x =</span> K, <span class="dt">y =</span> Tasa.Error)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb11-13" title="13"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb11-14" title="14"><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb11-15" title="15"><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Tasa de Error (test)&quot;</span>) <span class="op">+</span><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;K: número de vecinos&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb11-16" title="16"><span class="st">  </span><span class="kw">theme_light</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-7-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Se representa la tasa de error al aumentar el número de vecinos. Los <em>saltos</em> de la curva son resultado del pequeño tamaño de la muestra test. Como cualquier otro modelo de machine learning, el interés está en seleccionar el nivel de flexibilidad (número de vecinos) que mejore la clasificación… inténtalo!</p>
</div>
<div id="el-paquete-caret" class="section level2">
<h2><span class="header-section-number">2.2</span> El paquete <code>caret</code></h2>
<p>El paquete <code>caret</code> (<em><strong>C</strong>lassification <strong>A</strong>nd <strong>RE</strong>gression <strong>T</strong>raining</em>) es uno de los más populares para entrenar modelos de <em>machine learning</em>. Contiene una interface uniforme para la mayoría de los algoritmos que se tratan en este curso y, en particular, los 3 que veremos en estas sesiones. Las ventajas del paquete son que permite hacer:</p>
<ul>
<li>partición de los datos</li>
<li>pre-procesado de los datos</li>
<li>selección de variables</li>
<li>ajuste del modelo usando remuestreo</li>
<li>estimación de la importancia/relevancia de las variables</li>
</ul>
<p>Más información disponible en <a href="http://topepo.github.io/caret/index.html">topepo.github.io/caret</a>.</p>
<div id="visualización" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Visualización</h3>
<p>Seguiremos con los datos <code>iris</code>. El paso inicial: análisis descriptivo y visualización de los datos podríamos obviarlo… pero a modo didáctico reproducimos el mismo análisis, esta vez usando la función <code>featurePlot</code> de <code>caret</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" title="1"><span class="kw">library</span>(caret)</a></code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" title="1"><span class="kw">str</span>(iris)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>Diagramas de dispersión:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" title="1"><span class="kw">featurePlot</span>(<span class="dt">x =</span> iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], </a>
<a class="sourceLine" id="cb16-2" title="2">            <span class="dt">y =</span> iris<span class="op">$</span>Species, </a>
<a class="sourceLine" id="cb16-3" title="3">            <span class="dt">plot =</span> <span class="st">&quot;pairs&quot;</span>,</a>
<a class="sourceLine" id="cb16-4" title="4">            <span class="co">## Add a key at the top</span></a>
<a class="sourceLine" id="cb16-5" title="5">            <span class="dt">auto.key =</span> <span class="kw">list</span>(<span class="dt">columns =</span> <span class="dv">3</span>))</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-9-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Densidades estimadas:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" title="1"><span class="kw">featurePlot</span>(<span class="dt">x =</span> iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], </a>
<a class="sourceLine" id="cb17-2" title="2">            <span class="dt">y =</span> iris<span class="op">$</span>Species,</a>
<a class="sourceLine" id="cb17-3" title="3">            <span class="dt">plot =</span> <span class="st">&quot;density&quot;</span>, </a>
<a class="sourceLine" id="cb17-4" title="4">            <span class="co">## Pass in options to xyplot() to </span></a>
<a class="sourceLine" id="cb17-5" title="5">            <span class="co">## make it prettier</span></a>
<a class="sourceLine" id="cb17-6" title="6">            <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">relation=</span><span class="st">&quot;free&quot;</span>), </a>
<a class="sourceLine" id="cb17-7" title="7">                          <span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">relation=</span><span class="st">&quot;free&quot;</span>)), </a>
<a class="sourceLine" id="cb17-8" title="8">            <span class="dt">adjust =</span> <span class="fl">1.5</span>, </a>
<a class="sourceLine" id="cb17-9" title="9">            <span class="dt">pch =</span> <span class="st">&quot;|&quot;</span>, </a>
<a class="sourceLine" id="cb17-10" title="10">            <span class="dt">layout =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">1</span>), </a>
<a class="sourceLine" id="cb17-11" title="11">            <span class="dt">auto.key =</span> <span class="kw">list</span>(<span class="dt">columns =</span> <span class="dv">3</span>))</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-10-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Diagramas de cajas:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" title="1"><span class="kw">featurePlot</span>(<span class="dt">x =</span> iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>], </a>
<a class="sourceLine" id="cb18-2" title="2">            <span class="dt">y =</span> iris<span class="op">$</span>Species, </a>
<a class="sourceLine" id="cb18-3" title="3">            <span class="dt">plot =</span> <span class="st">&quot;box&quot;</span>, </a>
<a class="sourceLine" id="cb18-4" title="4">            <span class="co">## Pass in options to bwplot() </span></a>
<a class="sourceLine" id="cb18-5" title="5">            <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">relation=</span><span class="st">&quot;free&quot;</span>),</a>
<a class="sourceLine" id="cb18-6" title="6">                          <span class="dt">x =</span> <span class="kw">list</span>(<span class="dt">rot =</span> <span class="dv">90</span>)),  </a>
<a class="sourceLine" id="cb18-7" title="7">            <span class="dt">layout =</span> <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">1</span> ), </a>
<a class="sourceLine" id="cb18-8" title="8">            <span class="dt">auto.key =</span> <span class="kw">list</span>(<span class="dt">columns =</span> <span class="dv">2</span>))</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-11-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="clasificación-con-knn" class="section level3">
<h3><span class="header-section-number">2.2.2</span> Clasificación con KNN</h3>
<p>Necesitamos extraer una muestra independiente (<em>test</em>) para probar el modelo, una vez ajustado. Ahora usaremos la función <code>createDataPartition</code>, que permite hacer la partición teniendo en cuenta la variable respuesta. Esto es esencial para mantener el balance de la muestra.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" title="1"><span class="co"># creamos una partición test</span></a>
<a class="sourceLine" id="cb19-2" title="2">df &lt;-<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb19-3" title="3"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb19-4" title="4">train.ID &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(df<span class="op">$</span>Species, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb19-5" title="5"></a>
<a class="sourceLine" id="cb19-6" title="6">train_df &lt;-<span class="st"> </span>df[train.ID, ]</a>
<a class="sourceLine" id="cb19-7" title="7">test_df &lt;-<span class="st"> </span>df[<span class="op">-</span>train.ID, ]</a></code></pre></div>
<p>Para ajustar el modelo usaremos la función <code>train</code>, que permite:
* evaluar, usando remuestreo, el efecto de distintos parámetros en la precisión del modelo
* escoger el modelo óptimo, de acuerdo a los parámetros probados
* estimar la precisión del modelo, de acuerdo a diferentes medidas</p>
<p>Actualmente hay uno <span class="math inline">\(\approx 238\)</span> modelos disponibles. Nosotros empezaremos probando el <code>knn</code>, pero antes tenemos que especificar el método de remuestreo, usando la función <code>trainControl</code>. Con esta función, podemos fijar una validación cruzada <em>k-Fold</em> o <em>leave-one-out (LOOCV)</em>. También están disponibles las opciones <em>bootstrap</em> y <em>k-Fold repetitivo</em>.</p>
<p>En este ejemplo, hemos fijado un <em>k-Fold</em> con 10 hojas. Además, hacemos el escalado de las variables dentro del propio algoritmo, usando la opción <code>preProcess</code>. Finalmente, le decimos al algoritmo que intente 10 valores diferentes para escoger el número de vecinos óptimo, usando la opción <code>tuneLength</code></p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" title="1"><span class="co"># primeros pasos con la validación cruzada...</span></a>
<a class="sourceLine" id="cb20-2" title="2">fit_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;cv&#39;</span>, <span class="dt">number =</span> <span class="dv">10</span>)  </a>
<a class="sourceLine" id="cb20-3" title="3"></a>
<a class="sourceLine" id="cb20-4" title="4">model_knn_iris &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span>., </a>
<a class="sourceLine" id="cb20-5" title="5">                       <span class="dt">data =</span> train_df, </a>
<a class="sourceLine" id="cb20-6" title="6">                       <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb20-7" title="7">                       <span class="dt">trControl =</span> fit_control, </a>
<a class="sourceLine" id="cb20-8" title="8">                       <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),  </a>
<a class="sourceLine" id="cb20-9" title="9">                       <span class="dt">tuneLength =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb20-10" title="10">model_knn_iris</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 120 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (4), scaled (4) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa 
##    5  0.9666667  0.9500
##    7  0.9583333  0.9375
##    9  0.9750000  0.9625
##   11  0.9583333  0.9375
##   13  0.9583333  0.9375
##   15  0.9583333  0.9375
##   17  0.9583333  0.9375
##   19  0.9416667  0.9125
##   21  0.9500000  0.9250
##   23  0.9333333  0.9000
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 9.</code></pre>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" title="1"><span class="kw">plot</span>(model_knn_iris)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-13-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Podemos ver en el resumen el número óptimo de vecinos (entre los valores probados) del modelo final. En el gráfico, vemos cómo varía el <em>accuracy</em> en función del número de vecinos. La tabla de confusión y medidas de precisión para los datos test:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" title="1"><span class="co"># hagamos las predicciones del conjunto de prueba</span></a>
<a class="sourceLine" id="cb23-2" title="2">prediction_knn_iris &lt;-<span class="st"> </span><span class="kw">predict</span>(model_knn_iris, <span class="dt">newdata =</span> test_df)</a>
<a class="sourceLine" id="cb23-3" title="3"><span class="kw">confusionMatrix</span>(prediction_knn_iris, <span class="dt">reference =</span> test_df<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0         10         2
##   virginica       0          0         8
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9333          
##                  95% CI : (0.7793, 0.9918)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : 8.747e-12       
##                                           
##                   Kappa : 0.9             
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            1.0000           0.8000
## Specificity                 1.0000            0.9000           1.0000
## Pos Pred Value              1.0000            0.8333           1.0000
## Neg Pred Value              1.0000            1.0000           0.9091
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3333           0.2667
## Detection Prevalence        0.3333            0.4000           0.2667
## Balanced Accuracy           1.0000            0.9500           0.9000</code></pre>
<p>Intentemos ahora fijar las cantidades de vecinos a probar. También cambiamos el método de remuestreo…</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" title="1"><span class="co"># definimos el grid:</span></a>
<a class="sourceLine" id="cb25-2" title="2">some_k &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">15</span>) </a>
<a class="sourceLine" id="cb25-3" title="3"></a>
<a class="sourceLine" id="cb25-4" title="4"><span class="co"># k-fold CV pero con repeticiones</span></a>
<a class="sourceLine" id="cb25-5" title="5">fit_control1 &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb25-6" title="6">  <span class="dt">method =</span> <span class="st">&quot;repeatedcv&quot;</span>, </a>
<a class="sourceLine" id="cb25-7" title="7">  <span class="dt">number =</span> <span class="dv">10</span>, <span class="co"># número de folds</span></a>
<a class="sourceLine" id="cb25-8" title="8">  <span class="dt">repeats =</span> <span class="dv">5</span> ) <span class="co"># repeticiones</span></a>
<a class="sourceLine" id="cb25-9" title="9"></a>
<a class="sourceLine" id="cb25-10" title="10"><span class="co"># bootstrap</span></a>
<a class="sourceLine" id="cb25-11" title="11">fit_control2 &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb25-12" title="12">  <span class="dt">method =</span> <span class="st">&quot;boot&quot;</span>,  </a>
<a class="sourceLine" id="cb25-13" title="13">  <span class="dt">number =</span> <span class="dv">10</span>) <span class="co"># número de muestras bootstrap</span></a>
<a class="sourceLine" id="cb25-14" title="14"></a>
<a class="sourceLine" id="cb25-15" title="15"><span class="co"># LOOCV</span></a>
<a class="sourceLine" id="cb25-16" title="16">fit_control3 &lt;-<span class="st"> </span><span class="kw">trainControl</span>(</a>
<a class="sourceLine" id="cb25-17" title="17">  <span class="dt">method =</span> <span class="st">&quot;LOOCV&quot;</span>) </a>
<a class="sourceLine" id="cb25-18" title="18"></a>
<a class="sourceLine" id="cb25-19" title="19">model2_knn_iris &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span>., </a>
<a class="sourceLine" id="cb25-20" title="20">                        <span class="dt">data =</span> train_df, </a>
<a class="sourceLine" id="cb25-21" title="21">                        <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb25-22" title="22">                        <span class="dt">trControl =</span> fit_control2, </a>
<a class="sourceLine" id="cb25-23" title="23">                        <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),  </a>
<a class="sourceLine" id="cb25-24" title="24">                        <span class="dt">tuneGrid =</span> some_k)</a>
<a class="sourceLine" id="cb25-25" title="25">model2_knn_iris</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 120 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (4), scaled (4) 
## Resampling: Bootstrapped (10 reps) 
## Summary of sample sizes: 120, 120, 120, 120, 120, 120, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    1  0.9481717  0.9211545
##    2  0.9475159  0.9196833
##    3  0.9453882  0.9165257
##    4  0.9599231  0.9393154
##    5  0.9665503  0.9492382
##    6  0.9686637  0.9524826
##    7  0.9688968  0.9528083
##    8  0.9596084  0.9388259
##    9  0.9647549  0.9465745
##   10  0.9600366  0.9392605
##   11  0.9624756  0.9431153
##   12  0.9555270  0.9326274
##   13  0.9623479  0.9429620
##   14  0.9439143  0.9151811
##   15  0.9438559  0.9150458
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 7.</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" title="1"><span class="kw">plot</span>(model2_knn_iris)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-15-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" title="1"><span class="co"># hagamos las predicciones del conjunto de prueba</span></a>
<a class="sourceLine" id="cb28-2" title="2">prediction_knn_iris2 &lt;-<span class="st"> </span><span class="kw">predict</span>(model2_knn_iris, <span class="dt">newdata =</span> test_df)</a>
<a class="sourceLine" id="cb28-3" title="3"><span class="kw">confusionMatrix</span>(prediction_knn_iris2, <span class="dt">reference =</span> test_df<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0         10         2
##   virginica       0          0         8
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9333          
##                  95% CI : (0.7793, 0.9918)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : 8.747e-12       
##                                           
##                   Kappa : 0.9             
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            1.0000           0.8000
## Specificity                 1.0000            0.9000           1.0000
## Pos Pred Value              1.0000            0.8333           1.0000
## Neg Pred Value              1.0000            1.0000           0.9091
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3333           0.2667
## Detection Prevalence        0.3333            0.4000           0.2667
## Balanced Accuracy           1.0000            0.9500           0.9000</code></pre>
</div>
<div id="importancia-de-las-variables" class="section level3">
<h3><span class="header-section-number">2.2.3</span> Importancia de las variables</h3>
<p>Para KNN no tenemos un método que permita determinar la relevancia de cada predictor. Por ejemplo, en mínimos cuadrados, sí se puede conducir un test para determinar si cada coeficiente <span class="math inline">\(\beta_i\)</span> del modelo es significativamente distinto de cero. Aún así, <code>caret</code> incorpora la función <code>varImp</code> que da una medida de <em>importancia</em> de cada predictor del problema de clasificación o regresión.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" title="1"><span class="kw">varImp</span>(model_knn_iris)</a></code></pre></div>
<pre><code>## ROC curve variable importance
## 
##   variables are sorted by maximum importance across the classes
##              setosa versicolor virginica
## Petal.Width  100.00     100.00     100.0
## Petal.Length 100.00     100.00     100.0
## Sepal.Length  90.80      72.07      90.8
## Sepal.Width   56.32      56.32       0.0</code></pre>
<p>Aunque esto no debe usarse como método de selección de variables, sí motiva el estudio del problema al disminuir la dimensión <span class="math inline">\(p = 4\)</span>. Por ejemplo, veamos la precisión del modelo al dejar solo <code>Petal.Length</code> y <code>Petal.Width</code>.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" title="1"><span class="co"># seleccionamos los predictores que queremos y la respuesta</span></a>
<a class="sourceLine" id="cb32-2" title="2">df_petal &lt;-<span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>, <span class="st">&quot;Species&quot;</span>)]</a>
<a class="sourceLine" id="cb32-3" title="3">train_df_petal &lt;-<span class="st"> </span>df_petal[train.ID, ]</a>
<a class="sourceLine" id="cb32-4" title="4">test_df_petal &lt;-<span class="st"> </span>df_petal[<span class="op">-</span>train.ID, ]</a>
<a class="sourceLine" id="cb32-5" title="5"></a>
<a class="sourceLine" id="cb32-6" title="6"><span class="co"># el modelo...</span></a>
<a class="sourceLine" id="cb32-7" title="7">fit_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;cv&#39;</span>, <span class="dt">number =</span> <span class="dv">10</span>)  </a>
<a class="sourceLine" id="cb32-8" title="8"></a>
<a class="sourceLine" id="cb32-9" title="9">model_knn_petal &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span>., </a>
<a class="sourceLine" id="cb32-10" title="10">                        <span class="dt">data =</span> train_df_petal, </a>
<a class="sourceLine" id="cb32-11" title="11">                        <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb32-12" title="12">                        <span class="dt">trControl =</span> fit_control, </a>
<a class="sourceLine" id="cb32-13" title="13">                        <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),  </a>
<a class="sourceLine" id="cb32-14" title="14">                        <span class="dt">tuneLength =</span> <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb32-15" title="15">model_knn_petal</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 120 samples
##   2 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (2), scaled (2) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa 
##    5  0.9666667  0.9500
##    7  0.9666667  0.9500
##    9  0.9666667  0.9500
##   11  0.9666667  0.9500
##   13  0.9666667  0.9500
##   15  0.9666667  0.9500
##   17  0.9583333  0.9375
##   19  0.9666667  0.9500
##   21  0.9583333  0.9375
##   23  0.9666667  0.9500
##   25  0.9666667  0.9500
##   27  0.9666667  0.9500
##   29  0.9666667  0.9500
##   31  0.9666667  0.9500
##   33  0.9750000  0.9625
##   35  0.9833333  0.9750
##   37  0.9750000  0.9625
##   39  0.9833333  0.9750
##   41  0.9750000  0.9625
##   43  0.9666667  0.9500
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 39.</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" title="1"><span class="kw">plot</span>(model_knn_petal)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-18-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" title="1"><span class="co"># hagamos las predicciones del conjunto de prueba</span></a>
<a class="sourceLine" id="cb35-2" title="2">prediction_knn_petal &lt;-<span class="st"> </span><span class="kw">predict</span>(model_knn_petal, <span class="dt">newdata =</span> test_df_petal)</a>
<a class="sourceLine" id="cb35-3" title="3"><span class="kw">confusionMatrix</span>(prediction_knn_petal, <span class="dt">reference =</span> test_df_petal<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0         10         3
##   virginica       0          0         7
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9             
##                  95% CI : (0.7347, 0.9789)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : 1.665e-10       
##                                           
##                   Kappa : 0.85            
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            1.0000           0.7000
## Specificity                 1.0000            0.8500           1.0000
## Pos Pred Value              1.0000            0.7692           1.0000
## Neg Pred Value              1.0000            1.0000           0.8696
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3333           0.2333
## Detection Prevalence        0.3333            0.4333           0.2333
## Balanced Accuracy           1.0000            0.9250           0.8500</code></pre>
<p>Pero, ¿cómo visualizar las fronteras de decisión del método? Ahora que <span class="math inline">\(p = 2\)</span>, podemos representar esto en el plano usando la siguiente función:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" title="1">decision_bound =<span class="st"> </span><span class="cf">function</span>(train_df_in, test_df_in, model_in){</a>
<a class="sourceLine" id="cb37-2" title="2">  <span class="co"># plot decision boundary  for iris[,c(&quot;Petal.Length&quot;, &quot;Petal.Width&quot;, &quot;Species&quot;)]</span></a>
<a class="sourceLine" id="cb37-3" title="3"></a>
<a class="sourceLine" id="cb37-4" title="4">  <span class="kw">require</span>(MASS)</a>
<a class="sourceLine" id="cb37-5" title="5">  <span class="kw">require</span>(caret)</a>
<a class="sourceLine" id="cb37-6" title="6">  <span class="kw">require</span>(ggplot2)</a>
<a class="sourceLine" id="cb37-7" title="7">  <span class="kw">require</span>(gridExtra)</a>
<a class="sourceLine" id="cb37-8" title="8"></a>
<a class="sourceLine" id="cb37-9" title="9">  <span class="co"># Paso 1: crear un grid de valores desde min a max de ambos predictores</span></a>
<a class="sourceLine" id="cb37-10" title="10">  pl =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(train_df_in<span class="op">$</span>Petal.Length), <span class="kw">max</span>(train_df_in<span class="op">$</span>Petal.Length), <span class="dt">length.out =</span> <span class="dv">80</span>)</a>
<a class="sourceLine" id="cb37-11" title="11">  pw =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(train_df_in<span class="op">$</span>Petal.Width), <span class="kw">max</span>(train_df_in<span class="op">$</span>Petal.Width), <span class="dt">length.out =</span> <span class="dv">80</span>)</a>
<a class="sourceLine" id="cb37-12" title="12"></a>
<a class="sourceLine" id="cb37-13" title="13">  lgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">Petal.Length=</span>pl, <span class="dt">Petal.Width=</span>pw)</a>
<a class="sourceLine" id="cb37-14" title="14"></a>
<a class="sourceLine" id="cb37-15" title="15">  <span class="co"># Paso 2: obtener las predicciones tanto para el grid como para el test</span></a>
<a class="sourceLine" id="cb37-16" title="16">  modelPredGrid &lt;-<span class="st"> </span><span class="kw">predict</span>(model_in, <span class="dt">newdata=</span>lgrid)</a>
<a class="sourceLine" id="cb37-17" title="17">  train_df_in<span class="op">$</span>Pred.Class &lt;-<span class="st"> </span><span class="kw">predict</span>(model_in, <span class="dt">newdata =</span> train_df_in)</a>
<a class="sourceLine" id="cb37-18" title="18">  test_df_in<span class="op">$</span>Pred.Class &lt;-<span class="st"> </span><span class="kw">predict</span>(model_in, <span class="dt">newdata =</span> test_df_in)</a>
<a class="sourceLine" id="cb37-19" title="19"></a>
<a class="sourceLine" id="cb37-20" title="20">  <span class="co"># Paso 3: ggplot con la funcion contour</span></a>
<a class="sourceLine" id="cb37-21" title="21">  gg1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span>lgrid) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-22" title="22"><span class="st">    </span><span class="kw">stat_contour</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Petal.Length, <span class="dt">y=</span>Petal.Width, <span class="dt">z=</span><span class="kw">as.numeric</span>(modelPredGrid)), <span class="dt">bins=</span><span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-23" title="23"><span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Petal.Length, <span class="dt">y=</span>Petal.Width, <span class="dt">colour=</span>modelPredGrid), <span class="dt">alpha=</span><span class="fl">0.1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-24" title="24"><span class="st">    </span><span class="kw">labs</span>(<span class="dt">colour =</span> <span class="st">&quot;Clases&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Train&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-25" title="25"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data=</span>train_df_in,</a>
<a class="sourceLine" id="cb37-26" title="26">               <span class="kw">aes</span>(<span class="dt">x=</span>Petal.Length, <span class="dt">y=</span>Petal.Width,</a>
<a class="sourceLine" id="cb37-27" title="27">                   <span class="dt">colour=</span>Species), <span class="dt">size=</span><span class="dv">5</span>, <span class="dt">shape=</span><span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-28" title="28"><span class="st">    </span><span class="kw">theme_light</span>()</a>
<a class="sourceLine" id="cb37-29" title="29"></a>
<a class="sourceLine" id="cb37-30" title="30">  gg2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(<span class="dt">data=</span>lgrid) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-31" title="31"><span class="st">    </span><span class="kw">stat_contour</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Petal.Length, <span class="dt">y=</span>Petal.Width, <span class="dt">z=</span><span class="kw">as.numeric</span>(modelPredGrid)), <span class="dt">bins=</span><span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-32" title="32"><span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>Petal.Length, <span class="dt">y=</span>Petal.Width, <span class="dt">colour=</span>modelPredGrid), <span class="dt">alpha=</span><span class="fl">0.1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-33" title="33"><span class="st">    </span><span class="kw">labs</span>(<span class="dt">colour =</span> <span class="st">&quot;Clases&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Test&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-34" title="34"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data=</span>test_df_in,</a>
<a class="sourceLine" id="cb37-35" title="35">               <span class="kw">aes</span>(<span class="dt">x=</span>Petal.Length, <span class="dt">y=</span>Petal.Width,</a>
<a class="sourceLine" id="cb37-36" title="36">                   <span class="dt">colour=</span>Species), <span class="dt">size=</span><span class="dv">5</span>, <span class="dt">shape=</span><span class="dv">1</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb37-37" title="37"><span class="st">    </span><span class="kw">theme_light</span>()</a>
<a class="sourceLine" id="cb37-38" title="38">  <span class="kw">grid.arrange</span>(gg1, gg2, <span class="dt">ncol=</span><span class="dv">1</span>, <span class="dt">nrow=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb37-39" title="39">}</a></code></pre></div>
<p>Así que aplicando esto a nuestros datos de entrenamiento (o los del test) obtenemos las fronteras de decisión:</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" title="1"><span class="co"># fronteras de decisión, usando la nueva función</span></a>
<a class="sourceLine" id="cb38-2" title="2"><span class="kw">decision_bound</span>(train_df_petal, test_df_petal, model_knn_petal)</a></code></pre></div>
<pre><code>## Loading required package: MASS</code></pre>
<pre><code>## Loading required package: gridExtra</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-21-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="regresión" class="section level2">
<h2><span class="header-section-number">2.3</span> Regresión</h2>
<p>Abordamos ahora el problema de regresión con KNN, o sea, la respuesta es cuantitativa-continua. Seguimos usando el paquete <code>caret</code> que tiene implementado el algoritmo y ofrece facilidades para el preprocesado de los datos y la validación del modelo.</p>
<p>Particularmente, atacaremos el problema de predecir el precio medio de las viviendas (<code>medv</code>) en los suburbios de Boston, usando otras 13 variables predictoras.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" title="1"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb41-2" title="2"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb41-3" title="3"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb41-4" title="4"></a>
<a class="sourceLine" id="cb41-5" title="5"><span class="co"># cargar e inspeccionar los datos</span></a>
<a class="sourceLine" id="cb41-6" title="6"><span class="co"># para detalles sobre las variables predictoras:</span></a>
<a class="sourceLine" id="cb41-7" title="7"><span class="co"># ?Boston</span></a>
<a class="sourceLine" id="cb41-8" title="8"><span class="kw">data</span>(Boston)</a>
<a class="sourceLine" id="cb41-9" title="9"><span class="kw">str</span>(Boston)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ black  : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" title="1"><span class="kw">summary</span>(Boston)</a></code></pre></div>
<pre><code>##       crim                zn             indus            chas        
##  Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  
##  1st Qu.: 0.08204   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  
##  Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  
##  Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  
##  3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  
##  Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  
##       nox               rm             age              dis        
##  Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  
##  1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  
##  Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  
##  Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  
##  3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  
##  Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  
##       rad              tax           ptratio          black       
##  Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32  
##  1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38  
##  Median : 5.000   Median :330.0   Median :19.05   Median :391.44  
##  Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67  
##  3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23  
##  Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90  
##      lstat            medv      
##  Min.   : 1.73   Min.   : 5.00  
##  1st Qu.: 6.95   1st Qu.:17.02  
##  Median :11.36   Median :21.20  
##  Mean   :12.65   Mean   :22.53  
##  3rd Qu.:16.95   3rd Qu.:25.00  
##  Max.   :37.97   Max.   :50.00</code></pre>
<p>Veamos las relaciones entre predictores y la variable respuesta (en este ejemplo, solo hemos representado algunas).</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" title="1"><span class="co"># ver correlaciones y posibles relaciones:</span></a>
<a class="sourceLine" id="cb45-2" title="2"></a>
<a class="sourceLine" id="cb45-3" title="3"><span class="co"># todos los predictores:</span></a>
<a class="sourceLine" id="cb45-4" title="4"><span class="co"># ggpairs(Boston, ggplot2::aes(y = medv, alpha = 0.2)) + theme_light()</span></a>
<a class="sourceLine" id="cb45-5" title="5"><span class="co"># algunos predictores:</span></a>
<a class="sourceLine" id="cb45-6" title="6"><span class="kw">ggpairs</span>(Boston[, <span class="kw">c</span>(<span class="st">&quot;lstat&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;rad&quot;</span>, <span class="st">&quot;rm&quot;</span>, <span class="st">&quot;ptratio&quot;</span>, <span class="st">&quot;medv&quot;</span>)]) <span class="op">+</span><span class="st"> </span><span class="kw">theme_light</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Por ejemplo, es notable la relación lineal entre <code>medv</code> y las variables predictoras <code>rm</code> y <code>lstat</code>. Estas dos corresponden al número medio de habitaciones por vivienda y al ínfimo estatus poblacional, respectivamente.</p>
<p>Ajustemos un modelo de regresión, usando todas las variables y el algoritmo KNN.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" title="1"><span class="co"># Split the data into training and test set</span></a>
<a class="sourceLine" id="cb46-2" title="2"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb46-3" title="3">train.ID &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(Boston<span class="op">$</span>medv, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb46-4" title="4">train.data  &lt;-<span class="st"> </span>Boston[train.ID, ]</a>
<a class="sourceLine" id="cb46-5" title="5">test.data &lt;-<span class="st"> </span>Boston[<span class="op">-</span>train.ID, ]</a>
<a class="sourceLine" id="cb46-6" title="6"></a>
<a class="sourceLine" id="cb46-7" title="7"><span class="co"># Fit the model on the training set</span></a>
<a class="sourceLine" id="cb46-8" title="8"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb46-9" title="9">knn_reg_model &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb46-10" title="10">  medv<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb46-11" title="11">  <span class="dt">data =</span> train.data,</a>
<a class="sourceLine" id="cb46-12" title="12">  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</a>
<a class="sourceLine" id="cb46-13" title="13">  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb46-14" title="14">  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb46-15" title="15">  <span class="dt">tuneLength =</span> <span class="dv">20</span></a>
<a class="sourceLine" id="cb46-16" title="16">)</a>
<a class="sourceLine" id="cb46-17" title="17"></a>
<a class="sourceLine" id="cb46-18" title="18">knn_reg_model</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 407 samples
##  13 predictor
## 
## Pre-processing: centered (13), scaled (13) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 366, 367, 366, 366, 366, 366, ... 
## Resampling results across tuning parameters:
## 
##   k   RMSE      Rsquared   MAE     
##    5  4.446665  0.7748420  2.877705
##    7  4.513433  0.7760351  2.931696
##    9  4.510659  0.7822810  2.961313
##   11  4.557329  0.7806128  2.997708
##   13  4.589419  0.7769459  3.031409
##   15  4.683314  0.7715242  3.108591
##   17  4.727456  0.7658943  3.133048
##   19  4.794512  0.7608152  3.190837
##   21  4.871432  0.7555146  3.242578
##   23  4.896989  0.7568145  3.256753
##   25  4.982893  0.7505900  3.318786
##   27  5.035171  0.7497412  3.356345
##   29  5.099070  0.7472969  3.412470
##   31  5.196655  0.7375974  3.477003
##   33  5.260674  0.7329066  3.507057
##   35  5.320022  0.7310458  3.543261
##   37  5.393032  0.7258230  3.592311
##   39  5.457338  0.7205076  3.643305
##   41  5.500014  0.7178285  3.669332
##   43  5.561854  0.7125909  3.713330
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 5.</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" title="1"><span class="co"># Plot model error RMSE vs different values of k</span></a>
<a class="sourceLine" id="cb48-2" title="2"><span class="kw">plot</span>(knn_reg_model)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-24-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Ahora, lo que nos interesa es disminuir el Error Cuadrático Medio:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" title="1"><span class="co"># predicciones</span></a>
<a class="sourceLine" id="cb49-2" title="2">predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_reg_model, test.data)</a>
<a class="sourceLine" id="cb49-3" title="3"><span class="co"># RMSE: raíz del error cuadrático medio</span></a>
<a class="sourceLine" id="cb49-4" title="4"><span class="kw">RMSE</span>(predictions, test.data<span class="op">$</span>medv)</a></code></pre></div>
<pre><code>## [1] 4.762122</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" title="1"><span class="co"># MAE: error absoluto medio</span></a>
<a class="sourceLine" id="cb51-2" title="2"><span class="kw">MAE</span>(predictions, test.data<span class="op">$</span>medv)</a></code></pre></div>
<pre><code>## [1] 3.050101</code></pre>
<p>Si representamos las predicciones y los valores reales de la variable <code>medv</code>, esperamos que los puntos estén muy cercanos a la recta <span class="math inline">\(Y = X\)</span>.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" title="1">df_plot &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">pred =</span> predictions, <span class="dt">real =</span> test.data<span class="op">$</span>medv)</a>
<a class="sourceLine" id="cb53-2" title="2"><span class="kw">ggplot</span>(df_plot, <span class="kw">aes</span>(<span class="dt">x =</span> pred, <span class="dt">y =</span> real)) <span class="op">+</span></a>
<a class="sourceLine" id="cb53-3" title="3"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb53-4" title="4"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb53-5" title="5"><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">expression</span>(<span class="kw">hat</span>( y))) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb53-6" title="6"><span class="st">  </span><span class="kw">theme_light</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-26-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>¿Será posible mejorar esto? ¿Son todas las variables realmente necesarias? ¿Un grid con valores más pequeños a <span class="math inline">\(K = 5\)</span> podría resultar mejor? Veamos qué tal es el ajuste y las predicciones si nos limitamos a unas pocas variables predictoras.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" title="1"><span class="co"># importancia de las variables según impacto en la predicción</span></a>
<a class="sourceLine" id="cb54-2" title="2"><span class="kw">varImp</span>(knn_reg_model)</a></code></pre></div>
<pre><code>## loess r-squared variable importance
## 
##         Overall
## lstat    100.00
## rm        84.11
## ptratio   35.71
## indus     33.76
## crim      30.80
## tax       30.16
## black     25.33
## nox       24.24
## age       21.19
## rad       18.94
## dis       15.00
## zn        14.23
## chas       0.00</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" title="1"><span class="co"># seleccionemos solo algunas variables:</span></a>
<a class="sourceLine" id="cb56-2" title="2">boston &lt;-<span class="st"> </span>Boston[, <span class="kw">c</span>(<span class="st">&quot;lstat&quot;</span>, <span class="st">&quot;rm&quot;</span>, <span class="st">&quot;ptratio&quot;</span>, <span class="st">&quot;medv&quot;</span>)]</a></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" title="1"><span class="co"># ajustamos el modelo en el nuevo diseño</span></a>
<a class="sourceLine" id="cb57-2" title="2">train.data  &lt;-<span class="st"> </span>boston[train.ID, ]</a>
<a class="sourceLine" id="cb57-3" title="3">test.data &lt;-<span class="st"> </span>boston[<span class="op">-</span>train.ID, ]</a>
<a class="sourceLine" id="cb57-4" title="4"></a>
<a class="sourceLine" id="cb57-5" title="5"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb57-6" title="6">knn_reg_model &lt;-<span class="st"> </span><span class="kw">train</span>(</a>
<a class="sourceLine" id="cb57-7" title="7">  medv<span class="op">~</span>.,</a>
<a class="sourceLine" id="cb57-8" title="8">  <span class="dt">data =</span> train.data,</a>
<a class="sourceLine" id="cb57-9" title="9">  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</a>
<a class="sourceLine" id="cb57-10" title="10">  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb57-11" title="11">  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>,<span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb57-12" title="12">  <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">k =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">15</span>)</a>
<a class="sourceLine" id="cb57-13" title="13">)</a>
<a class="sourceLine" id="cb57-14" title="14"></a>
<a class="sourceLine" id="cb57-15" title="15"><span class="co"># Veamos si el modelo ha mejorado algo:</span></a>
<a class="sourceLine" id="cb57-16" title="16"></a>
<a class="sourceLine" id="cb57-17" title="17"><span class="co"># predicciones</span></a>
<a class="sourceLine" id="cb57-18" title="18">predictions &lt;-<span class="st"> </span><span class="kw">predict</span>(knn_reg_model, test.data)</a>
<a class="sourceLine" id="cb57-19" title="19"><span class="co"># RMSE: raíz del error cuadrático medio</span></a>
<a class="sourceLine" id="cb57-20" title="20"><span class="kw">RMSE</span>(predictions, test.data<span class="op">$</span>medv)</a></code></pre></div>
<pre><code>## [1] 4.33752</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" title="1"><span class="co"># MAE: error absoluto medio</span></a>
<a class="sourceLine" id="cb59-2" title="2"><span class="kw">MAE</span>(predictions, test.data<span class="op">$</span>medv)</a></code></pre></div>
<pre><code>## [1] 2.805195</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" title="1">df_plot &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">pred =</span> predictions, <span class="dt">real =</span> test.data<span class="op">$</span>medv)</a>
<a class="sourceLine" id="cb61-2" title="2"><span class="kw">ggplot</span>(df_plot, <span class="kw">aes</span>(<span class="dt">x =</span> pred, <span class="dt">y =</span> real)) <span class="op">+</span></a>
<a class="sourceLine" id="cb61-3" title="3"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb61-4" title="4"><span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb61-5" title="5"><span class="st">  </span><span class="kw">xlab</span>(<span class="kw">expression</span>(<span class="kw">hat</span>( y))) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb61-6" title="6"><span class="st">  </span><span class="kw">theme_light</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-29-1.png" width="80%" style="display: block; margin: auto;" /></p>
</div>
<div id="weighted-knn" class="section level2">
<h2><span class="header-section-number">2.4</span> Weighted KNN</h2>
<p>El método de K-Vecinos Más Próximos Ponderados (WKNN: <em>Weighted K-Nearest Neighbors</em>) es una variante del KNN. El principio básico es el mismo: predecir una respuesta en función de los puntos más cercanos de la muestra. La diferencia es que WKNN da más importancia a los más próximos, dentro de los K prefijados. Esto se logra ponderando o dando pesos a los vecinos.</p>
<p>En <code>caret</code> podemos fijar el método <code>kknn</code> que implementa WKNN, tanto para regresión como para clasificación. Ahora debemos optimizar 3 hiperparámetros:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" title="1"><span class="kw">getModelInfo</span>(<span class="st">&quot;kknn&quot;</span>)<span class="op">$</span>kknn<span class="op">$</span>parameters</a></code></pre></div>
<pre><code>##   parameter     class           label
## 1      kmax   numeric Max. #Neighbors
## 2  distance   numeric        Distance
## 3    kernel character          Kernel</code></pre>
<p>El número de vecinos <code>K</code> se corresponde al campo<code>kmax</code>. El campo <code>distance</code> se refiere al order del parámetro <span class="math inline">\(p\)</span> en la <em>Distancia de Minkowski</em>. El <code>kernel</code> es la transformación de los ejes de coordenadas, las opciones son:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" title="1">kerns &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;rectangular&quot;</span>, <span class="st">&quot;triangular&quot;</span>, <span class="st">&quot;epanechnikov&quot;</span>, <span class="st">&quot;biweight&quot;</span>, <span class="st">&quot;triweight&quot;</span>, </a>
<a class="sourceLine" id="cb64-2" title="2">                                 <span class="st">&quot;cos&quot;</span>, <span class="st">&quot;inv&quot;</span>, <span class="st">&quot;gaussian&quot;</span>)</a></code></pre></div>
<p>Veamos un ejemplo con los datos <code>iris</code>. Empezamos fijando un grid o malla de posibles valores de los hiperparámetros a optimizar:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" title="1"><span class="co"># muestra, por eso es necesario una particion balanceada con createDataPartition</span></a>
<a class="sourceLine" id="cb65-2" title="2">df &lt;-<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb65-3" title="3"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb65-4" title="4">train.ID &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(df<span class="op">$</span>Species, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb65-5" title="5"></a>
<a class="sourceLine" id="cb65-6" title="6">train_df &lt;-<span class="st"> </span>df[train.ID, ]</a>
<a class="sourceLine" id="cb65-7" title="7">test_df &lt;-<span class="st"> </span>df[<span class="op">-</span>train.ID, ]</a>
<a class="sourceLine" id="cb65-8" title="8"></a>
<a class="sourceLine" id="cb65-9" title="9"><span class="co"># hacemos una validación cruzada con 10-folds 10 veces</span></a>
<a class="sourceLine" id="cb65-10" title="10">fit_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;repeatedcv&#39;</span>, <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb65-11" title="11"></a>
<a class="sourceLine" id="cb65-12" title="12"><span class="co"># fijamos el grid de valores de los hiperparámetros:</span></a>
<a class="sourceLine" id="cb65-13" title="13">buscar_mejor &lt;-<span class="st"> </span><span class="kw">expand.grid</span>( <span class="dt">kmax =</span>  <span class="dv">3</span><span class="op">:</span><span class="dv">9</span>,</a>
<a class="sourceLine" id="cb65-14" title="14">                             <span class="dt">distance =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb65-15" title="15">                             <span class="dt">kernel =</span> <span class="kw">c</span>(<span class="st">&quot;rectangular&quot;</span>, <span class="co">#standard knn</span></a>
<a class="sourceLine" id="cb65-16" title="16">                                        <span class="st">&quot;triangular&quot;</span>,</a>
<a class="sourceLine" id="cb65-17" title="17">                                        <span class="st">&quot;gaussian&quot;</span>))</a></code></pre></div>
<p>El modelo se ajusta igual a como ya hemos estudiado:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" title="1"><span class="kw">set.seed</span>(<span class="dv">321</span>)</a>
<a class="sourceLine" id="cb66-2" title="2">model.w.knn &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span>.,</a>
<a class="sourceLine" id="cb66-3" title="3">                     <span class="dt">data =</span> train_df,</a>
<a class="sourceLine" id="cb66-4" title="4">                     <span class="dt">method =</span> <span class="st">&quot;kknn&quot;</span>,</a>
<a class="sourceLine" id="cb66-5" title="5">                     <span class="dt">trControl =</span> fit_control,</a>
<a class="sourceLine" id="cb66-6" title="6">                     <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb66-7" title="7">                     <span class="dt">tuneGrid =</span> buscar_mejor)</a>
<a class="sourceLine" id="cb66-8" title="8">model.w.knn</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 120 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (4), scaled (4) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   kmax  distance  kernel       Accuracy   Kappa  
##   3     1         rectangular  0.9600000  0.94000
##   3     1         triangular   0.9558333  0.93375
##   3     1         gaussian     0.9583333  0.93750
##   3     2         rectangular  0.9508333  0.92625
##   3     2         triangular   0.9550000  0.93250
##   3     2         gaussian     0.9483333  0.92250
##   4     1         rectangular  0.9600000  0.94000
##   4     1         triangular   0.9558333  0.93375
##   4     1         gaussian     0.9583333  0.93750
##   4     2         rectangular  0.9491667  0.92375
##   4     2         triangular   0.9550000  0.93250
##   4     2         gaussian     0.9458333  0.91875
##   5     1         rectangular  0.9575000  0.93625
##   5     1         triangular   0.9558333  0.93375
##   5     1         gaussian     0.9583333  0.93750
##   5     2         rectangular  0.9400000  0.91000
##   5     2         triangular   0.9566667  0.93500
##   5     2         gaussian     0.9591667  0.93875
##   6     1         rectangular  0.9575000  0.93625
##   6     1         triangular   0.9558333  0.93375
##   6     1         gaussian     0.9550000  0.93250
##   6     2         rectangular  0.9416667  0.91250
##   6     2         triangular   0.9558333  0.93375
##   6     2         gaussian     0.9558333  0.93375
##   7     1         rectangular  0.9550000  0.93250
##   7     1         triangular   0.9558333  0.93375
##   7     1         gaussian     0.9558333  0.93375
##   7     2         rectangular  0.9433333  0.91500
##   7     2         triangular   0.9575000  0.93625
##   7     2         gaussian     0.9558333  0.93375
##   8     1         rectangular  0.9550000  0.93250
##   8     1         triangular   0.9558333  0.93375
##   8     1         gaussian     0.9558333  0.93375
##   8     2         rectangular  0.9433333  0.91500
##   8     2         triangular   0.9616667  0.94250
##   8     2         gaussian     0.9583333  0.93750
##   9     1         rectangular  0.9550000  0.93250
##   9     1         triangular   0.9558333  0.93375
##   9     1         gaussian     0.9558333  0.93375
##   9     2         rectangular  0.9450000  0.91750
##   9     2         triangular   0.9675000  0.95125
##   9     2         gaussian     0.9608333  0.94125
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were kmax = 9, distance = 2 and kernel
##  = triangular.</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" title="1">model.w.knn<span class="op">$</span>finalModel</a></code></pre></div>
<pre><code>## 
## Call:
## kknn::train.kknn(formula = .outcome ~ ., data = dat, kmax = param$kmax,     distance = param$distance, kernel = as.character(param$kernel))
## 
## Type of response variable: nominal
## Minimal misclassification: 0.01666667
## Best kernel: triangular
## Best k: 9</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" title="1"><span class="kw">plot</span>(model.w.knn)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-34-1.png" width="80%" style="display: block; margin: auto;" />
Observamos que el modelo final—el mejor de acuerdo al <code>Accuracy</code>—es aquel con 8 vecinos, donde el parámetro <span class="math inline">\(p=2\)</span> en la Distancia de Minkowski (equivalente a la Distancia Euclídea) y el kernel es triangular. Esto se obtuvo al probar 7 valores de <code>kmax</code> <span class="math inline">\(\times\)</span> 2 valores de <code>distance</code> <span class="math inline">\(\times\)</span> 3 posibles <code>kernel</code>.</p>
<p>Por otro lado, en lugar de escribir explícitamente el grid de valores a probar, en <code>caret</code> tenemos la opción de realizar una búsqueda aleatoria. Esto podría ser un primer paso para detectar rangos de valores de los hiperparámetros donde luego afinar la búsqueda. Como ejemplo, lo haremos para solo 8 combinaciones de posibles hiperparámetros (en la práctica debemos fijar un mayor número de combinaciones, lo que conlleva un mayor coste computacional):</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" title="1"><span class="co"># random search WKNN</span></a>
<a class="sourceLine" id="cb71-2" title="2">fit_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;repeatedcv&#39;</span>, <span class="dt">number =</span> <span class="dv">10</span>, </a>
<a class="sourceLine" id="cb71-3" title="3">                            <span class="dt">repeats =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb71-4" title="4">                            <span class="dt">search =</span> <span class="st">&quot;random&quot;</span>)</a>
<a class="sourceLine" id="cb71-5" title="5"><span class="kw">set.seed</span>(<span class="dv">321</span>)</a>
<a class="sourceLine" id="cb71-6" title="6">model.w.knn &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span>.,</a>
<a class="sourceLine" id="cb71-7" title="7">                     <span class="dt">data =</span> train_df,</a>
<a class="sourceLine" id="cb71-8" title="8">                     <span class="dt">method =</span> <span class="st">&quot;kknn&quot;</span>,</a>
<a class="sourceLine" id="cb71-9" title="9">                     <span class="dt">trControl =</span> fit_control,</a>
<a class="sourceLine" id="cb71-10" title="10">                     <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb71-11" title="11">                     <span class="dt">tuneLength =</span> <span class="dv">8</span>)</a>
<a class="sourceLine" id="cb71-12" title="12">model.w.knn</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 120 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (4), scaled (4) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   kmax  distance   kernel        Accuracy   Kappa  
##    1    0.8715102  biweight      0.9566667  0.93500
##   10    1.9213770  cos           0.9658333  0.94875
##   22    0.6048966  epanechnikov  0.9541667  0.93125
##   25    1.7765156  triangular    0.9683333  0.95250
##   28    1.8982141  inv           0.9666667  0.95000
##   37    0.1352608  rectangular   0.9491667  0.92375
##   37    2.2990154  inv           0.9725000  0.95875
##   40    1.2107700  triangular    0.9583333  0.93750
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were kmax = 37, distance = 2.299015
##  and kernel = inv.</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" title="1"><span class="kw">plot</span>(model.w.knn)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-35-1.png" width="80%" style="display: block; margin: auto;" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="DA.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
