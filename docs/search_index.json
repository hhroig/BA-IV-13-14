[["index.html", "Introducción al Aprendizaje Supervisado 1 Introducción", " Introducción al Aprendizaje Supervisado Harold A. Hernández-Roig (hahernan@est-econ.uc3m.es) 25-26 Febrero 2021 1 Introducción ¡Bienvenidos a las sesiones de Aprendizaje Supervisado ! La bibliografía fundamental para estas dos sesiones es el libro de (James et al. 2013). En el mismo, podrán encontrar las principales ideas ya estudiadas, así como ejemplos prácticos sencillos en R (ver el paquete ISLR (James et al. 2017)). También recomendamos los libros (Rebala, Ravi, and Churiwala 2019) y (Burger 2018), como material complementario. Los paquetes de R empleados para ajustar los algoritmos supervisados K-Nearest-Neighbors, Linear Discriminant Analysis (LDA), y Quadratic Discriminant Analysis (QDA); son caret(Kuhn 2020) y class(Ripley 2020). Para el paquete caret, se recomienda además revisar la documentación en topepo.github.io/caret. La organización del documento corresponde, en general, a la seguida durante las prácticas. Have fun! :) "],["k-vecinos-más-próximos.html", "2 K - Vecinos más Próximos 2.1 Clasificación con el paquete class 2.2 El paquete caret 2.3 Regresión 2.4 Weighted KNN 2.5 Procesamiento en paralelo", " 2 K - Vecinos más Próximos Comenzamos con uno de los algoritmos más sencillos e intuitivos para regresión y clasificación: los K - Vecinos Más Próximos (KNN: K-Nearest Neighbors). Nuestro bautizo será con un problema de clasificación, empleando los paquetes class y caret. Luego, pasaremos a un problema sencillo de regresión, esta vez usando solo el paquete caret. Trataremos los problemas de ajuste y validación del modelo, usando técnicas de remuestreo. 2.1 Clasificación con el paquete class El problema inicial está relacionado con la clasificación de la especie de flor Irissetosa, virginica y versicolora partir de mediciones sus pétalos y sépalos. Estos datos fueron recogidos por Ronald Fisher con el objetivo de cuantificar la variación morfológica de la flor. Actualmente están disponibles en diversas plataformas. En R es uno de los datos que vienen de base (iris). En la Tabla 2.1 representamos una muestra del dataset, que en su totalidad consiste de 50 observaciones de cada una de las 3 especies. Como todo estudio, debemos comenzar por un análisis descriptivo de la muestra. Table 2.1: Estructura del dataset Iris Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa Cargamos las librerías necesarias para llevar a cabo el estudio. Luego inspeccionamos los datos. # ya conocemos esta: library(tidyverse) # para usar knn: library(class) # esta es nueva para nosotros: library(GGally) df &lt;- data(iris) # cargar datos summary(iris) # un breve descriptivo ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## # ver el balance de la muestra, según las clases prop.table(table(iris$Species)) ## ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 # visualización p1 &lt;- ggpairs(iris, aes(colour = Species, alpha = 0.2), columns = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;)) + theme_bw() p1 p2 &lt;- ggpairs(iris, aes(colour = Species, alpha = 0.2), lower=list(combo=wrap(&quot;facethist&quot;, bins=round(sqrt(50))))) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) p2 De estos análisis observamos, por ejemplo, que 2 de las 4 variablesPetal.Width y Petal.Lengthparecen separar bastante bien las 3 especies, y que la muestra está muy bien balanceada. Antes de pasar a ajustar nuestro modelo, debemos preprocesar la muestra. El algoritmo KNN es muy sensible a la escala de los datos, por ejemplo, podría favorecer distancias entre elementos con valores más grandes. Una forma sencilla de estandarizar o escalar los datos es usando: iris.scl &lt;- scale(iris[,1:4]) Otro elemento importante es la validación del modelo que ajustemos: ¿cómo y con qué muestra medir la precisión? Por lo pronto, fijaremos aleatoriamente un 20% de los datos para calcular la tasa de error. # set de índices para entrenar-validar (80% - 20%) set.seed(123) train.ID &lt;- sample(1:nrow(iris), 0.8 * nrow(iris)) # matriz de diseño para entrenar X.train &lt;- iris.scl[train.ID,1:4] # matriz de diseño para testear X.test &lt;- iris.scl[-train.ID,1:4] # respuesta (categórica) entrenamiento Y &lt;- iris[train.ID,5] # respuesta (categórica) test Y.test &lt;- iris[-train.ID,5] Usando la función knn podemos predecir las clases de los datos en X.test. Otro problema es cómo seleccionar la cantidad de vecinos k apropiada. Una regla de pulgar (thumb rule) es fijar \\(k = \\sqrt{n_{train}}\\). Para analizar la precisión del modelo creamos la matriz de confusión y calculamos la tasa de error correspondiente para el conjunto de datos test. # KNN pr &lt;- knn(X.train, X.test, cl=Y, k = round(sqrt(nrow(X.train)))) # matriz de confusión tab &lt;- table(pr,Y.test) Table 2.2: Matriz de Confusión - KNN Iris setosa versicolor virginica setosa 10 0 0 versicolor 0 14 0 virginica 0 1 5 # tasa de error test test.error &lt;- sum(pr != Y.test)/sum(tab) test.error ## [1] 0.03333333 La tasa de error test es bastante baja, además indica que se clasifican bien el \\(\\approx 97\\%\\) de las observaciones. Veamos ahora qué pasa al variar el número de vecinos K. test.error &lt;- data.frame() for (K in seq(1, 120, by = 5)) { # KNN pr &lt;- knn(X.train,X.test,cl=Y,k=K) # matriz de confusion tab &lt;- table(pr,Y.test) # tasa de error test test.error &lt;- rbind(test.error, data.frame(Tasa.Error = sum(pr != Y.test)/sum(tab), K)) } ggplot(test.error, aes(x = K, y = Tasa.Error)) + geom_point() + geom_line() + ylab(&quot;Tasa de Error (test)&quot;) + xlab(&quot;K: número de vecinos&quot;) + theme_light() Se representa la tasa de error al aumentar el número de vecinos. Los saltos de la curva son resultado del pequeño tamaño de la muestra test. Como cualquier otro modelo de machine learning, el interés está en seleccionar el nivel de flexibilidad (número de vecinos) que mejore la clasificación inténtalo! 2.2 El paquete caret El paquete caret (Classification And REgression Training) es uno de los más populares para entrenar modelos de machine learning. Contiene una interfaz uniforme para la mayoría de los algoritmos que se tratan en este curso y, en particular, los 3 que veremos en estas sesiones. Las ventajas del paquete son que permite hacer: partición de los datos pre-procesado de los datos selección de variables ajuste del modelo usando remuestreo estimación de la importancia/relevancia de las variables Más información disponible en topepo.github.io/caret. 2.2.1 Visualización Seguiremos con los datos iris. El paso inicial: análisis descriptivo y visualización de los datos podríamos obviarlo pero a modo didáctico reproducimos el mismo análisis, esta vez usando la función featurePlot de caret. library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Diagramas de dispersión: featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;pairs&quot;, ## Add a key at the top auto.key = list(columns = 3)) Densidades estimadas: featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;density&quot;, ## Pass in options to xyplot() to ## make it prettier scales = list(x = list(relation=&quot;free&quot;), y = list(relation=&quot;free&quot;)), adjust = 1.5, pch = &quot;|&quot;, layout = c(4, 1), auto.key = list(columns = 3)) Diagramas de cajas: featurePlot(x = iris[, 1:4], y = iris$Species, plot = &quot;box&quot;, ## Pass in options to bwplot() scales = list(y = list(relation=&quot;free&quot;), x = list(rot = 90)), layout = c(4,1 ), auto.key = list(columns = 2)) 2.2.2 Clasificación con KNN Necesitamos extraer una muestra independiente (test) para probar el modelo, una vez ajustado. Ahora usaremos la función createDataPartition, que permite hacer la partición teniendo en cuenta la variable respuesta. Esto es esencial para mantener el balance de la muestra. # creamos una partición test df &lt;- iris set.seed(123) train.ID &lt;- createDataPartition(df$Species, p = 0.8, list = FALSE) train_df &lt;- df[train.ID, ] test_df &lt;- df[-train.ID, ] Para ajustar el modelo usaremos la función train, que permite: evaluar, usando remuestreo, el efecto de distintos parámetros en la precisión del modelo; escoger el modelo óptimo, de acuerdo a los parámetros probados; estimar la precisión del modelo, de acuerdo a diferentes medidas. Actualmente hay unos \\(\\approx 238\\) modelos disponibles. Nosotros empezaremos probando el knn, pero antes tenemos que especificar el método de remuestreo, usando la función trainControl. Con esta función, podemos fijar una validación cruzada k-Fold o leave-one-out (LOOCV). También están disponibles las opciones bootstrap y k-Fold repetitivo. En este ejemplo, hemos fijado un k-Fold con 10 hojas. Además, hacemos el escalado de las variables dentro del propio algoritmo, usando la opción preProcess. Finalmente, le decimos al algoritmo que intente 10 valores diferentes para escoger el número de vecinos óptimo, usando la opción tuneLength # primeros pasos con la validación cruzada... fit_control &lt;- trainControl(method=&#39;cv&#39;, number = 10) model_knn_iris &lt;- train(Species ~., data = train_df, method = &quot;knn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10) model_knn_iris ## k-Nearest Neighbors ## ## 120 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## Pre-processing: centered (4), scaled (4) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.9666667 0.9500 ## 7 0.9583333 0.9375 ## 9 0.9750000 0.9625 ## 11 0.9583333 0.9375 ## 13 0.9583333 0.9375 ## 15 0.9583333 0.9375 ## 17 0.9583333 0.9375 ## 19 0.9416667 0.9125 ## 21 0.9500000 0.9250 ## 23 0.9333333 0.9000 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 9. plot(model_knn_iris) Podemos ver en el resumen el número óptimo de vecinos (entre los valores probados) del modelo final. En el gráfico, vemos cómo varía el accuracy en función del número de vecinos. La tabla de confusión y medidas de precisión para los datos test: # hagamos las predicciones del conjunto de prueba prediction_knn_iris &lt;- predict(model_knn_iris, newdata = test_df) confusionMatrix(prediction_knn_iris, reference = test_df$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 10 2 ## virginica 0 0 8 ## ## Overall Statistics ## ## Accuracy : 0.9333 ## 95% CI : (0.7793, 0.9918) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : 8.747e-12 ## ## Kappa : 0.9 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.8000 ## Specificity 1.0000 0.9000 1.0000 ## Pos Pred Value 1.0000 0.8333 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9091 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3333 0.2667 ## Detection Prevalence 0.3333 0.4000 0.2667 ## Balanced Accuracy 1.0000 0.9500 0.9000 Intentemos ahora fijar las cantidades de vecinos a probar. También cambiamos el método de remuestreo # definimos el grid: some_k &lt;- expand.grid(k = 1:15) # k-fold CV pero con repeticiones fit_control1 &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 10, # número de folds repeats = 5 ) # repeticiones # bootstrap fit_control2 &lt;- trainControl( method = &quot;boot&quot;, number = 10) # número de muestras bootstrap # LOOCV fit_control3 &lt;- trainControl( method = &quot;LOOCV&quot;) model2_knn_iris &lt;- train(Species ~., data = train_df, method = &quot;knn&quot;, trControl = fit_control2, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = some_k) model2_knn_iris ## k-Nearest Neighbors ## ## 120 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## Pre-processing: centered (4), scaled (4) ## Resampling: Bootstrapped (10 reps) ## Summary of sample sizes: 120, 120, 120, 120, 120, 120, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.9481717 0.9211545 ## 2 0.9475159 0.9196833 ## 3 0.9453882 0.9165257 ## 4 0.9599231 0.9393154 ## 5 0.9665503 0.9492382 ## 6 0.9686637 0.9524826 ## 7 0.9688968 0.9528083 ## 8 0.9596084 0.9388259 ## 9 0.9647549 0.9465745 ## 10 0.9600366 0.9392605 ## 11 0.9624756 0.9431153 ## 12 0.9555270 0.9326274 ## 13 0.9623479 0.9429620 ## 14 0.9439143 0.9151811 ## 15 0.9438559 0.9150458 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 7. plot(model2_knn_iris) # hagamos las predicciones del conjunto de prueba prediction_knn_iris2 &lt;- predict(model2_knn_iris, newdata = test_df) confusionMatrix(prediction_knn_iris2, reference = test_df$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 10 2 ## virginica 0 0 8 ## ## Overall Statistics ## ## Accuracy : 0.9333 ## 95% CI : (0.7793, 0.9918) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : 8.747e-12 ## ## Kappa : 0.9 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.8000 ## Specificity 1.0000 0.9000 1.0000 ## Pos Pred Value 1.0000 0.8333 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9091 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3333 0.2667 ## Detection Prevalence 0.3333 0.4000 0.2667 ## Balanced Accuracy 1.0000 0.9500 0.9000 2.2.3 Importancia de las variables Para KNN no tenemos un método que permita determinar la relevancia de cada predictor. Por ejemplo, en mínimos cuadrados, sí se puede conducir un test para determinar si cada coeficiente \\(\\beta_i\\) del modelo es significativamente distinto de cero. Aún así, caret incorpora la función varImp que da una medida de importancia de cada predictor del problema de clasificación o regresión. varImp(model_knn_iris) ## ROC curve variable importance ## ## variables are sorted by maximum importance across the classes ## setosa versicolor virginica ## Petal.Width 100.00 100.00 100.0 ## Petal.Length 100.00 100.00 100.0 ## Sepal.Length 90.80 72.07 90.8 ## Sepal.Width 56.32 56.32 0.0 Aunque esto no debe usarse como método de selección de variables, sí motiva el estudio del problema al disminuir la dimensión \\(p = 4\\). Por ejemplo, veamos la precisión del modelo al dejar solo Petal.Length y Petal.Width. # seleccionamos los predictores que queremos y la respuesta df_petal &lt;- iris[,c(&quot;Petal.Length&quot;, &quot;Petal.Width&quot;, &quot;Species&quot;)] train_df_petal &lt;- df_petal[train.ID, ] test_df_petal &lt;- df_petal[-train.ID, ] # el modelo... fit_control &lt;- trainControl(method=&#39;cv&#39;, number = 10) model_knn_petal &lt;- train(Species ~., data = train_df_petal, method = &quot;knn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 20) model_knn_petal ## k-Nearest Neighbors ## ## 120 samples ## 2 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## Pre-processing: centered (2), scaled (2) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.9666667 0.9500 ## 7 0.9666667 0.9500 ## 9 0.9666667 0.9500 ## 11 0.9666667 0.9500 ## 13 0.9666667 0.9500 ## 15 0.9666667 0.9500 ## 17 0.9583333 0.9375 ## 19 0.9666667 0.9500 ## 21 0.9583333 0.9375 ## 23 0.9666667 0.9500 ## 25 0.9666667 0.9500 ## 27 0.9666667 0.9500 ## 29 0.9666667 0.9500 ## 31 0.9666667 0.9500 ## 33 0.9750000 0.9625 ## 35 0.9833333 0.9750 ## 37 0.9750000 0.9625 ## 39 0.9833333 0.9750 ## 41 0.9750000 0.9625 ## 43 0.9666667 0.9500 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 39. plot(model_knn_petal) # hagamos las predicciones del conjunto de prueba prediction_knn_petal &lt;- predict(model_knn_petal, newdata = test_df_petal) confusionMatrix(prediction_knn_petal, reference = test_df_petal$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 10 3 ## virginica 0 0 7 ## ## Overall Statistics ## ## Accuracy : 0.9 ## 95% CI : (0.7347, 0.9789) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : 1.665e-10 ## ## Kappa : 0.85 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.7000 ## Specificity 1.0000 0.8500 1.0000 ## Pos Pred Value 1.0000 0.7692 1.0000 ## Neg Pred Value 1.0000 1.0000 0.8696 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3333 0.2333 ## Detection Prevalence 0.3333 0.4333 0.2333 ## Balanced Accuracy 1.0000 0.9250 0.8500 Pero, ¿cómo visualizar las fronteras de decisión del método? Ahora que \\(p = 2\\), podemos representar esto en el plano usando la siguiente función: decision_bound = function(train_df_in, test_df_in, model_in){ # plot decision boundary for iris[,c(&quot;Petal.Length&quot;, &quot;Petal.Width&quot;, &quot;Species&quot;)] require(MASS) require(caret) require(ggplot2) require(gridExtra) # Paso 1: crear un grid de valores desde min a max de ambos predictores pl = seq(min(train_df_in$Petal.Length), max(train_df_in$Petal.Length), length.out = 80) pw = seq(min(train_df_in$Petal.Width), max(train_df_in$Petal.Width), length.out = 80) lgrid &lt;- expand.grid(Petal.Length=pl, Petal.Width=pw) # Paso 2: obtener las predicciones tanto para el grid como para el test modelPredGrid &lt;- predict(model_in, newdata=lgrid) train_df_in$Pred.Class &lt;- predict(model_in, newdata = train_df_in) test_df_in$Pred.Class &lt;- predict(model_in, newdata = test_df_in) # Paso 3: ggplot con la funcion contour gg1 &lt;- ggplot(data=lgrid) + stat_contour(aes(x=Petal.Length, y=Petal.Width, z=as.numeric(modelPredGrid)), bins=2) + geom_point(aes(x=Petal.Length, y=Petal.Width, colour=modelPredGrid), alpha=0.1) + labs(colour = &quot;Clases&quot;) + ggtitle(&quot;Train&quot;) + geom_point(data=train_df_in, aes(x=Petal.Length, y=Petal.Width, colour=Species), size=5, shape=1) + theme_light() gg2 &lt;- ggplot(data=lgrid) + stat_contour(aes(x=Petal.Length, y=Petal.Width, z=as.numeric(modelPredGrid)), bins=2) + geom_point(aes(x=Petal.Length, y=Petal.Width, colour=modelPredGrid), alpha=0.1) + labs(colour = &quot;Clases&quot;) + ggtitle(&quot;Test&quot;) + geom_point(data=test_df_in, aes(x=Petal.Length, y=Petal.Width, colour=Species), size=5, shape=1) + theme_light() grid.arrange(gg1, gg2, ncol=1, nrow=2) } Así que aplicando esto a nuestros datos de entrenamiento (o los del test) obtenemos las fronteras de decisión: # fronteras de decisión, usando la nueva función decision_bound(train_df_petal, test_df_petal, model_knn_petal) ## Loading required package: MASS ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## Loading required package: gridExtra ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine 2.3 Regresión Abordamos ahora el problema de regresión con KNN, o sea, la respuesta es cuantitativa-continua. Seguimos usando el paquete caret que tiene implementado el algoritmo y ofrece facilidades para el preprocesado de los datos y la validación del modelo. Particularmente, atacaremos el problema de predecir el precio medio de las viviendas (medv) en los suburbios de Boston, usando otras 13 variables predictoras. library(MASS) library(caret) library(ggplot2) # cargar e inspeccionar los datos # para detalles sobre las variables predictoras: # ?Boston data(Boston) str(Boston) ## &#39;data.frame&#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... summary(Boston) ## crim zn indus chas ## Min. : 0.00632 Min. : 0.00 Min. : 0.46 Min. :0.00000 ## 1st Qu.: 0.08205 1st Qu.: 0.00 1st Qu.: 5.19 1st Qu.:0.00000 ## Median : 0.25651 Median : 0.00 Median : 9.69 Median :0.00000 ## Mean : 3.61352 Mean : 11.36 Mean :11.14 Mean :0.06917 ## 3rd Qu.: 3.67708 3rd Qu.: 12.50 3rd Qu.:18.10 3rd Qu.:0.00000 ## Max. :88.97620 Max. :100.00 Max. :27.74 Max. :1.00000 ## nox rm age dis ## Min. :0.3850 Min. :3.561 Min. : 2.90 Min. : 1.130 ## 1st Qu.:0.4490 1st Qu.:5.886 1st Qu.: 45.02 1st Qu.: 2.100 ## Median :0.5380 Median :6.208 Median : 77.50 Median : 3.207 ## Mean :0.5547 Mean :6.285 Mean : 68.57 Mean : 3.795 ## 3rd Qu.:0.6240 3rd Qu.:6.623 3rd Qu.: 94.08 3rd Qu.: 5.188 ## Max. :0.8710 Max. :8.780 Max. :100.00 Max. :12.127 ## rad tax ptratio black ## Min. : 1.000 Min. :187.0 Min. :12.60 Min. : 0.32 ## 1st Qu.: 4.000 1st Qu.:279.0 1st Qu.:17.40 1st Qu.:375.38 ## Median : 5.000 Median :330.0 Median :19.05 Median :391.44 ## Mean : 9.549 Mean :408.2 Mean :18.46 Mean :356.67 ## 3rd Qu.:24.000 3rd Qu.:666.0 3rd Qu.:20.20 3rd Qu.:396.23 ## Max. :24.000 Max. :711.0 Max. :22.00 Max. :396.90 ## lstat medv ## Min. : 1.73 Min. : 5.00 ## 1st Qu.: 6.95 1st Qu.:17.02 ## Median :11.36 Median :21.20 ## Mean :12.65 Mean :22.53 ## 3rd Qu.:16.95 3rd Qu.:25.00 ## Max. :37.97 Max. :50.00 Veamos las relaciones entre predictores y la variable respuesta (en este ejemplo, solo hemos representado algunas). # ver correlaciones y posibles relaciones: # todos los predictores: # ggpairs(Boston, ggplot2::aes(y = medv, alpha = 0.2)) + theme_light() # algunos predictores: ggpairs(Boston[, c(&quot;lstat&quot;, &quot;age&quot;, &quot;rad&quot;, &quot;rm&quot;, &quot;ptratio&quot;, &quot;medv&quot;)]) + theme_light() Por ejemplo, es notable la relación lineal entre medv y las variables predictoras rm y lstat. Estas dos corresponden al número medio de habitaciones por vivienda y al ínfimo estatus poblacional, respectivamente. Ajustemos un modelo de regresión, usando todas las variables y el algoritmo KNN. # Split the data into training and test set set.seed(123) train.ID &lt;- createDataPartition(Boston$medv, p = 0.8, list = FALSE) train.data &lt;- Boston[train.ID, ] test.data &lt;- Boston[-train.ID, ] # Fit the model on the training set set.seed(123) knn_reg_model &lt;- train( medv~., data = train.data, method = &quot;knn&quot;, trControl = trainControl(&quot;cv&quot;, number = 10), preProcess = c(&quot;center&quot;,&quot;scale&quot;), tuneLength = 20 ) knn_reg_model ## k-Nearest Neighbors ## ## 407 samples ## 13 predictor ## ## Pre-processing: centered (13), scaled (13) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 366, 367, 366, 366, 366, 366, ... ## Resampling results across tuning parameters: ## ## k RMSE Rsquared MAE ## 5 4.446665 0.7748420 2.877705 ## 7 4.513433 0.7760351 2.931696 ## 9 4.510659 0.7822810 2.961313 ## 11 4.557329 0.7806128 2.997708 ## 13 4.589419 0.7769459 3.031409 ## 15 4.683314 0.7715242 3.108591 ## 17 4.727456 0.7658943 3.133048 ## 19 4.794512 0.7608152 3.190837 ## 21 4.871432 0.7555146 3.242578 ## 23 4.896989 0.7568145 3.256753 ## 25 4.982893 0.7505900 3.318786 ## 27 5.035171 0.7497412 3.356345 ## 29 5.099070 0.7472969 3.412470 ## 31 5.196655 0.7375974 3.477003 ## 33 5.260674 0.7329066 3.507057 ## 35 5.320022 0.7310458 3.543261 ## 37 5.393032 0.7258230 3.592311 ## 39 5.457338 0.7205076 3.643305 ## 41 5.500014 0.7178285 3.669332 ## 43 5.561854 0.7125909 3.713330 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was k = 5. # Plot model error RMSE vs different values of k plot(knn_reg_model) Ahora, lo que nos interesa es disminuir el Error Cuadrático Medio: # predicciones predictions &lt;- predict(knn_reg_model, test.data) # RMSE: raíz del error cuadrático medio RMSE(predictions, test.data$medv) ## [1] 4.762122 # MAE: error absoluto medio MAE(predictions, test.data$medv) ## [1] 3.050101 Si representamos las predicciones y los valores reales de la variable medv, esperamos que los puntos estén muy cercanos a la recta \\(Y = X\\). df_plot &lt;- data.frame(pred = predictions, real = test.data$medv) ggplot(df_plot, aes(x = pred, y = real)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlab(expression(hat( y))) + ylab(&quot;y&quot;) + theme_light() ¿Será posible mejorar esto? ¿Son todas las variables realmente necesarias? ¿Un grid con valores más pequeños a \\(K = 5\\) podría resultar mejor? Veamos qué tal es el ajuste y las predicciones si nos limitamos a unas pocas variables predictoras. # importancia de las variables según impacto en la predicción varImp(knn_reg_model) ## loess r-squared variable importance ## ## Overall ## lstat 100.00 ## rm 84.11 ## ptratio 35.71 ## indus 33.76 ## crim 30.80 ## tax 30.16 ## black 25.33 ## nox 24.24 ## age 21.19 ## rad 18.94 ## dis 15.00 ## zn 14.23 ## chas 0.00 # seleccionemos solo algunas variables: boston &lt;- Boston[, c(&quot;lstat&quot;, &quot;rm&quot;, &quot;ptratio&quot;, &quot;medv&quot;)] # ajustamos el modelo en el nuevo diseño train.data &lt;- boston[train.ID, ] test.data &lt;- boston[-train.ID, ] set.seed(123) knn_reg_model &lt;- train( medv~., data = train.data, method = &quot;knn&quot;, trControl = trainControl(&quot;cv&quot;, number = 10), preProcess = c(&quot;center&quot;,&quot;scale&quot;), tuneGrid = expand.grid(k = 1:15) ) # Veamos si el modelo ha mejorado algo: # predicciones predictions &lt;- predict(knn_reg_model, test.data) # RMSE: raíz del error cuadrático medio RMSE(predictions, test.data$medv) ## [1] 4.33752 # MAE: error absoluto medio MAE(predictions, test.data$medv) ## [1] 2.805195 df_plot &lt;- data.frame(pred = predictions, real = test.data$medv) ggplot(df_plot, aes(x = pred, y = real)) + geom_point() + geom_abline(slope = 1, intercept = 0) + xlab(expression(hat( y))) + ylab(&quot;y&quot;) + theme_light() 2.4 Weighted KNN El método de K-Vecinos Más Próximos Ponderados (WKNN: Weighted K-Nearest Neighbors) es una variante del KNN. El principio básico es el mismo: predecir una respuesta en función de los puntos más cercanos de la muestra. La diferencia es que WKNN da más importancia a los más próximos, dentro de los K prefijados. Esto se logra ponderando o dando pesos a los vecinos. En caret podemos fijar el método kknn que implementa WKNN, tanto para regresión como para clasificación. Ahora debemos optimizar 3 hiperparámetros: getModelInfo(&quot;kknn&quot;)$kknn$parameters ## parameter class label ## 1 kmax numeric Max. #Neighbors ## 2 distance numeric Distance ## 3 kernel character Kernel El número de vecinos K se corresponde al campokmax. El campo distance se refiere al order del parámetro \\(p\\) en la Distancia de Minkowski. El kernel es la transformación de los ejes de coordenadas, las opciones son: kerns &lt;- c(&quot;rectangular&quot;, &quot;triangular&quot;, &quot;epanechnikov&quot;, &quot;biweight&quot;, &quot;triweight&quot;, &quot;cos&quot;, &quot;inv&quot;, &quot;gaussian&quot;) Veamos un ejemplo con los datos iris. Empezamos fijando un grid o malla de posibles valores de los hiperparámetros a optimizar: # muestra, por eso es necesario una particion balanceada con createDataPartition df &lt;- iris set.seed(123) train.ID &lt;- createDataPartition(df$Species, p = 0.8, list = FALSE) train_df &lt;- df[train.ID, ] test_df &lt;- df[-train.ID, ] # hacemos una validación cruzada con 10-folds 10 veces fit_control &lt;- trainControl(method=&#39;repeatedcv&#39;, number = 10, repeats = 10) # fijamos el grid de valores de los hiperparámetros: buscar_mejor &lt;- expand.grid( kmax = 3:9, distance = 1:2, kernel = c(&quot;rectangular&quot;, #standard knn &quot;triangular&quot;, &quot;gaussian&quot;)) El modelo se ajusta igual a como ya hemos estudiado: set.seed(321) model.w.knn &lt;- train(Species ~., data = train_df, method = &quot;kknn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = buscar_mejor) model.w.knn ## k-Nearest Neighbors ## ## 120 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## Pre-processing: centered (4), scaled (4) ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... ## Resampling results across tuning parameters: ## ## kmax distance kernel Accuracy Kappa ## 3 1 rectangular 0.9600000 0.94000 ## 3 1 triangular 0.9558333 0.93375 ## 3 1 gaussian 0.9583333 0.93750 ## 3 2 rectangular 0.9508333 0.92625 ## 3 2 triangular 0.9550000 0.93250 ## 3 2 gaussian 0.9483333 0.92250 ## 4 1 rectangular 0.9600000 0.94000 ## 4 1 triangular 0.9558333 0.93375 ## 4 1 gaussian 0.9583333 0.93750 ## 4 2 rectangular 0.9491667 0.92375 ## 4 2 triangular 0.9550000 0.93250 ## 4 2 gaussian 0.9458333 0.91875 ## 5 1 rectangular 0.9575000 0.93625 ## 5 1 triangular 0.9558333 0.93375 ## 5 1 gaussian 0.9583333 0.93750 ## 5 2 rectangular 0.9400000 0.91000 ## 5 2 triangular 0.9566667 0.93500 ## 5 2 gaussian 0.9591667 0.93875 ## 6 1 rectangular 0.9575000 0.93625 ## 6 1 triangular 0.9558333 0.93375 ## 6 1 gaussian 0.9550000 0.93250 ## 6 2 rectangular 0.9416667 0.91250 ## 6 2 triangular 0.9558333 0.93375 ## 6 2 gaussian 0.9558333 0.93375 ## 7 1 rectangular 0.9550000 0.93250 ## 7 1 triangular 0.9558333 0.93375 ## 7 1 gaussian 0.9558333 0.93375 ## 7 2 rectangular 0.9433333 0.91500 ## 7 2 triangular 0.9575000 0.93625 ## 7 2 gaussian 0.9558333 0.93375 ## 8 1 rectangular 0.9550000 0.93250 ## 8 1 triangular 0.9558333 0.93375 ## 8 1 gaussian 0.9558333 0.93375 ## 8 2 rectangular 0.9433333 0.91500 ## 8 2 triangular 0.9616667 0.94250 ## 8 2 gaussian 0.9583333 0.93750 ## 9 1 rectangular 0.9550000 0.93250 ## 9 1 triangular 0.9558333 0.93375 ## 9 1 gaussian 0.9558333 0.93375 ## 9 2 rectangular 0.9450000 0.91750 ## 9 2 triangular 0.9675000 0.95125 ## 9 2 gaussian 0.9608333 0.94125 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were kmax = 9, distance = 2 and kernel ## = triangular. model.w.knn$finalModel ## ## Call: ## kknn::train.kknn(formula = .outcome ~ ., data = dat, kmax = param$kmax, distance = param$distance, kernel = as.character(param$kernel)) ## ## Type of response variable: nominal ## Minimal misclassification: 0.01666667 ## Best kernel: triangular ## Best k: 9 plot(model.w.knn) Observamos que el modelo finalel mejor de acuerdo al Accuracyes aquel con 8 vecinos, donde el parámetro \\(p=2\\) en la Distancia de Minkowski (equivalente a la Distancia Euclídea) y el kernel es triangular. Esto se obtuvo al probar 7 valores de kmax \\(\\times\\) 2 valores de distance \\(\\times\\) 3 posibles kernel. Por otro lado, en lugar de escribir explícitamente el grid de valores a probar, en caret tenemos la opción de realizar una búsqueda aleatoria. Esto podría ser un primer paso para detectar rangos de valores de los hiperparámetros donde luego afinar la búsqueda. Como ejemplo, lo haremos para solo 8 combinaciones de posibles hiperparámetros (en la práctica debemos fijar un mayor número de combinaciones, lo que conlleva un mayor coste computacional): # random search WKNN fit_control &lt;- trainControl(method=&#39;repeatedcv&#39;, number = 10, repeats = 10, search = &quot;random&quot;) set.seed(321) model.w.knn &lt;- train(Species ~., data = train_df, method = &quot;kknn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 8) model.w.knn ## k-Nearest Neighbors ## ## 120 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## Pre-processing: centered (4), scaled (4) ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... ## Resampling results across tuning parameters: ## ## kmax distance kernel Accuracy Kappa ## 1 0.8715102 biweight 0.9566667 0.93500 ## 10 1.9213770 cos 0.9658333 0.94875 ## 22 0.6048966 epanechnikov 0.9541667 0.93125 ## 25 1.7765156 triangular 0.9683333 0.95250 ## 28 1.8982141 inv 0.9666667 0.95000 ## 37 0.1352608 rectangular 0.9491667 0.92375 ## 37 2.2990154 inv 0.9725000 0.95875 ## 40 1.2107700 triangular 0.9583333 0.93750 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were kmax = 37, distance = 2.299015 ## and kernel = inv. plot(model.w.knn) 2.5 Procesamiento en paralelo No cambia la estructura de entrenamiento, solo es necesario fijar el número de núcleos. En este caso tenemos un ejemplo donde comparamos una ejecución sin paralelizar y otra que emplea 8 núcleos en paralelo con el paquete doParallel. El número de hiperparámetros es 16. # random search WKNN fit_control &lt;- trainControl(method=&#39;repeatedcv&#39;, number = 16, repeats = 10, search = &quot;random&quot;) ## No-Parallel ---- tic(&quot;wknn-train&quot;) set.seed(321) model.w.knn &lt;- train(Species ~., data = train_df, method = &quot;kknn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 16) time_wknn &lt;- toc() ## Parallel ---- library(doParallel) # Número de núcleos: cl &lt;- makePSOCKcluster(8) # Registrar: registerDoParallel(cl) # Misma estructura que hasta ahora: tic(&quot;wknn-train_para&quot;) set.seed(321) model.w.knn &lt;- train(Species ~., data = train_df, method = &quot;kknn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 16) time_wknn_para &lt;- toc() stopImplicitCluster() "],["DA.html", "3 Análisis Discriminante 3.1 Análisis Discriminante Lineal 3.2 Análisis Discriminante Cuadrático 3.3 Análisis Discriminante Regularizado", " 3 Análisis Discriminante En esta sección nos concentramos en el problema de clasificación. Particularmente, estudiaremos los métodos de Análisis Discriminante Lineal (LDA), Cuadrático (QDA) y Regularizado (RDA). Usaremos el conjunto de datos Default del paquete ISLR: library(caret) library(ISLR) data(&quot;Default&quot;) head(Default, 10) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 ## 7 No No 825.5133 24905.227 ## 8 No Yes 808.6675 17600.451 ## 9 No No 1161.0579 37468.529 ## 10 No No 0.0000 29275.268 str(Default) ## &#39;data.frame&#39;: 10000 obs. of 4 variables: ## $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ... ## $ balance: num 730 817 1074 529 786 ... ## $ income : num 44362 12106 31767 35704 38463 ... El objetivo es predecir si un sujeto de la muestra fallará en el pago de su tarjeta de crédito. Por tanto, la variable respuesta es default, categórica con solo los niveles Yes y No. Tenemos información sobre el balance mensual de crédito en balance, el salario anual en income y si es estudiante o no en student. Solo un \\(3\\%\\) de la muestra es de la clase Yes, así que está bastante desbalanceada. summary(Default) ## default student balance income ## No :9667 No :7056 Min. : 0.0 Min. : 772 ## Yes: 333 Yes:2944 1st Qu.: 481.7 1st Qu.:21340 ## Median : 823.6 Median :34553 ## Mean : 835.4 Mean :33517 ## 3rd Qu.:1166.3 3rd Qu.:43808 ## Max. :2654.3 Max. :73554 # ver el balance de la muestra prop.table(table(Default$default)) ## ## No Yes ## 0.9667 0.0333 Nos concentramos en predecir default a partir de las variables predictoras balance e income. En el siguiente diagrama de dispersión se observa cierto solapamiento entre las clases a predecir, pero una clara diferenciación de acuerdo a la variable balance. library(ggplot2) library(gridExtra) ## Scatter plot con densidades ---- plot.2d &lt;- ggplot(Default, aes(x = balance, y = income, group = default)) + geom_point(aes(shape = default, color = default), alpha = 0.5) + theme_light() # Empty plot empty &lt;- ggplot()+geom_point(aes(1,1), color=&quot;white&quot;) + theme( plot.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.border = element_blank(), panel.background = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank() ) # arriba dens.balance &lt;- ggplot(Default, aes(x = balance, group = default)) + geom_density(aes(color = default, fill = default), alpha = 0.2) + theme_light()+ theme(legend.position = &quot;none&quot;) # derecha dens.income &lt;- ggplot(Default, aes(x = income, group = default)) + geom_density(aes(color = default, fill = default), alpha = 0.2) + theme_light() + coord_flip() + theme(legend.position = &quot;none&quot;) grid.arrange(dens.balance, empty, plot.2d, dens.income, ncol=2, nrow=2, widths=c(4, 1), heights=c(1, 4)) 3.1 Análisis Discriminante Lineal df &lt;- Default[, c(&quot;income&quot;, &quot;balance&quot;, &quot;default&quot;)] set.seed(123) train.ID &lt;- createDataPartition(df$default, p = 0.8, list = FALSE) train_df &lt;- df[train.ID, ] test_df &lt;- df[-train.ID, ] # definimos como control una validación cruzada con 10 hojas, sin repeticiones fit_control &lt;- trainControl(method=&#39;cv&#39;, number = 10) set.seed(123) model_lda_def &lt;- train(default ~., data = train_df, method = &quot;lda&quot;, trControl = fit_control) model_lda_def ## Linear Discriminant Analysis ## ## 8001 samples ## 2 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 7200, 7200, 7201, 7202, 7201, 7202, ... ## Resampling results: ## ## Accuracy Kappa ## 0.973379 0.3710518 model_lda_def$finalModel ## Call: ## lda(x, grouping = y) ## ## Prior probabilities of groups: ## No Yes ## 0.96662917 0.03337083 ## ## Group means: ## income balance ## No 33513.73 805.9109 ## Yes 32450.16 1753.3628 ## ## Coefficients of linear discriminants: ## LD1 ## income 9.334558e-06 ## balance 2.233976e-03 La precisión durante el entrenamiento es de un \\(\\approx 97\\%\\). También vemos que las probabilidades a priori \\(\\pi_i, i = 1,2\\) de pertenecer a cada clase son aproximadamente \\(97\\%\\) y \\(3\\%\\) respectivamente, lo cual corresponde a la razón de fallo que se comenta al inicio. El resultado Coefficients of linear discriminants indica las constantes que se multiplican a cada elemento de la muestra \\((\\text{income}_i, \\text{balance}_i)\\), \\(i = 1, \\ldots, n_{\\text{train}}\\), para obtener su correspondiente valor de la función discriminante lineal: \\[ \\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\dfrac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k+ log(\\pi_k) \\] Todo indica que la variable balance tiene un mayor peso en la discriminación. Como alternativa, podemos comprobarlo usando varImp: varImp(model_lda_def) ## ROC curve variable importance ## ## Importance ## balance 100 ## income 0 Veamos ahora qué tal es el ajuste en los datos test. # hagamos las predicciones del conjunto de prueba prediction_lda_def &lt;- predict(model_lda_def, newdata = test_df) confusionMatrix(prediction_lda_def, reference = test_df$default) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 1927 54 ## Yes 6 12 ## ## Accuracy : 0.97 ## 95% CI : (0.9615, 0.977) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.2488 ## ## Kappa : 0.2755 ## ## Mcnemar&#39;s Test P-Value : 1.298e-09 ## ## Sensitivity : 0.9969 ## Specificity : 0.1818 ## Pos Pred Value : 0.9727 ## Neg Pred Value : 0.6667 ## Prevalence : 0.9670 ## Detection Rate : 0.9640 ## Detection Prevalence : 0.9910 ## Balanced Accuracy : 0.5894 ## ## &#39;Positive&#39; Class : No ## # extraemos el Accuracy o Precisión confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1] ## Accuracy ## 0.969985 # la tasa de error tasa.error.lda &lt;- 1-confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1] names(tasa.error.lda) &lt;- &quot;Error LDA&quot; tasa.error.lda ## Error LDA ## 0.03001501 Vemos que la especificidad es muy buena (casi no falla al clasificar los verdaderos No), pero la sensibilidad (capacidad para detectar los verdaderos Yes) es muy mala. Vamos a cambiar el umbral de decisión (por defecto probabilidad \\(0.5\\)) para intentar mejorar el algoritmo para protegernos de los malos pagadores: # predecimos las probabilidades: prediction_lda_prob &lt;- predict(model_lda_def, newdata = test_df, type = &quot;prob&quot;) prediction_lda_def &lt;- as.factor( ifelse(prediction_lda_prob$Yes &gt; 0.2, &quot;Yes&quot;, &quot;No&quot;) ) confusionMatrix(prediction_lda_def, reference = test_df$default, positive = &quot;Yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 1885 32 ## Yes 48 34 ## ## Accuracy : 0.96 ## 95% CI : (0.9504, 0.9681) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.96205 ## ## Kappa : 0.4389 ## ## Mcnemar&#39;s Test P-Value : 0.09353 ## ## Sensitivity : 0.51515 ## Specificity : 0.97517 ## Pos Pred Value : 0.41463 ## Neg Pred Value : 0.98331 ## Precision : 0.41463 ## Recall : 0.51515 ## F1 : 0.45946 ## Prevalence : 0.03302 ## Detection Rate : 0.01701 ## Detection Prevalence : 0.04102 ## Balanced Accuracy : 0.74516 ## ## &#39;Positive&#39; Class : Yes ## 3.1.1 Frontera de decisión en 2D Como estamos en un problema de clasificación en dos dimensiones (\\(p = 2\\)), es posible representar la frontera de decisión del algoritmo, usando la función decision_bound. Debemos modificar los campos para que coincidan con las variables de Default: decision_bound = function(train_df_in, test_df_in, model_in){ # plot decision boundary for df &lt;- Default[, c(&quot;income&quot;, &quot;balance&quot;, &quot;default&quot;)] require(MASS) require(caret) require(ggplot2) require(gridExtra) # Paso 1: crear un grid de valores desde min a max de ambos predictores pl = seq(min(train_df_in$balance), max(train_df_in$balance), length.out = 80) pw = seq(min(train_df_in$income), max(train_df_in$income), length.out = 80) lgrid &lt;- expand.grid(balance=pl, income=pw) # Paso 2: obtener las predicciones tanto para el grid como para el test modelPredGrid &lt;- predict(model_in, newdata=lgrid) train_df_in$Pred.Class &lt;- predict(model_in, newdata = train_df_in) test_df_in$Pred.Class &lt;- predict(model_in, newdata = test_df_in) # Paso 3: ggplot con la funcion contour gg1 &lt;- ggplot(data=lgrid) + stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) + geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) + labs(colour = &quot;Clases&quot;) + ggtitle(&quot;Train&quot;) + geom_point(data=train_df_in, aes(x=balance, y=income, colour=default), size=5, shape=1) + theme_light() gg2 &lt;- ggplot(data=lgrid) + stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) + geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) + labs(colour = &quot;Clases&quot;) + ggtitle(&quot;Test&quot;) + geom_point(data=test_df_in, aes(x=balance, y=income, colour=default), size=5, shape=1) + theme_light() grid.arrange(gg1, gg2, ncol=1, nrow=2) } decision_bound(train_df, test_df, model_lda_def) 3.2 Análisis Discriminante Cuadrático El ajuste para el modelo QDA lo hacemos con el mismo control y la misma partición de la muestra. set.seed(123) model_qda_def &lt;- train(default ~., data = train_df, method = &quot;qda&quot;, trControl = fit_control) model_qda_def ## Quadratic Discriminant Analysis ## ## 8001 samples ## 2 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 7200, 7200, 7201, 7202, 7201, 7202, ... ## Resampling results: ## ## Accuracy Kappa ## 0.9730043 0.3787741 model_qda_def$finalModel ## Call: ## qda(x, grouping = y) ## ## Prior probabilities of groups: ## No Yes ## 0.96662917 0.03337083 ## ## Group means: ## income balance ## No 33513.73 805.9109 ## Yes 32450.16 1753.3628 Los resultados al entrenar son similares al caso LDA. Veamos las predicciones para la muestra test. # hagamos las predicciones del conjunto de prueba prediction_qda_def &lt;- predict(model_qda_def, newdata = test_df) confusionMatrix(prediction_qda_def, reference = test_df$default, positive = &quot;Yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 1924 51 ## Yes 9 15 ## ## Accuracy : 0.97 ## 95% CI : (0.9615, 0.977) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.2488 ## ## Kappa : 0.3214 ## ## Mcnemar&#39;s Test P-Value : 1.203e-07 ## ## Sensitivity : 0.227273 ## Specificity : 0.995344 ## Pos Pred Value : 0.625000 ## Neg Pred Value : 0.974177 ## Precision : 0.625000 ## Recall : 0.227273 ## F1 : 0.333333 ## Prevalence : 0.033017 ## Detection Rate : 0.007504 ## Detection Prevalence : 0.012006 ## Balanced Accuracy : 0.611308 ## ## &#39;Positive&#39; Class : Yes ## Notamos un rendimiento similar, con cierta mejora de la precisión y la sensibilidad. Finalmente, representamos la frontera de decisión del algoritmo. decision_bound(train_df, test_df, model_qda_def) ¡Ahora observamos que las regiones están separadas por curvas, en lugar de la recta del LDA! 3.3 Análisis Discriminante Regularizado Opción 1: el paquete caret crea el grid para \\((\\lambda, \\gamma)\\): set.seed(123) model_rda_def &lt;- train(default ~., data = train_df, method = &quot;rda&quot;, tuneLength = 2, trControl = fit_control) model_rda_def ## Regularized Discriminant Analysis ## ## 8001 samples ## 2 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 7200, 7200, 7201, 7202, 7201, 7202, ... ## Resampling results across tuning parameters: ## ## gamma lambda Accuracy Kappa ## 0 0 0.9730043 0.3787741 ## 0 1 0.9733790 0.3710518 ## 1 0 0.9666297 0.0000000 ## 1 1 0.9666297 0.0000000 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were gamma = 0 and lambda = 1. model_rda_def$finalModel ## Call: ## rda.default(x = x, grouping = y, gamma = param$gamma, lambda = param$lambda) ## ## Regularization parameters: ## gamma lambda ## 0 1 ## ## Prior probabilities of groups: ## No Yes ## 0.96662917 0.03337083 ## ## Misclassification rate: ## apparent: 2.662 % # en este caso el ggplot nos da información sobre los # hiperparametros y su correspondiente Accuracy ggplot(model_rda_def) + theme_light() Opción 2: podemos proporcionar un grid predefinido de valores \\((\\lambda, \\gamma)\\) en un data.frame que le pasamos a tuneGrid: # el grid se puede definir tambien &quot;a mano&quot; mi.grid &lt;- data.frame(lambda = c(0, 0.3, 0.6, 1) , gamma = c(0, 0, 0, 0)) set.seed(123) model_rda_def &lt;- train(default ~., data = train_df, method = &quot;rda&quot;, tuneGrid = mi.grid, trControl = fit_control) model_rda_def ## Regularized Discriminant Analysis ## ## 8001 samples ## 2 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 7200, 7200, 7201, 7202, 7201, 7202, ... ## Resampling results across tuning parameters: ## ## lambda Accuracy Kappa ## 0.0 0.9730043 0.3787741 ## 0.3 0.9733790 0.3791078 ## 0.6 0.9735039 0.3764691 ## 1.0 0.9733790 0.3710518 ## ## Tuning parameter &#39;gamma&#39; was held constant at a value of 0 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were gamma = 0 and lambda = 0.6. model_rda_def$finalModel ## Call: ## rda.default(x = x, grouping = y, gamma = param$gamma, lambda = param$lambda) ## ## Regularization parameters: ## gamma lambda ## 0.0 0.6 ## ## Prior probabilities of groups: ## No Yes ## 0.96662917 0.03337083 ## ## Misclassification rate: ## apparent: 2.675 % # en este caso el ggplot nos da información sobre los # hiperparametros y su correspondiente Accuracy ggplot(model_rda_def) + theme_light() Los resultados indican que los hiperparámetros óptimos en este caso corresponden a \\((\\lambda, \\gamma) = (0.6, 0)\\). Esto que hemos hecho es comparar diferentes modelos (porque han sido ajustados con diferentes hiperparámetros) resultantes del mismo algoritmo. Veamos las predicciones para la muestra test, la tasa de error correspondiente y la frontera de decisión. # hagamos las predicciones del conjunto de prueba prediction_rda_def &lt;- predict(model_rda_def, newdata = test_df) confusionMatrix(prediction_rda_def, reference = test_df$default, positive = &quot;Yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 1926 52 ## Yes 7 14 ## ## Accuracy : 0.9705 ## 95% CI : (0.9621, 0.9775) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.2098 ## ## Kappa : 0.3109 ## ## Mcnemar&#39;s Test P-Value : 1.014e-08 ## ## Sensitivity : 0.212121 ## Specificity : 0.996379 ## Pos Pred Value : 0.666667 ## Neg Pred Value : 0.973711 ## Precision : 0.666667 ## Recall : 0.212121 ## F1 : 0.321839 ## Prevalence : 0.033017 ## Detection Rate : 0.007004 ## Detection Prevalence : 0.010505 ## Balanced Accuracy : 0.604250 ## ## &#39;Positive&#39; Class : Yes ## # extraemos el Accuracy o Precisión confusionMatrix(prediction_rda_def, reference = test_df$default)$overall[1] ## Accuracy ## 0.9704852 # la tasa de error tasa.error.rda &lt;- 1-confusionMatrix(prediction_rda_def, reference = test_df$default)$overall[1] names(tasa.error.rda) &lt;- &quot;Error RDA&quot; tasa.error.rda ## Error RDA ## 0.02951476 decision_bound(train_df, test_df, model_rda_def) "],["compara.html", "4 Comparación entre Modelos 4.1 Comparando según Accuracy 4.2 Curva ROC", " 4 Comparación entre Modelos Hasta ahora hemos visto cómo comparar el ajuste de un modelo para diferentes hiperparámetros, por ejemplo, en el caso RDA se escoge el modelo con mayor Accuracy para diferentes combinaciones de \\((\\lambda, \\gamma)\\); o la selección del \\(K\\) óptimo en KNN. Este enfoque lo que hace es estudiar la distribución del Accuracy (o cualquier otra medida de precisión como el Kappa) para cada modelo independientemente (within-model). El enfoque que abordamos en esta sección es la comparación de las distribuciones de estas medidas de precisión, ahora entre los diferentes modelos (between-models). 4.1 Comparando según Accuracy Empezamos comparando los modelos ajustados en la Sección 3. Vamos a usar los mismos datos de entrenamiento para cada modelo, y además plantaremos una semilla para que el remuestreo se haga en los mismos conjuntos. library(caret) library(ISLR) df &lt;- Default[, c(&quot;income&quot;, &quot;balance&quot;, &quot;default&quot;)] set.seed(123) train.ID &lt;- createDataPartition(df$default, p = 0.8, list = FALSE) train_df &lt;- df[train.ID, ] test_df &lt;- df[-train.ID, ] # definimos como control una validación cruzada con 10 hojas y repeticiones fit_control &lt;- trainControl(method=&#39;repeatedcv&#39;, number = 10, repeats = 5) LDA: # LDA set.seed(321) model_lda_def &lt;- train(default ~., data = train_df, method = &quot;lda&quot;, trControl = fit_control) QDA: # QDA set.seed(321) model_qda_def &lt;- train(default ~., data = train_df, method = &quot;qda&quot;, trControl = fit_control) RDA: # RDA mi.grid &lt;- data.frame(lambda = c(0) , gamma = c(0)) set.seed(321) model_rda_def &lt;- train(default ~., data = train_df, method = &quot;rda&quot;, tuneGrid = mi.grid, trControl = fit_control) Agregamos también un KNN: set.seed(321) model_knn_def &lt;- train(default ~., data = train_df, method = &quot;knn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 5) Ahora, usamos la función resamples para agrupar todos los resultados calculados de cada modelo: resamps &lt;- resamples(list(LDA = model_lda_def, QDA = model_qda_def, RDA = model_rda_def, KNN = model_knn_def)) resamps ## ## Call: ## resamples.default(x = list(LDA = model_lda_def, QDA = model_qda_def, RDA ## = model_rda_def, KNN = model_knn_def)) ## ## Models: LDA, QDA, RDA, KNN ## Number of resamples: 50 ## Performance metrics: Accuracy, Kappa ## Time estimates for: everything, final model fit summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: LDA, QDA, RDA, KNN ## Number of resamples: 50 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.9650437 0.9712140 0.9737500 0.9732539 0.9750234 0.9800250 0 ## QDA 0.9650437 0.9712230 0.9725172 0.9733785 0.9762426 0.9800250 0 ## RDA 0.9650437 0.9712230 0.9725172 0.9733785 0.9762426 0.9800250 0 ## KNN 0.9675000 0.9712859 0.9737828 0.9740536 0.9762500 0.9825218 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.1274436 0.2932769 0.3612833 0.3694266 0.4450344 0.5755961 0 ## QDA 0.1648794 0.3059514 0.3966133 0.3950321 0.4595555 0.5944787 0 ## RDA 0.1648794 0.3059514 0.3966133 0.3950321 0.4595555 0.5944787 0 ## KNN 0.2653811 0.3977481 0.4629179 0.4570037 0.5016312 0.6422322 0 # box plots bwplot(resamps, metric = &quot;Accuracy&quot;) Los 4 métodos se comportan de forma similar en términos de precisión. Como se ha fijado una semilla y todos los subconjuntos donde se han ajustado los modelos son iguales, es posible hacer inferencias sobre las diferencias entre modelos. Vamos a calcular las diferencias (2 a 2) y luego hacer un t-test bajo la hipótesis nula de que no hay diferencias entre modelos. difValues &lt;- diff(resamps) difValues ## ## Call: ## diff.resamples(x = resamps) ## ## Models: LDA, QDA, RDA, KNN ## Metrics: Accuracy, Kappa ## Number of differences: 6 ## p-value adjustment: bonferroni summary(difValues) ## ## Call: ## summary.diff.resamples(object = difValues) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## Accuracy ## LDA QDA RDA KNN ## LDA -0.0001247 -0.0001247 -0.0007997 ## QDA 1.00000 0.0000000 -0.0006750 ## RDA 1.00000 NA -0.0006750 ## KNN 0.03606 0.06672 0.06672 ## ## Kappa ## LDA QDA RDA KNN ## LDA -0.02561 -0.02561 -0.08758 ## QDA 0.0001052 0.00000 -0.06197 ## RDA 0.0001052 NA -0.06197 ## KNN 1.406e-13 8.481e-11 8.481e-11 Los resultados indican lo que sospechábamos: no hay diferencias significativas entre los modelos, salvo tal vez entre LDA y KNN (p-valor \\(&gt; 0.05\\)). En estos casos, hacer un diagrama con los intervalos de confianza es muy ilustrativo. # intervalos de confianza para las diferencias dotplot(difValues) Solo el intervalo de confianza para la diferencia entre LDA y KNN no contiene al cero, por tanto, hay diferencias significativas para el nivel de confianza fijado. 4.2 Curva ROC 4.2.1 Análisis en la muestra test Hasta ahora solo hemos estudiado la precisión de los modelos usando el Accuracy, pero hay un gran número de medidas cuya aplicación está estrechamente ligada a la naturaleza del problema. Por ejemplo, el clasificador de Bayes asigna una observación a la clase con mayor probabilidad a posteriori \\(p_k(X)\\). Para el problema de los datos Default, donde solo tenemos las clases Yes (el cliente falla en el pago de su tarjeta de crédito) y No (el cliente no falla en el pago), asignamos una observación a la clase Yes si se cumple: \\[ \\Pr (\\text{ default = Yes} | X = x) &gt; 0.5. \\] Pero seguramente el interés del banco es asignar la clase correcta a los malos pagadores y así obtener ganancias, denegando créditos. Esto puede lograrse bajando este umbral de \\(0.5\\) a \\(0.2\\), o sea, asignamos una observación a la clase Yes si: \\[ \\Pr (\\text{ default = Yes} | X = x) &gt; 0.2. \\] Decisiones como estas deben basarse en la experiencia de expertos (e.g. el banco que aprueba el crédito). Vamos a estudiar los tipos de errores que se comenten al variar el umbral de decisión. Para ello, empezamos estimando las probabilidades a posteriori del método LDA en nuestra muestra test: # hagamos las predicciones del conjunto de prueba pred_prob &lt;- predict(model_lda_def, newdata = test_df, type = &quot;prob&quot;) Usamos el paquete ROCR (Sing et al. 2020) para calcular la Curva Receiver Operating Characteristic (ROC) que compara simultáneamente dos tipos de errores: la Razón de Falsos Positivos (FPR, siglas en inglés) y la Razón de Verdaderos Positivos (TPR, siglas en inglés), para un grid de valores del umbral. library(ROCR) library(dplyr) prob.pred &lt;- prediction(pred_prob[,2], test_df$default) # ROC prob.pred %&gt;% performance(measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) %&gt;% plot() # AUC: mientras mas cercano a 1, mejor predicciones auc.lda &lt;- performance(prob.pred, measure = &quot;auc&quot;)@y.values[[1]] auc.lda ## [1] 0.9526643 El Área bajo la Curva ROC (AUC, siglas en inglés) resume el rendimiento del clasificador, para todos los posibles umbrales. Una curva ROC ideal debería alcanzar el borde superior izquierdo, por tanto, mientras más cercano a 1 esté el AUC, mejor será. En nuestro ejemplo el área es de \\(0.95\\), lo cual indica muy buen ajuste. Por otro lado, un AUC cercano a 0.5 indica que el clasificador asigna las clases al azar. Con el mismo paquete podemos representar otras curvas. Por ejemplo, podemos estudiar por separado, y para diferentes umbrales: La tasa de error general para diferentes umbrales: \\[ \\Pr (\\hat{Y} \\neq Y) \\approx (FP + FN)/(P+N); \\] donde FP: Falsos Positivos, FN: False Negativos, P: Positivos en la muestra (reales) y N: Negativos en la muestra (reales). prob.pred %&gt;% performance(&quot;err&quot;) %&gt;% plot() La Razón de Verdaderos Positivos: \\[ P(\\hat Y = + | Y = +) \\approx TP/P;\\] prob.pred %&gt;% performance(&quot;tpr&quot;) %&gt;% plot() La Razón de Falsos Positivos: \\[ \\Pr(\\hat Y = + | Y = -) \\approx FP/N;\\] prob.pred %&gt;% performance(&quot;fpr&quot;) %&gt;% plot() La Razón de Falsos Negativos: \\[ \\Pr(\\hat Y = - | Y = +) \\approx FN/P;\\] prob.pred %&gt;% performance(&quot;fnr&quot;) %&gt;% plot() Toda la información sobre los errores podemos representarla en un mismo gráfico y así ver el equilibrio entre error y umbral: # Podemos combinar las 3 ultimas en un mismo grafico df_perfor &lt;- data.frame(Error.Rate = performance(prob.pred, &quot;err&quot;)@y.values[[1]], FNR = performance(prob.pred, &quot;fnr&quot;)@y.values[[1]], FPR = performance(prob.pred, &quot;fpr&quot;)@y.values[[1]], TPR = performance(prob.pred, &quot;tpr&quot;)@y.values[[1]], CutOffs = performance(prob.pred, &quot;err&quot;)@x.values[[1]]) # plot tasas de error errores.lda &lt;- ggplot(df_perfor, aes(x = CutOffs)) + geom_line(aes(y = Error.Rate, colour = &quot;Tasa Error General&quot;)) + geom_line(aes(y = FNR, colour = &quot;FNR&quot;)) + geom_line(aes(y = FPR, colour = &quot;FPR&quot;)) + scale_colour_discrete(name = &quot;Medidas&quot; ) + xlab(&quot;Puntos de corte&quot;) + ylab(&quot;Tasas de Error&quot;) + theme_light() errores.lda La curva ROC podemos hacerla en ggplot como se muestra a continuación: # plot de la curva ROC roc.lda &lt;- ggplot(df_perfor, aes(x = FPR, y = TPR)) + geom_line() + xlab(&quot;FPR: 1- especificidad&quot;) + ylab(&quot;TPR: sensibilidad&quot;) + ggtitle(paste0(&quot;Curva ROC - LDA (Area Under Curve = &quot;, round(auc.lda, digits = 3),&quot;)&quot;)) + theme_light() roc.lda 4.2.2 Análisis en la muestra de entrenamiento Por defecto, caret calcula el RMSE, el MAE y el R^2 como medidas de precisión en el caso de la regresión. En problemas de clasificación, por defecto se computa Accuracy y Kappa, como hemos visto hasta ahora. En el caso de la estimación de los parámetros, se emplea RMSE y Accuracy por defecto. De hecho, el argumento metric de la función train permite al usuario el criterio que desee. En el caso de clasificación binaria es posible emplear las curvas ROC para comparar el rendimiento entre modelos, justo como hicimos con el Accuracy. Ahora, en lugar de estimar la clase correspondiente, es necesario calcular las probabilidades de cada clase (hacer classProbs = T en el trainControl) y debemos agregar la opción summaryFunction = twoClassSummary: # definimos como control una validación cruzada con 10 hojas y repeticiones fit_control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5, ## Estimar las probabilidades: classProbs = TRUE, ## Evaluar rendimiento del modelo: summaryFunction = twoClassSummary) # LDA set.seed(321) model_lda_def &lt;- train(default ~., data = train_df, method = &quot;lda&quot;, trControl = fit_control, ## Especificamos la métrica para optimizar: metric = &quot;ROC&quot;) # QDA set.seed(321) model_qda_def &lt;- train(default ~., data = train_df, method = &quot;qda&quot;, trControl = fit_control, ## Especificamos la métrica para optimizar: metric = &quot;ROC&quot;) # RDA mi.grid &lt;- data.frame(lambda = c(0) , gamma = c(0)) set.seed(321) model_rda_def &lt;- train(default ~., data = train_df, method = &quot;rda&quot;, tuneGrid = mi.grid, trControl = fit_control, ## Especificamos la métrica para optimizar: metric = &quot;ROC&quot;) set.seed(321) model_knn_def &lt;- train(default ~., data = train_df, method = &quot;knn&quot;, trControl = fit_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 5, ## Especificamos la métrica para optimizar: metric = &quot;ROC&quot;) model_knn_def ## k-Nearest Neighbors ## ## 8001 samples ## 2 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## Pre-processing: centered (2), scaled (2) ## Resampling: Cross-Validated (10 fold, repeated 5 times) ## Summary of sample sizes: 7200, 7200, 7201, 7201, 7201, 7201, ... ## Resampling results across tuning parameters: ## ## k ROC Sens Spec ## 5 0.7988217 0.9933019 0.3594302 ## 7 0.8170610 0.9936381 0.3722792 ## 9 0.8328763 0.9947759 0.3684330 ## 11 0.8508985 0.9950345 0.3602849 ## 13 0.8614489 0.9956036 0.3498575 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was k = 13. Usamos una vez más la función resamples para agrupar todos los resultados calculados de cada modelo: resamps &lt;- resamples(list(LDA = model_lda_def, QDA = model_qda_def, RDA = model_rda_def, KNN = model_knn_def)) resamps ## ## Call: ## resamples.default(x = list(LDA = model_lda_def, QDA = model_qda_def, RDA ## = model_rda_def, KNN = model_knn_def)) ## ## Models: LDA, QDA, RDA, KNN ## Number of resamples: 50 ## Performance metrics: ROC, Sens, Spec ## Time estimates for: everything, final model fit summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: LDA, QDA, RDA, KNN ## Number of resamples: 50 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.8957110 0.9386303 0.9524699 0.9476517 0.9609670 0.9730248 0 ## QDA 0.8959100 0.9386739 0.9519189 0.9474556 0.9607047 0.9725936 0 ## RDA 0.8959100 0.9386739 0.9519189 0.9474556 0.9607047 0.9725936 0 ## KNN 0.7301416 0.8398544 0.8648040 0.8614489 0.8882339 0.9492118 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.9948320 0.9974127 0.9980612 0.9981379 0.9996770 1 0 ## QDA 0.9935317 0.9961190 0.9974127 0.9973362 0.9987063 1 0 ## RDA 0.9935317 0.9961190 0.9974127 0.9973362 0.9987063 1 0 ## KNN 0.9909444 0.9948254 0.9961190 0.9956036 0.9970905 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## LDA 0.07407407 0.1869658 0.2307692 0.2524786 0.2962963 0.4615385 0 ## QDA 0.11111111 0.1997863 0.2642450 0.2794017 0.3333333 0.4814815 0 ## RDA 0.11111111 0.1997863 0.2642450 0.2794017 0.3333333 0.4814815 0 ## KNN 0.18518519 0.2962963 0.3333333 0.3498575 0.4074074 0.5925926 0 # box plots bwplot(resamps, metric = &quot;ROC&quot;) Los 3 métodos Discriminantes se comportan de forma similar, lo cual es de esperar ya que los hemos entrenado poco por cuestiones prácticas (¡demoran!). El KNN parece ser el peor de todos, pero tampoco hemos puesto mucho empeño en calcular el número óptimo de vecinos. Aún así, estos valores de AUC son muy buenos, en la práctica es difícil conseguir estos resultados. Pasamos a hacer algunas inferencias. Particularmente, vamos a calcular las diferencias (2 a 2) y luego hacer un t-test bajo la hipótesis nula de que no hay diferencias entre modelos. difValues &lt;- diff(resamps) difValues ## ## Call: ## diff.resamples(x = resamps) ## ## Models: LDA, QDA, RDA, KNN ## Metrics: ROC, Sens, Spec ## Number of differences: 6 ## p-value adjustment: bonferroni summary(difValues) ## ## Call: ## summary.diff.resamples(object = difValues) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## ROC ## LDA QDA RDA KNN ## LDA 0.0001961 0.0001961 0.0862029 ## QDA 0.05335 0.0000000 0.0860067 ## RDA 0.05335 NA 0.0860067 ## KNN &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 ## ## Sens ## LDA QDA RDA KNN ## LDA 0.0008017 0.0008017 0.0025343 ## QDA 4.441e-06 0.0000000 0.0017326 ## RDA 4.441e-06 NA 0.0017326 ## KNN 1.367e-12 1.261e-08 1.261e-08 ## ## Spec ## LDA QDA RDA KNN ## LDA -0.02692 -0.02692 -0.09738 ## QDA 1.942e-07 0.00000 -0.07046 ## RDA 1.942e-07 NA -0.07046 ## KNN &lt; 2.2e-16 9.408e-14 9.408e-14 Los resultados indican lo que sospechábamos: hay diferencias significativas entre los modelos (X)DA y el KNN (p-valor \\(&gt; 0.05\\)). En estos casos, hacer un diagrama con los intervalos de confianza es mucho más ilustrativo. # intervalos de confianza para las diferencias dotplot(difValues) En la práctica, el siguiente paso sería escoger el modelo más competitivo de acuerdo a alguno de los criterios estudiados y, con este, predecir las respuestas de la muestra test. "],["wiscon.html", "5 Wisconsin Breast-Cancer Data 5.1 Reducción de la dimensión", " 5 Wisconsin Breast-Cancer Data Los datos de cáncer de mama Wisconsin están disponibles en diversas plataformas. Por ejemplo, en Kaggle. Estos corresponden a mediciones obtenidas de una imagen digitalizada de un aspirado con aguja fina (FNA) de una masa mamaria. La variables describen las características de los núcleos celulares presentes en la imagen. Este conjunto de datos es muy didáctico y permite estimar si los tumores son malignos o benignos, conociendo la media, desviación estándar y valor máximo de 10 mediciones de cada una de las 10 características: radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter^2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (coastline approximation - 1) El resultado es un problema de clasificación binario (\\(Y =\\) diagnosis) con 30 variables predictoras. La muestra de 569 pacientes corresponde a 357 en la clase B y 212 en la clase M. Los datos están disponibles en este repositorio y también en Aula Global. library(caret) library(ggplot2) library(readr) library(dplyr) library(gridExtra) library(ROCR) ## Cargar datos ---- wiscon &lt;- read_csv(&quot;data_breast_cancer_wisconsin.csv&quot;) # no nos interesan los ID, y la última columna no se ha cargado bien wiscon &lt;- wiscon[, 2:32] # la respuesta es diagnosis: B = benign, M = malignant wiscon$diagnosis &lt;- as.factor(wiscon$diagnosis) df &lt;- as.data.frame(wiscon) str(df) ## &#39;data.frame&#39;: 569 obs. of 31 variables: ## $ diagnosis : Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ radius_mean : num 18 20.6 19.7 11.4 20.3 ... ## $ texture_mean : num 10.4 17.8 21.2 20.4 14.3 ... ## $ perimeter_mean : num 122.8 132.9 130 77.6 135.1 ... ## $ area_mean : num 1001 1326 1203 386 1297 ... ## $ smoothness_mean : num 0.1184 0.0847 0.1096 0.1425 0.1003 ... ## $ compactness_mean : num 0.2776 0.0786 0.1599 0.2839 0.1328 ... ## $ concavity_mean : num 0.3001 0.0869 0.1974 0.2414 0.198 ... ## $ concave points_mean : num 0.1471 0.0702 0.1279 0.1052 0.1043 ... ## $ symmetry_mean : num 0.242 0.181 0.207 0.26 0.181 ... ## $ fractal_dimension_mean : num 0.0787 0.0567 0.06 0.0974 0.0588 ... ## $ radius_se : num 1.095 0.543 0.746 0.496 0.757 ... ## $ texture_se : num 0.905 0.734 0.787 1.156 0.781 ... ## $ perimeter_se : num 8.59 3.4 4.58 3.44 5.44 ... ## $ area_se : num 153.4 74.1 94 27.2 94.4 ... ## $ smoothness_se : num 0.0064 0.00522 0.00615 0.00911 0.01149 ... ## $ compactness_se : num 0.049 0.0131 0.0401 0.0746 0.0246 ... ## $ concavity_se : num 0.0537 0.0186 0.0383 0.0566 0.0569 ... ## $ concave points_se : num 0.0159 0.0134 0.0206 0.0187 0.0188 ... ## $ symmetry_se : num 0.03 0.0139 0.0225 0.0596 0.0176 ... ## $ fractal_dimension_se : num 0.00619 0.00353 0.00457 0.00921 0.00511 ... ## $ radius_worst : num 25.4 25 23.6 14.9 22.5 ... ## $ texture_worst : num 17.3 23.4 25.5 26.5 16.7 ... ## $ perimeter_worst : num 184.6 158.8 152.5 98.9 152.2 ... ## $ area_worst : num 2019 1956 1709 568 1575 ... ## $ smoothness_worst : num 0.162 0.124 0.144 0.21 0.137 ... ## $ compactness_worst : num 0.666 0.187 0.424 0.866 0.205 ... ## $ concavity_worst : num 0.712 0.242 0.45 0.687 0.4 ... ## $ concave points_worst : num 0.265 0.186 0.243 0.258 0.163 ... ## $ symmetry_worst : num 0.46 0.275 0.361 0.664 0.236 ... ## $ fractal_dimension_worst: num 0.1189 0.089 0.0876 0.173 0.0768 ... ## - attr(*, &quot;problems&quot;)= tibble [569 x 5] (S3: tbl_df/tbl/data.frame) ## ..$ row : int [1:569] 1 2 3 4 5 6 7 8 9 10 ... ## ..$ col : chr [1:569] NA NA NA NA ... ## ..$ expected: chr [1:569] &quot;33 columns&quot; &quot;33 columns&quot; &quot;33 columns&quot; &quot;33 columns&quot; ... ## ..$ actual : chr [1:569] &quot;32 columns&quot; &quot;32 columns&quot; &quot;32 columns&quot; &quot;32 columns&quot; ... ## ..$ file : chr [1:569] &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; ... Vamos a crear una partición independiente (con una semilla) de test y aplicar todo lo estudiado hasta ahora. set.seed(666) train.ID &lt;- createDataPartition(df$diagnosis, p = 0.7, list = FALSE) train_df &lt;- df[train.ID, ] test_df &lt;- df[-train.ID, ] ¡A por ello! 5.1 Reducción de la dimensión Aunque queda fuera del aprendizaje supervisado, como posible solución a la alta dimensionalidad de los datos, en caret es posible aplicar técnicas no supervisadas que permiten reducir la dimensión. Una de ellas es el Análisis de Componentes Principales (PCA, por sus siglas en inglés). Veamos cómo hacer esto con la función preProcess: # en este caso estamos reduciendo la cantidad de variables iniciales # a solamente ¡2! preProc.res &lt;- preProcess(df, method = c(&#39;pca&#39;), pcaComp = 2) df.pca &lt;- predict(preProc.res, df) head(df.pca, 7) ## diagnosis PC1 PC2 ## 1 M -9.184755 -1.946870 ## 2 M -2.385703 3.764859 ## 3 M -5.728855 1.074229 ## 4 M -7.116691 -10.266556 ## 5 M -3.931842 1.946359 ## 6 M -2.378155 -3.946456 ## 7 M -2.236915 2.687666 Veamos qué tan separadas quedan las clases ahora: ggplot(df.pca, aes(x = PC1, y = PC2, group = diagnosis)) + geom_point(aes(color = diagnosis ), alpha = 0.8) + theme_light() Si volvemos a hacer la partición de los datos (mismos índices para el test); # Ajustemos nuestros modelos con los datos transformados: train_df &lt;- df.pca[train.ID, ] test_df &lt;- df.pca[-train.ID, ] entonces, podemos aplicar todos los modelos estudiados a un conjunto de datos de menor complejidad. Esto es una ganancia en tiempo de cómputo ¿será también en términos predictivos? Intenta también representar la frontera de decisión correspondiente a cada método, usando como base la ya conocida decision_bound. El LDA también puede ser visto como un método de reducción de la dimensión (a lo sumo # clases - 1). La visión de Fisher del discriminante lineal contempla encontrar la mejor proyección de los datos (a una dimensión inferior) que permita separar bien las clases. Esto se logra persiguiendo la mayor dispersión posible en los datos. Una buena introducción a esta visión del LDA está disponible en las lecciones de Prof. Olga Veksler. También recomiendo este post de Matthias Döring. Veamos un ejemplo con los datos iris: library(MASS) df &lt;- iris set.seed(123) train.ID &lt;- createDataPartition(df$Species, p = 0.8, list = FALSE) train_df &lt;- df[train.ID, ] test_df &lt;- df[-train.ID, ] lda_iris &lt;- MASS::lda(Species ~ ., train_df) lda_iris ## Call: ## lda(Species ~ ., data = train_df) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 4.9800 3.3700 1.4650 0.2400 ## versicolor 5.9400 2.7700 4.2325 1.3275 ## virginica 6.6375 3.0125 5.6225 2.0700 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.787581 0.05173815 ## Sepal.Width 1.605418 -2.45346114 ## Petal.Length -2.144011 0.80572094 ## Petal.Width -2.909670 -2.51645779 ## ## Proportion of trace: ## LD1 LD2 ## 0.9902 0.0098 El campo Coefficients of linear discriminants indica los coeficientes de cada discriminante. Por ejemplo, el primer discriminante lineal (LD1) es la combinación lineal: (0.79*Sepal.Length) + (1.60*Sepal.Width) + (-2.14*Petal.Length) + (-2.90*Petal.Width) El campo Proportions of trace describe la proporción de varianza entre clases que es explicada por los discriminantes lineales sucesivos. En este caso, LD1 explica 99% de la varianza, o sea, solo con la primera componente podríamos ser capaces de discriminar con buena precisión. Veamos la proyección en el espacio LD1 vs. LD2: plot(lda_iris, col = as.integer(train_df$Species)) Vemos que LD1 permite separar bien ambas clases, aunque hay un poco de superposición entre virginica y versicolor. Finalmente, en una sola dimensión (la definida por LD1): plot(lda_iris, dimen = 1, type = &quot;b&quot;) "],["bibliografía.html", "Bibliografía", " Bibliografía Burger, Scott V. 2018. Introduction to machine learning with R - Rigorous mathematical modeling. Vol. 1. OReilly. James, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2017. ISLR: Data for an Introduction to Statistical Learning with Applications in r. http://www.StatLearning.com. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning with Applications in r*. Vol. 6. Springer. Kuhn, Max. 2020. Caret: Classification and Regression Training. https://github.com/topepo/caret/. Rebala, Gopinath, Ajay Ravi, and Sanjay Churiwala. 2019. An Introduction to Machine Learning. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-15729-6. Ripley, Brian. 2020. Class: Functions for Classification. http://www.stats.ox.ac.uk/pub/MASS4/. Sing, Tobias, Oliver Sander, Niko Beerenwinkel, and Thomas Lengauer. 2020. ROCR: Visualizing the Performance of Scoring Classifiers. http://ipa-tys.github.io/ROCR/. "]]
