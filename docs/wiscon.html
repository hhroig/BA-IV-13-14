<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Wisconsin Breast-Cancer Data | Introducción al Aprendizaje Supervisado</title>
  <meta name="description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Wisconsin Breast-Cancer Data | Introducción al Aprendizaje Supervisado" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Wisconsin Breast-Cancer Data | Introducción al Aprendizaje Supervisado" />
  
  <meta name="twitter:description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento." />
  

<meta name="author" content="Harold A. Hernández-Roig (hahernan@est-econ.uc3m.es)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="compara.html"/>
<link rel="next" href="bibliografía.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html"><i class="fa fa-check"></i><b>2</b> K - Vecinos más Próximos</a><ul>
<li class="chapter" data-level="2.1" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#clasificación-con-el-paquete-class"><i class="fa fa-check"></i><b>2.1</b> Clasificación con el paquete <code>class</code></a></li>
<li class="chapter" data-level="2.2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#el-paquete-caret"><i class="fa fa-check"></i><b>2.2</b> El paquete <code>caret</code></a><ul>
<li class="chapter" data-level="2.2.1" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#visualización"><i class="fa fa-check"></i><b>2.2.1</b> Visualización</a></li>
<li class="chapter" data-level="2.2.2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#clasificación-con-knn"><i class="fa fa-check"></i><b>2.2.2</b> Clasificación con KNN</a></li>
<li class="chapter" data-level="2.2.3" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#importancia-de-las-variables"><i class="fa fa-check"></i><b>2.2.3</b> Importancia de las variables</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#regresión"><i class="fa fa-check"></i><b>2.3</b> Regresión</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="DA.html"><a href="DA.html"><i class="fa fa-check"></i><b>3</b> Análisis Discriminante</a><ul>
<li class="chapter" data-level="3.1" data-path="DA.html"><a href="DA.html#LDA"><i class="fa fa-check"></i><b>3.1</b> Análisis Discriminante Lineal</a></li>
<li class="chapter" data-level="3.2" data-path="DA.html"><a href="DA.html#QDA"><i class="fa fa-check"></i><b>3.2</b> Análisis Discriminante Cuadrático</a></li>
<li class="chapter" data-level="3.3" data-path="DA.html"><a href="DA.html#RDA"><i class="fa fa-check"></i><b>3.3</b> Análisis Discriminante Regularizado</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="compara.html"><a href="compara.html"><i class="fa fa-check"></i><b>4</b> Comparación entre Modelos</a><ul>
<li class="chapter" data-level="4.1" data-path="compara.html"><a href="compara.html#comparando-según-accuracy"><i class="fa fa-check"></i><b>4.1</b> Comparando según <code>Accuracy</code></a></li>
<li class="chapter" data-level="4.2" data-path="compara.html"><a href="compara.html#curva-roc"><i class="fa fa-check"></i><b>4.2</b> Curva <em>ROC</em></a><ul>
<li class="chapter" data-level="4.2.1" data-path="compara.html"><a href="compara.html#análisis-en-la-muestra-test"><i class="fa fa-check"></i><b>4.2.1</b> Análisis en la muestra test</a></li>
<li class="chapter" data-level="4.2.2" data-path="compara.html"><a href="compara.html#análisis-en-la-muestra-de-entrenamiento"><i class="fa fa-check"></i><b>4.2.2</b> Análisis en la muestra de entrenamiento</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="wiscon.html"><a href="wiscon.html"><i class="fa fa-check"></i><b>5</b> Wisconsin Breast-Cancer Data</a><ul>
<li class="chapter" data-level="5.1" data-path="wiscon.html"><a href="wiscon.html#comentarios-finales-reducción-de-la-dimensión"><i class="fa fa-check"></i><b>5.1</b> Comentarios finales: reducción de la dimensión</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción al Aprendizaje Supervisado</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="wiscon" class="section level1">
<h1><span class="header-section-number">5</span> Wisconsin Breast-Cancer Data</h1>
<p>Los datos de cáncer de mama Wisconsin están disponibles en diversas plataformas. Por ejemplo, en <a href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data">Kaggle</a>. Estos corresponden a mediciones obtenidas “<em>de una imagen digitalizada de un aspirado con aguja fina (FNA) de una masa mamaria</em>”. La variables describen las características de los núcleos celulares presentes en la imagen. Este conjunto de datos es muy didáctico y permite estimar si los tumores son malignos o benignos, conociendo la media, desviación estándar y valor máximo de 10 mediciones de cada una de las 10 características:</p>
<ul>
<li>radius (mean of distances from center to points on the perimeter)</li>
<li>texture (standard deviation of gray-scale values)</li>
<li>perimeter</li>
<li>area</li>
<li>smoothness (local variation in radius lengths)</li>
<li>compactness (perimeter^2 / area - 1.0)</li>
<li>concavity (severity of concave portions of the contour)</li>
<li>concave points (number of concave portions of the contour)</li>
<li>symmetry</li>
<li>fractal dimension (“coastline approximation” - 1)</li>
</ul>
<p>El resultado es un problema de clasificación binario (<span class="math inline">\(Y =\)</span> <code>diagnosis</code>) con 30 variables predictoras. La muestra de 569 pacientes corresponde a 357 en la clase <code>B</code> y 212 en la clase <code>M</code>. Los datos están disponibles en este repositorio y también en Aula Global.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb161-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb161-3" data-line-number="3"><span class="kw">library</span>(readr)</a>
<a class="sourceLine" id="cb161-4" data-line-number="4"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb161-5" data-line-number="5"><span class="kw">library</span>(gridExtra)</a>
<a class="sourceLine" id="cb161-6" data-line-number="6"><span class="kw">library</span>(ROCR)</a>
<a class="sourceLine" id="cb161-7" data-line-number="7"></a>
<a class="sourceLine" id="cb161-8" data-line-number="8"><span class="co">## Cargar datos ----</span></a>
<a class="sourceLine" id="cb161-9" data-line-number="9">wiscon &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data_breast_cancer_wisconsin.csv&quot;</span>)</a>
<a class="sourceLine" id="cb161-10" data-line-number="10"></a>
<a class="sourceLine" id="cb161-11" data-line-number="11"><span class="co"># no nos interesan los ID, y la última columna no se ha cargado bien</span></a>
<a class="sourceLine" id="cb161-12" data-line-number="12">wiscon &lt;-<span class="st"> </span>wiscon[, <span class="dv">2</span><span class="op">:</span><span class="dv">32</span>]</a>
<a class="sourceLine" id="cb161-13" data-line-number="13"></a>
<a class="sourceLine" id="cb161-14" data-line-number="14"><span class="co"># la respuesta es diagnosis: B = benign, M = malignant</span></a>
<a class="sourceLine" id="cb161-15" data-line-number="15">wiscon<span class="op">$</span>diagnosis &lt;-<span class="st"> </span><span class="kw">as.factor</span>(wiscon<span class="op">$</span>diagnosis)</a>
<a class="sourceLine" id="cb161-16" data-line-number="16"></a>
<a class="sourceLine" id="cb161-17" data-line-number="17">df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(wiscon)</a>
<a class="sourceLine" id="cb161-18" data-line-number="18"></a>
<a class="sourceLine" id="cb161-19" data-line-number="19"><span class="kw">str</span>(df)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    569 obs. of  31 variables:
##  $ diagnosis              : Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ radius_mean            : num  18 20.6 19.7 11.4 20.3 ...
##  $ texture_mean           : num  10.4 17.8 21.2 20.4 14.3 ...
##  $ perimeter_mean         : num  122.8 132.9 130 77.6 135.1 ...
##  $ area_mean              : num  1001 1326 1203 386 1297 ...
##  $ smoothness_mean        : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...
##  $ compactness_mean       : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...
##  $ concavity_mean         : num  0.3001 0.0869 0.1974 0.2414 0.198 ...
##  $ concave points_mean    : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...
##  $ symmetry_mean          : num  0.242 0.181 0.207 0.26 0.181 ...
##  $ fractal_dimension_mean : num  0.0787 0.0567 0.06 0.0974 0.0588 ...
##  $ radius_se              : num  1.095 0.543 0.746 0.496 0.757 ...
##  $ texture_se             : num  0.905 0.734 0.787 1.156 0.781 ...
##  $ perimeter_se           : num  8.59 3.4 4.58 3.44 5.44 ...
##  $ area_se                : num  153.4 74.1 94 27.2 94.4 ...
##  $ smoothness_se          : num  0.0064 0.00522 0.00615 0.00911 0.01149 ...
##  $ compactness_se         : num  0.049 0.0131 0.0401 0.0746 0.0246 ...
##  $ concavity_se           : num  0.0537 0.0186 0.0383 0.0566 0.0569 ...
##  $ concave points_se      : num  0.0159 0.0134 0.0206 0.0187 0.0188 ...
##  $ symmetry_se            : num  0.03 0.0139 0.0225 0.0596 0.0176 ...
##  $ fractal_dimension_se   : num  0.00619 0.00353 0.00457 0.00921 0.00511 ...
##  $ radius_worst           : num  25.4 25 23.6 14.9 22.5 ...
##  $ texture_worst          : num  17.3 23.4 25.5 26.5 16.7 ...
##  $ perimeter_worst        : num  184.6 158.8 152.5 98.9 152.2 ...
##  $ area_worst             : num  2019 1956 1709 568 1575 ...
##  $ smoothness_worst       : num  0.162 0.124 0.144 0.21 0.137 ...
##  $ compactness_worst      : num  0.666 0.187 0.424 0.866 0.205 ...
##  $ concavity_worst        : num  0.712 0.242 0.45 0.687 0.4 ...
##  $ concave points_worst   : num  0.265 0.186 0.243 0.258 0.163 ...
##  $ symmetry_worst         : num  0.46 0.275 0.361 0.664 0.236 ...
##  $ fractal_dimension_worst: num  0.1189 0.089 0.0876 0.173 0.0768 ...
##  - attr(*, &quot;problems&quot;)=Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 569 obs. of  5 variables:
##   ..$ row     : int  1 2 3 4 5 6 7 8 9 10 ...
##   ..$ col     : chr  NA NA NA NA ...
##   ..$ expected: chr  &quot;33 columns&quot; &quot;33 columns&quot; &quot;33 columns&quot; &quot;33 columns&quot; ...
##   ..$ actual  : chr  &quot;32 columns&quot; &quot;32 columns&quot; &quot;32 columns&quot; &quot;32 columns&quot; ...
##   ..$ file    : chr  &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; ...</code></pre>
<p>Vamos a crear una partición independiente (con una semilla) de test y aplicar todo lo estudiado hasta ahora.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">666</span>)</a>
<a class="sourceLine" id="cb163-2" data-line-number="2">train.ID &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(df<span class="op">$</span>diagnosis, <span class="dt">p =</span> <span class="fl">0.7</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb163-3" data-line-number="3"></a>
<a class="sourceLine" id="cb163-4" data-line-number="4">train_df &lt;-<span class="st"> </span>df[train.ID, ]</a>
<a class="sourceLine" id="cb163-5" data-line-number="5">test_df &lt;-<span class="st"> </span>df[<span class="op">-</span>train.ID, ]</a></code></pre></div>
<p>¡A por ello!</p>
<div id="comentarios-finales-reducción-de-la-dimensión" class="section level2">
<h2><span class="header-section-number">5.1</span> Comentarios finales: reducción de la dimensión</h2>
<p>Aunque queda fuera del <em>aprendizaje supervisado</em>, como posible solución a la alta dimensionalidad de los datos, en <code>caret</code> es posible aplicar técnicas <em>no supervisadas</em> que permiten <em>reducir la dimensión</em>. Una de ellas es el <em>Análisis de Componentes Principales</em> (<em>PCA</em>, por sus siglas en inglés). Veamos cómo hacer esto con la función <code>preProcess</code>:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1"><span class="co"># en este caso estamos reduciendo la cantidad de variables iniciales</span></a>
<a class="sourceLine" id="cb164-2" data-line-number="2"><span class="co"># a solamente ¡2!</span></a>
<a class="sourceLine" id="cb164-3" data-line-number="3">preProc.res &lt;-<span class="st"> </span><span class="kw">preProcess</span>(df, <span class="dt">method =</span> <span class="kw">c</span>(<span class="st">&#39;pca&#39;</span>), <span class="dt">pcaComp =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb164-4" data-line-number="4">df.pca &lt;-<span class="st"> </span><span class="kw">predict</span>(preProc.res, df)</a>
<a class="sourceLine" id="cb164-5" data-line-number="5"></a>
<a class="sourceLine" id="cb164-6" data-line-number="6"><span class="kw">head</span>(df.pca, <span class="dv">7</span>)</a></code></pre></div>
<pre><code>##   diagnosis       PC1        PC2
## 1         M -9.184755  -1.946870
## 2         M -2.385703   3.764859
## 3         M -5.728855   1.074229
## 4         M -7.116691 -10.266556
## 5         M -3.931842   1.946359
## 6         M -2.378155  -3.946456
## 7         M -2.236915   2.687666</code></pre>
<p>Veamos qué tan separadas quedan las clases ahora:</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb166-1" data-line-number="1"><span class="kw">ggplot</span>(df.pca,  <span class="kw">aes</span>(<span class="dt">x =</span> PC1, <span class="dt">y =</span> PC2, <span class="dt">group =</span> diagnosis)) <span class="op">+</span></a>
<a class="sourceLine" id="cb166-2" data-line-number="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> diagnosis ), <span class="dt">alpha =</span> <span class="fl">0.8</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb166-3" data-line-number="3"><span class="st">  </span><span class="kw">theme_light</span>()</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-72-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Si volvemos a hacer la partición de los datos (mismos índices para el test);</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1"><span class="co"># Ajustemos nuestros modelos con los datos transformados:</span></a>
<a class="sourceLine" id="cb167-2" data-line-number="2">train_df &lt;-<span class="st"> </span>df.pca[train.ID, ]</a>
<a class="sourceLine" id="cb167-3" data-line-number="3">test_df &lt;-<span class="st"> </span>df.pca[<span class="op">-</span>train.ID, ]</a></code></pre></div>
<p>entonces, podemos aplicar todos los modelos estudiados a un conjunto de datos de menor complejidad. Esto es una ganancia en tiempo de cómputo… ¿será también en términos predictivos? Intenta también <strong>representar la frontera de decisión</strong> correspondiente a cada método, usando como base la ya conocida <code>decision_bound</code>.</p>
<p>Finalmente, comentar que el LDA también puede ser visto como un método de reducción de la dimensión. La visión de Fisher del discriminante lineal contempla encontrar la mejor proyección de los datos (a una dimensión inferior) que permita separar bien las clases. Esto se logra persiguiendo la mayor dispersión posible en los datos.</p>
<p>Más información sobre el PCA puede ser consultada en los libros que se citan al final del documento. Una buena introducción a esta visión del LDA está disponible en las <a href="https://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture8.pdf">lecciones de Prof. Olga Veksler</a>. También se recomiendo el excelente <a href="https://www.datascienceblog.net/post/machine-learning/linear-discriminant-analysis/">post de Matthias Döring</a>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="compara.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografía.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
