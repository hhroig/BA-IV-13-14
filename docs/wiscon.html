<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Wisconsin Breast-Cancer Data | Introducción al Aprendizaje Supervisado</title>
  <meta name="description" content="Big Analytics V" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Wisconsin Breast-Cancer Data | Introducción al Aprendizaje Supervisado" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Big Analytics V" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Wisconsin Breast-Cancer Data | Introducción al Aprendizaje Supervisado" />
  
  <meta name="twitter:description" content="Big Analytics V" />
  

<meta name="author" content="Harold A. Hernández-Roig (hahernan@est-econ.uc3m.es)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="compara.html"/>
<link rel="next" href="bibliografía.html"/>
<script src="libs/header-attrs-2.6/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introducción</a></li>
<li class="chapter" data-level="2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html"><i class="fa fa-check"></i><b>2</b> K - Vecinos más Próximos</a>
<ul>
<li class="chapter" data-level="2.1" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#clasificación-con-el-paquete-class"><i class="fa fa-check"></i><b>2.1</b> Clasificación con el paquete <code>class</code></a></li>
<li class="chapter" data-level="2.2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#el-paquete-caret"><i class="fa fa-check"></i><b>2.2</b> El paquete <code>caret</code></a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#visualización"><i class="fa fa-check"></i><b>2.2.1</b> Visualización</a></li>
<li class="chapter" data-level="2.2.2" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#clasificación-con-knn"><i class="fa fa-check"></i><b>2.2.2</b> Clasificación con KNN</a></li>
<li class="chapter" data-level="2.2.3" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#importancia-de-las-variables"><i class="fa fa-check"></i><b>2.2.3</b> Importancia de las variables</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#regresión"><i class="fa fa-check"></i><b>2.3</b> Regresión</a></li>
<li class="chapter" data-level="2.4" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#weighted-knn"><i class="fa fa-check"></i><b>2.4</b> Weighted KNN</a></li>
<li class="chapter" data-level="2.5" data-path="k-vecinos-más-próximos.html"><a href="k-vecinos-más-próximos.html#procesamiento-en-paralelo"><i class="fa fa-check"></i><b>2.5</b> Procesamiento en paralelo</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="DA.html"><a href="DA.html"><i class="fa fa-check"></i><b>3</b> Análisis Discriminante</a>
<ul>
<li class="chapter" data-level="3.1" data-path="DA.html"><a href="DA.html#LDA"><i class="fa fa-check"></i><b>3.1</b> Análisis Discriminante Lineal</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="DA.html"><a href="DA.html#frontera-de-decisión-en-2d"><i class="fa fa-check"></i><b>3.1.1</b> Frontera de decisión en 2D</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="DA.html"><a href="DA.html#QDA"><i class="fa fa-check"></i><b>3.2</b> Análisis Discriminante Cuadrático</a></li>
<li class="chapter" data-level="3.3" data-path="DA.html"><a href="DA.html#RDA"><i class="fa fa-check"></i><b>3.3</b> Análisis Discriminante Regularizado</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="compara.html"><a href="compara.html"><i class="fa fa-check"></i><b>4</b> Comparación entre Modelos</a>
<ul>
<li class="chapter" data-level="4.1" data-path="compara.html"><a href="compara.html#comparando-según-accuracy"><i class="fa fa-check"></i><b>4.1</b> Comparando según <code>Accuracy</code></a></li>
<li class="chapter" data-level="4.2" data-path="compara.html"><a href="compara.html#curva-roc"><i class="fa fa-check"></i><b>4.2</b> Curva <em>ROC</em></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="compara.html"><a href="compara.html#análisis-en-la-muestra-test"><i class="fa fa-check"></i><b>4.2.1</b> Análisis en la muestra test</a></li>
<li class="chapter" data-level="4.2.2" data-path="compara.html"><a href="compara.html#análisis-en-la-muestra-de-entrenamiento"><i class="fa fa-check"></i><b>4.2.2</b> Análisis en la muestra de entrenamiento</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="wiscon.html"><a href="wiscon.html"><i class="fa fa-check"></i><b>5</b> Wisconsin Breast-Cancer Data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="wiscon.html"><a href="wiscon.html#reducción-de-la-dimensión"><i class="fa fa-check"></i><b>5.1</b> Reducción de la dimensión</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introducción al Aprendizaje Supervisado</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="wiscon" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Wisconsin Breast-Cancer Data</h1>
<p>Los datos de cáncer de mama Wisconsin están disponibles en diversas plataformas. Por ejemplo, en <a href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data">Kaggle</a>. Estos corresponden a mediciones obtenidas “<em>de una imagen digitalizada de un aspirado con aguja fina (FNA) de una masa mamaria</em>.” La variables describen las características de los núcleos celulares presentes en la imagen. Este conjunto de datos es muy didáctico y permite estimar si los tumores son malignos o benignos, conociendo la media, desviación estándar y valor máximo de 10 mediciones de cada una de las 10 características:</p>
<ul>
<li>radius (mean of distances from center to points on the perimeter)</li>
<li>texture (standard deviation of gray-scale values)</li>
<li>perimeter</li>
<li>area</li>
<li>smoothness (local variation in radius lengths)</li>
<li>compactness (perimeter^2 / area - 1.0)</li>
<li>concavity (severity of concave portions of the contour)</li>
<li>concave points (number of concave portions of the contour)</li>
<li>symmetry</li>
<li>fractal dimension (“coastline approximation” - 1)</li>
</ul>
<p>El resultado es un problema de clasificación binario (<span class="math inline">\(Y =\)</span> <code>diagnosis</code>) con 30 variables predictoras. La muestra de 569 pacientes corresponde a 357 en la clase <code>B</code> y 212 en la clase <code>M</code>. Los datos están disponibles en este repositorio y también en Aula Global.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="wiscon.html#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb171-2"><a href="wiscon.html#cb171-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb171-3"><a href="wiscon.html#cb171-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb171-4"><a href="wiscon.html#cb171-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb171-5"><a href="wiscon.html#cb171-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb171-6"><a href="wiscon.html#cb171-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ROCR)</span>
<span id="cb171-7"><a href="wiscon.html#cb171-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-8"><a href="wiscon.html#cb171-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Cargar datos ----</span></span>
<span id="cb171-9"><a href="wiscon.html#cb171-9" aria-hidden="true" tabindex="-1"></a>wiscon <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data_breast_cancer_wisconsin.csv&quot;</span>)</span>
<span id="cb171-10"><a href="wiscon.html#cb171-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-11"><a href="wiscon.html#cb171-11" aria-hidden="true" tabindex="-1"></a><span class="co"># no nos interesan los ID, y la última columna no se ha cargado bien</span></span>
<span id="cb171-12"><a href="wiscon.html#cb171-12" aria-hidden="true" tabindex="-1"></a>wiscon <span class="ot">&lt;-</span> wiscon[, <span class="dv">2</span><span class="sc">:</span><span class="dv">32</span>]</span>
<span id="cb171-13"><a href="wiscon.html#cb171-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-14"><a href="wiscon.html#cb171-14" aria-hidden="true" tabindex="-1"></a><span class="co"># la respuesta es diagnosis: B = benign, M = malignant</span></span>
<span id="cb171-15"><a href="wiscon.html#cb171-15" aria-hidden="true" tabindex="-1"></a>wiscon<span class="sc">$</span>diagnosis <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wiscon<span class="sc">$</span>diagnosis)</span>
<span id="cb171-16"><a href="wiscon.html#cb171-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-17"><a href="wiscon.html#cb171-17" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(wiscon)</span>
<span id="cb171-18"><a href="wiscon.html#cb171-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb171-19"><a href="wiscon.html#cb171-19" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(df)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    569 obs. of  31 variables:
##  $ diagnosis              : Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ radius_mean            : num  18 20.6 19.7 11.4 20.3 ...
##  $ texture_mean           : num  10.4 17.8 21.2 20.4 14.3 ...
##  $ perimeter_mean         : num  122.8 132.9 130 77.6 135.1 ...
##  $ area_mean              : num  1001 1326 1203 386 1297 ...
##  $ smoothness_mean        : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...
##  $ compactness_mean       : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...
##  $ concavity_mean         : num  0.3001 0.0869 0.1974 0.2414 0.198 ...
##  $ concave points_mean    : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...
##  $ symmetry_mean          : num  0.242 0.181 0.207 0.26 0.181 ...
##  $ fractal_dimension_mean : num  0.0787 0.0567 0.06 0.0974 0.0588 ...
##  $ radius_se              : num  1.095 0.543 0.746 0.496 0.757 ...
##  $ texture_se             : num  0.905 0.734 0.787 1.156 0.781 ...
##  $ perimeter_se           : num  8.59 3.4 4.58 3.44 5.44 ...
##  $ area_se                : num  153.4 74.1 94 27.2 94.4 ...
##  $ smoothness_se          : num  0.0064 0.00522 0.00615 0.00911 0.01149 ...
##  $ compactness_se         : num  0.049 0.0131 0.0401 0.0746 0.0246 ...
##  $ concavity_se           : num  0.0537 0.0186 0.0383 0.0566 0.0569 ...
##  $ concave points_se      : num  0.0159 0.0134 0.0206 0.0187 0.0188 ...
##  $ symmetry_se            : num  0.03 0.0139 0.0225 0.0596 0.0176 ...
##  $ fractal_dimension_se   : num  0.00619 0.00353 0.00457 0.00921 0.00511 ...
##  $ radius_worst           : num  25.4 25 23.6 14.9 22.5 ...
##  $ texture_worst          : num  17.3 23.4 25.5 26.5 16.7 ...
##  $ perimeter_worst        : num  184.6 158.8 152.5 98.9 152.2 ...
##  $ area_worst             : num  2019 1956 1709 568 1575 ...
##  $ smoothness_worst       : num  0.162 0.124 0.144 0.21 0.137 ...
##  $ compactness_worst      : num  0.666 0.187 0.424 0.866 0.205 ...
##  $ concavity_worst        : num  0.712 0.242 0.45 0.687 0.4 ...
##  $ concave points_worst   : num  0.265 0.186 0.243 0.258 0.163 ...
##  $ symmetry_worst         : num  0.46 0.275 0.361 0.664 0.236 ...
##  $ fractal_dimension_worst: num  0.1189 0.089 0.0876 0.173 0.0768 ...
##  - attr(*, &quot;problems&quot;)= tibble [569 x 5] (S3: tbl_df/tbl/data.frame)
##   ..$ row     : int [1:569] 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ col     : chr [1:569] NA NA NA NA ...
##   ..$ expected: chr [1:569] &quot;33 columns&quot; &quot;33 columns&quot; &quot;33 columns&quot; &quot;33 columns&quot; ...
##   ..$ actual  : chr [1:569] &quot;32 columns&quot; &quot;32 columns&quot; &quot;32 columns&quot; &quot;32 columns&quot; ...
##   ..$ file    : chr [1:569] &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; &quot;&#39;data_breast_cancer_wisconsin.csv&#39;&quot; ...</code></pre>
<p>Vamos a crear una partición independiente (con una semilla) de test y aplicar todo lo estudiado hasta ahora.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="wiscon.html#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">666</span>)</span>
<span id="cb173-2"><a href="wiscon.html#cb173-2" aria-hidden="true" tabindex="-1"></a>train.ID <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(df<span class="sc">$</span>diagnosis, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb173-3"><a href="wiscon.html#cb173-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-4"><a href="wiscon.html#cb173-4" aria-hidden="true" tabindex="-1"></a>train_df <span class="ot">&lt;-</span> df[train.ID, ]</span>
<span id="cb173-5"><a href="wiscon.html#cb173-5" aria-hidden="true" tabindex="-1"></a>test_df <span class="ot">&lt;-</span> df[<span class="sc">-</span>train.ID, ]</span></code></pre></div>
<p>¡A por ello!</p>
<div id="reducción-de-la-dimensión" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Reducción de la dimensión</h2>
<p>Aunque queda fuera del <em>aprendizaje supervisado</em>, como posible solución a la alta dimensionalidad de los datos, en <code>caret</code> es posible aplicar técnicas <em>no supervisadas</em> que permiten <em>reducir la dimensión</em>. Una de ellas es el <em>Análisis de Componentes Principales</em> (<em>PCA</em>, por sus siglas en inglés). Veamos cómo hacer esto con la función <code>preProcess</code>:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="wiscon.html#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="co"># en este caso estamos reduciendo la cantidad de variables iniciales</span></span>
<span id="cb174-2"><a href="wiscon.html#cb174-2" aria-hidden="true" tabindex="-1"></a><span class="co"># a solamente ¡2!</span></span>
<span id="cb174-3"><a href="wiscon.html#cb174-3" aria-hidden="true" tabindex="-1"></a>preProc.res <span class="ot">&lt;-</span> <span class="fu">preProcess</span>(df, <span class="at">method =</span> <span class="fu">c</span>(<span class="st">&#39;pca&#39;</span>), <span class="at">pcaComp =</span> <span class="dv">2</span>)</span>
<span id="cb174-4"><a href="wiscon.html#cb174-4" aria-hidden="true" tabindex="-1"></a>df.pca <span class="ot">&lt;-</span> <span class="fu">predict</span>(preProc.res, df)</span>
<span id="cb174-5"><a href="wiscon.html#cb174-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-6"><a href="wiscon.html#cb174-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df.pca, <span class="dv">7</span>)</span></code></pre></div>
<pre><code>##   diagnosis       PC1        PC2
## 1         M -9.184755  -1.946870
## 2         M -2.385703   3.764859
## 3         M -5.728855   1.074229
## 4         M -7.116691 -10.266556
## 5         M -3.931842   1.946359
## 6         M -2.378155  -3.946456
## 7         M -2.236915   2.687666</code></pre>
<p>Veamos qué tan separadas quedan las clases ahora:</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="wiscon.html#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df.pca,  <span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2, <span class="at">group =</span> diagnosis)) <span class="sc">+</span></span>
<span id="cb176-2"><a href="wiscon.html#cb176-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> diagnosis ), <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb176-3"><a href="wiscon.html#cb176-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_light</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-81-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Si volvemos a hacer la partición de los datos (mismos índices para el test);</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="wiscon.html#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ajustemos nuestros modelos con los datos transformados:</span></span>
<span id="cb177-2"><a href="wiscon.html#cb177-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="ot">&lt;-</span> df.pca[train.ID, ]</span>
<span id="cb177-3"><a href="wiscon.html#cb177-3" aria-hidden="true" tabindex="-1"></a>test_df <span class="ot">&lt;-</span> df.pca[<span class="sc">-</span>train.ID, ]</span></code></pre></div>
<p>entonces, podemos aplicar todos los modelos estudiados a un conjunto de datos de menor complejidad. Esto es una ganancia en tiempo de cómputo… ¿será también en términos predictivos? Intenta también <strong>representar la frontera de decisión</strong> correspondiente a cada método, usando como base la ya conocida <code>decision_bound</code>.</p>
<p>El LDA también puede ser visto como un método de reducción de la dimensión (a lo sumo <em># clases - 1</em>). La visión de Fisher del discriminante lineal contempla encontrar la mejor proyección de los datos (a una dimensión inferior) que permita separar bien las clases. Esto se logra persiguiendo la mayor dispersión posible en los datos. Una buena introducción a esta visión del LDA está disponible en las <a href="https://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture8.pdf">lecciones de Prof. Olga Veksler</a>. También recomiendo este <a href="https://www.datascienceblog.net/post/machine-learning/linear-discriminant-analysis/">post de Matthias Döring</a>.</p>
<p>Veamos un ejemplo con los datos <code>iris</code>:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="wiscon.html#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb178-2"><a href="wiscon.html#cb178-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-3"><a href="wiscon.html#cb178-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> iris</span>
<span id="cb178-4"><a href="wiscon.html#cb178-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb178-5"><a href="wiscon.html#cb178-5" aria-hidden="true" tabindex="-1"></a>train.ID <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(df<span class="sc">$</span>Species, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb178-6"><a href="wiscon.html#cb178-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-7"><a href="wiscon.html#cb178-7" aria-hidden="true" tabindex="-1"></a>train_df <span class="ot">&lt;-</span> df[train.ID, ]</span>
<span id="cb178-8"><a href="wiscon.html#cb178-8" aria-hidden="true" tabindex="-1"></a>test_df <span class="ot">&lt;-</span> df[<span class="sc">-</span>train.ID, ]</span>
<span id="cb178-9"><a href="wiscon.html#cb178-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb178-10"><a href="wiscon.html#cb178-10" aria-hidden="true" tabindex="-1"></a>lda_iris <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">lda</span>(Species <span class="sc">~</span> ., train_df)</span>
<span id="cb178-11"><a href="wiscon.html#cb178-11" aria-hidden="true" tabindex="-1"></a>lda_iris</span></code></pre></div>
<pre><code>## Call:
## lda(Species ~ ., data = train_df)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##  0.3333333  0.3333333  0.3333333 
## 
## Group means:
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa           4.9800      3.3700       1.4650      0.2400
## versicolor       5.9400      2.7700       4.2325      1.3275
## virginica        6.6375      3.0125       5.6225      2.0700
## 
## Coefficients of linear discriminants:
##                    LD1         LD2
## Sepal.Length  0.787581  0.05173815
## Sepal.Width   1.605418 -2.45346114
## Petal.Length -2.144011  0.80572094
## Petal.Width  -2.909670 -2.51645779
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9902 0.0098</code></pre>
<p>El campo <code>Coefficients of linear discriminants</code> indica los coeficientes de cada discriminante. Por ejemplo, el primer discriminante lineal (LD1) es la combinación lineal:</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="wiscon.html#cb180-1" aria-hidden="true" tabindex="-1"></a>(<span class="fl">0.79</span><span class="sc">*</span>Sepal.Length) <span class="sc">+</span> (<span class="fl">1.60</span><span class="sc">*</span>Sepal.Width) <span class="sc">+</span> (<span class="sc">-</span><span class="fl">2.14</span><span class="sc">*</span>Petal.Length) <span class="sc">+</span> (<span class="sc">-</span><span class="fl">2.90</span><span class="sc">*</span>Petal.Width)</span></code></pre></div>
<p>El campo <code>Proportions of trace</code> describe la proporción de varianza entre clases que es explicada por los discriminantes lineales sucesivos. En este caso, LD1 explica 99% de la varianza, o sea, solo con la primera componente podríamos ser capaces de discriminar con buena precisión. Veamos la proyección en el espacio LD1 vs. LD2:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="wiscon.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda_iris, <span class="at">col =</span> <span class="fu">as.integer</span>(train_df<span class="sc">$</span>Species))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-85-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Vemos que LD1 permite separar bien ambas clases, aunque hay un poco de superposición entre <code>virginica</code> y <code>versicolor</code>. Finalmente, en una sola dimensión (la definida por LD1):</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="wiscon.html#cb182-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lda_iris, <span class="at">dimen =</span> <span class="dv">1</span>, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-86-1.png" width="80%" style="display: block; margin: auto;" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="compara.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bibliografía.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
