<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.4 Weighted KNN | Introducción al Aprendizaje Supervisado" />
<meta property="og:type" content="book" />


<meta property="og:description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento." />


<meta name="author" content="Harold A. Hernández-Roig (hahernan@est-econ.uc3m.es)" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Sesiones 13-15 IV Edición del Big Analytics: de la información al conocimiento.">

<title>2.4 Weighted KNN | Introducción al Aprendizaje Supervisado</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#introducción"><span class="toc-section-number">1</span> Introducción</a></li>
<li class="has-sub"><a href="2-k-vecinos-más-próximos.html#k---vecinos-más-próximos"><span class="toc-section-number">2</span> K - Vecinos más Próximos</a><ul>
<li><a href="2-1-clasificación-con-el-paquete-class.html#clasificación-con-el-paquete-class"><span class="toc-section-number">2.1</span> Clasificación con el paquete <code>class</code></a></li>
<li class="has-sub"><a href="2-2-el-paquete-caret.html#el-paquete-caret"><span class="toc-section-number">2.2</span> El paquete <code>caret</code></a><ul>
<li><a href="2-2-el-paquete-caret.html#visualización"><span class="toc-section-number">2.2.1</span> Visualización</a></li>
<li><a href="2-2-el-paquete-caret.html#clasificación-con-knn"><span class="toc-section-number">2.2.2</span> Clasificación con KNN</a></li>
<li><a href="2-2-el-paquete-caret.html#importancia-de-las-variables"><span class="toc-section-number">2.2.3</span> Importancia de las variables</a></li>
</ul></li>
<li><a href="2-3-regresión.html#regresión"><span class="toc-section-number">2.3</span> Regresión</a></li>
<li><a href="2-4-weighted-knn.html#weighted-knn"><span class="toc-section-number">2.4</span> Weighted KNN</a></li>
</ul></li>
<li class="has-sub"><a href="3-DA.html#DA"><span class="toc-section-number">3</span> Análisis Discriminante</a><ul>
<li><a href="3-1-LDA.html#LDA"><span class="toc-section-number">3.1</span> Análisis Discriminante Lineal</a></li>
<li><a href="3-2-QDA.html#QDA"><span class="toc-section-number">3.2</span> Análisis Discriminante Cuadrático</a></li>
<li><a href="3-3-RDA.html#RDA"><span class="toc-section-number">3.3</span> Análisis Discriminante Regularizado</a></li>
</ul></li>
<li class="has-sub"><a href="4-compara.html#compara"><span class="toc-section-number">4</span> Comparación entre Modelos</a><ul>
<li><a href="4-1-comparando-según-accuracy.html#comparando-según-accuracy"><span class="toc-section-number">4.1</span> Comparando según <code>Accuracy</code></a></li>
<li class="has-sub"><a href="4-2-curva-roc.html#curva-roc"><span class="toc-section-number">4.2</span> Curva <em>ROC</em></a><ul>
<li><a href="4-2-curva-roc.html#análisis-en-la-muestra-test"><span class="toc-section-number">4.2.1</span> Análisis en la muestra test</a></li>
<li><a href="4-2-curva-roc.html#análisis-en-la-muestra-de-entrenamiento"><span class="toc-section-number">4.2.2</span> Análisis en la muestra de entrenamiento</a></li>
</ul></li>
</ul></li>
<li class="has-sub"><a href="5-wiscon.html#wiscon"><span class="toc-section-number">5</span> Wisconsin Breast-Cancer Data</a><ul>
<li><a href="5-1-comentarios-finales-reducción-de-la-dimensión.html#comentarios-finales-reducción-de-la-dimensión"><span class="toc-section-number">5.1</span> Comentarios finales: reducción de la dimensión</a></li>
</ul></li>
<li><a href="bibliografía.html#bibliografía">Bibliografía</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="weighted-knn" class="section level2">
<h2><span class="header-section-number">2.4</span> Weighted KNN</h2>
<p>El método de K-Vecinos Más Próximos Ponderados (WKNN: <em>Weighted K-Nearest Neighbors</em>) es una variante del KNN. El principio básico es el mismo: predecir una respuesta en función de los puntos más cercanos de la muestra. La diferencia es que WKNN da más importancia a los más próximos, dentro de los K prefijados. Esto se logra ponderando o dando pesos a los vecinos.</p>
<p>En <code>caret</code> podemos fijar el método <code>kknn</code> que implementa WKNN, tanto para regresión como para clasificación. Ahora debemos optimizar 3 hiperparámetros:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1"><span class="kw">getModelInfo</span>(<span class="st">&quot;kknn&quot;</span>)<span class="op">$</span>kknn<span class="op">$</span>parameters</a></code></pre></div>
<pre><code>##   parameter     class           label
## 1      kmax   numeric Max. #Neighbors
## 2  distance   numeric        Distance
## 3    kernel character          Kernel</code></pre>
<p>El número de vecinos <code>K</code> se corresponde al campo<code>kmax</code>. El campo <code>distance</code> se refiere al order del parámetro <span class="math inline">\(p\)</span> en la <em>Distancia de Minkowski</em>. El <code>kernel</code> es la transformación de los ejes de coordenadas, las opciones son:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">kerns &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;rectangular&quot;</span>, <span class="st">&quot;triangular&quot;</span>, <span class="st">&quot;epanechnikov&quot;</span>, <span class="st">&quot;biweight&quot;</span>, <span class="st">&quot;triweight&quot;</span>, </a>
<a class="sourceLine" id="cb64-2" data-line-number="2">                                 <span class="st">&quot;cos&quot;</span>, <span class="st">&quot;inv&quot;</span>, <span class="st">&quot;gaussian&quot;</span>)</a></code></pre></div>
<p>Veamos un ejemplo con los datos <code>iris</code>. Empezamos fijando un grid o malla de posibles valores de los hiperparámetros a optimizar:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="co"># muestra, por eso es necesario una particion balanceada con createDataPartition</span></a>
<a class="sourceLine" id="cb65-2" data-line-number="2">df &lt;-<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb65-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb65-4" data-line-number="4">train.ID &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(df<span class="op">$</span>Species, <span class="dt">p =</span> <span class="fl">0.8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb65-5" data-line-number="5"></a>
<a class="sourceLine" id="cb65-6" data-line-number="6">train_df &lt;-<span class="st"> </span>df[train.ID, ]</a>
<a class="sourceLine" id="cb65-7" data-line-number="7">test_df &lt;-<span class="st"> </span>df[<span class="op">-</span>train.ID, ]</a>
<a class="sourceLine" id="cb65-8" data-line-number="8"></a>
<a class="sourceLine" id="cb65-9" data-line-number="9"><span class="co"># hacemos una validación cruzada con 10-folds 10 veces</span></a>
<a class="sourceLine" id="cb65-10" data-line-number="10">fit_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;repeatedcv&#39;</span>, <span class="dt">number =</span> <span class="dv">10</span>, <span class="dt">repeats =</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb65-11" data-line-number="11"></a>
<a class="sourceLine" id="cb65-12" data-line-number="12"><span class="co"># fijamos el grid de valores de los hiperparámetros:</span></a>
<a class="sourceLine" id="cb65-13" data-line-number="13">buscar_mejor &lt;-<span class="st"> </span><span class="kw">expand.grid</span>( <span class="dt">kmax =</span>  <span class="dv">3</span><span class="op">:</span><span class="dv">9</span>,</a>
<a class="sourceLine" id="cb65-14" data-line-number="14">                             <span class="dt">distance =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,</a>
<a class="sourceLine" id="cb65-15" data-line-number="15">                             <span class="dt">kernel =</span> <span class="kw">c</span>(<span class="st">&quot;rectangular&quot;</span>, <span class="co">#standard knn</span></a>
<a class="sourceLine" id="cb65-16" data-line-number="16">                                        <span class="st">&quot;triangular&quot;</span>,</a>
<a class="sourceLine" id="cb65-17" data-line-number="17">                                        <span class="st">&quot;gaussian&quot;</span>))</a></code></pre></div>
<p>El modelo se ajusta igual a como ya hemos estudiado:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">321</span>)</a>
<a class="sourceLine" id="cb66-2" data-line-number="2">model.w.knn &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span>.,</a>
<a class="sourceLine" id="cb66-3" data-line-number="3">                     <span class="dt">data =</span> train_df,</a>
<a class="sourceLine" id="cb66-4" data-line-number="4">                     <span class="dt">method =</span> <span class="st">&quot;kknn&quot;</span>,</a>
<a class="sourceLine" id="cb66-5" data-line-number="5">                     <span class="dt">trControl =</span> fit_control,</a>
<a class="sourceLine" id="cb66-6" data-line-number="6">                     <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb66-7" data-line-number="7">                     <span class="dt">tuneGrid =</span> buscar_mejor)</a>
<a class="sourceLine" id="cb66-8" data-line-number="8">model.w.knn</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 120 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (4), scaled (4) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   kmax  distance  kernel       Accuracy   Kappa  
##   3     1         rectangular  0.9600000  0.94000
##   3     1         triangular   0.9558333  0.93375
##   3     1         gaussian     0.9583333  0.93750
##   3     2         rectangular  0.9508333  0.92625
##   3     2         triangular   0.9550000  0.93250
##   3     2         gaussian     0.9483333  0.92250
##   4     1         rectangular  0.9600000  0.94000
##   4     1         triangular   0.9558333  0.93375
##   4     1         gaussian     0.9583333  0.93750
##   4     2         rectangular  0.9491667  0.92375
##   4     2         triangular   0.9550000  0.93250
##   4     2         gaussian     0.9458333  0.91875
##   5     1         rectangular  0.9575000  0.93625
##   5     1         triangular   0.9558333  0.93375
##   5     1         gaussian     0.9583333  0.93750
##   5     2         rectangular  0.9400000  0.91000
##   5     2         triangular   0.9566667  0.93500
##   5     2         gaussian     0.9591667  0.93875
##   6     1         rectangular  0.9575000  0.93625
##   6     1         triangular   0.9558333  0.93375
##   6     1         gaussian     0.9550000  0.93250
##   6     2         rectangular  0.9416667  0.91250
##   6     2         triangular   0.9558333  0.93375
##   6     2         gaussian     0.9558333  0.93375
##   7     1         rectangular  0.9550000  0.93250
##   7     1         triangular   0.9558333  0.93375
##   7     1         gaussian     0.9558333  0.93375
##   7     2         rectangular  0.9433333  0.91500
##   7     2         triangular   0.9575000  0.93625
##   7     2         gaussian     0.9558333  0.93375
##   8     1         rectangular  0.9550000  0.93250
##   8     1         triangular   0.9558333  0.93375
##   8     1         gaussian     0.9558333  0.93375
##   8     2         rectangular  0.9433333  0.91500
##   8     2         triangular   0.9616667  0.94250
##   8     2         gaussian     0.9583333  0.93750
##   9     1         rectangular  0.9550000  0.93250
##   9     1         triangular   0.9558333  0.93375
##   9     1         gaussian     0.9558333  0.93375
##   9     2         rectangular  0.9450000  0.91750
##   9     2         triangular   0.9675000  0.95125
##   9     2         gaussian     0.9608333  0.94125
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were kmax = 9, distance = 2 and kernel
##  = triangular.</code></pre>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1">model.w.knn<span class="op">$</span>finalModel</a></code></pre></div>
<pre><code>## 
## Call:
## kknn::train.kknn(formula = .outcome ~ ., data = dat, kmax = param$kmax,     distance = param$distance, kernel = as.character(param$kernel))
## 
## Type of response variable: nominal
## Minimal misclassification: 0.01666667
## Best kernel: triangular
## Best k: 9</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1"><span class="kw">plot</span>(model.w.knn)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-34-1.png" width="80%" style="display: block; margin: auto;" />
Observamos que el modelo final, el mejor de acuerdo al <code>Accuracy</code> es aquel con 8 vecinos, donde el parámetro <span class="math inline">\(p=2\)</span> en la Distancia de Minkowski (esto es equivalente a la Distancia Euclídea) y el kernel es triangular. Por otro lado, en lugar de escribir explícitamente el grid de valores a probar, en <code>caret</code> tenemos la opción de realizar una búsqueda aleatoria. Esto podría ser un primer paso para detectar rangos de valores de los hiperparámetros donde luego afinar la búsqueda. Como ejemplo, lo haremos para solo 8 combinaciones de posibles hiperparámetros (en la práctica debemos fijar un mayor número de combinaciones, lo que conlleva un mayor coste computacional):</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1"><span class="co"># random search WKNN</span></a>
<a class="sourceLine" id="cb71-2" data-line-number="2">fit_control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&#39;repeatedcv&#39;</span>, <span class="dt">number =</span> <span class="dv">10</span>, </a>
<a class="sourceLine" id="cb71-3" data-line-number="3">                            <span class="dt">repeats =</span> <span class="dv">10</span>,</a>
<a class="sourceLine" id="cb71-4" data-line-number="4">                            <span class="dt">search =</span> <span class="st">&quot;random&quot;</span>)</a>
<a class="sourceLine" id="cb71-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">321</span>)</a>
<a class="sourceLine" id="cb71-6" data-line-number="6">model.w.knn &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span>.,</a>
<a class="sourceLine" id="cb71-7" data-line-number="7">                     <span class="dt">data =</span> train_df,</a>
<a class="sourceLine" id="cb71-8" data-line-number="8">                     <span class="dt">method =</span> <span class="st">&quot;kknn&quot;</span>,</a>
<a class="sourceLine" id="cb71-9" data-line-number="9">                     <span class="dt">trControl =</span> fit_control,</a>
<a class="sourceLine" id="cb71-10" data-line-number="10">                     <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</a>
<a class="sourceLine" id="cb71-11" data-line-number="11">                     <span class="dt">tuneLength =</span> <span class="dv">8</span>)</a>
<a class="sourceLine" id="cb71-12" data-line-number="12">model.w.knn</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 120 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## Pre-processing: centered (4), scaled (4) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 108, 108, 108, 108, 108, 108, ... 
## Resampling results across tuning parameters:
## 
##   kmax  distance   kernel        Accuracy   Kappa  
##    1    0.8715102  biweight      0.9566667  0.93500
##   10    1.9213770  cos           0.9658333  0.94875
##   22    0.6048966  epanechnikov  0.9541667  0.93125
##   25    1.7765156  triangular    0.9683333  0.95250
##   28    1.8982141  inv           0.9666667  0.95000
##   37    0.1352608  rectangular   0.9491667  0.92375
##   37    2.2990154  inv           0.9725000  0.95875
##   40    1.2107700  triangular    0.9583333  0.93750
## 
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were kmax = 37, distance = 2.299015
##  and kernel = inv.</code></pre>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="kw">plot</span>(model.w.knn)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-35-1.png" width="80%" style="display: block; margin: auto;" /></p>

</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="2-3-regresión.html"><button class="btn btn-default">Previous</button></a>
<a href="3-DA.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
