test_df <- df[-train.ID, ]
# definimos como control una validación cruzada con 10 hojas, sin repeticiones
fit_control <- trainControl(method='cv', number = 10)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control)
model_lda_def
# model_lda_def$finalModel
df <- Default[, c("income", "balance", "default")]
set.seed(123)
train.ID <- createDataPartition(df$default, p = 0.8, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# definimos como control una validación cruzada con 10 hojas, sin repeticiones
fit_control <- trainControl(method='cv', number = 10)
set.seed(123)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control)
model_lda_def
model_lda_def$finalModel
varImp(model_lda_def)
# hagamos las predicciones del conjunto de prueba
prediction_lda_def <- predict(model_lda_def, newdata = test_df)
confusionMatrix(prediction_lda_def, reference = test_df$default)
# extraemos el Accuracy o Precisión
confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.lda <- 1-confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1]
names(tasa.error.lda) <- "Error LDA"
tasa.error.lda
decision_bound = function(train_df_in, test_df_in, model_in){
# plot decision boundary  for df <- Default[, c("income", "balance", "default")]
require(MASS)
require(caret)
require(ggplot2)
require(gridExtra)
# Paso 1: crear un grid de valores desde min a max de ambos predictores
pl = seq(min(train_df_in$balance), max(train_df_in$balance), length.out = 80)
pw = seq(min(train_df_in$income), max(train_df_in$income), length.out = 80)
lgrid <- expand.grid(balance=pl, income=pw)
# Paso 2: obtener las predicciones tanto para el grid como para el test
modelPredGrid <- predict(model_in, newdata=lgrid)
train_df_in$Pred.Class <- predict(model_in, newdata = train_df_in)
test_df_in$Pred.Class <- predict(model_in, newdata = test_df_in)
# Paso 3: ggplot con la funcion contour
gg1 <- ggplot(data=lgrid) +
stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) +
geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) +
labs(colour = "Clases") + ggtitle("Train") +
geom_point(data=train_df_in,
aes(x=balance, y=income,
colour=Species), size=5, shape=1) +
theme_light()
gg2 <- ggplot(data=lgrid) +
stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) +
geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) +
labs(colour = "Clases") + ggtitle("Test") +
geom_point(data=test_df_in,
aes(x=balance, y=income,
colour=Species), size=5, shape=1) +
theme_light()
grid.arrange(gg1, gg2, ncol=1, nrow=2)
}
decision_bound(train_df, test_df, model_lda_def)
decision_bound = function(train_df_in, test_df_in, model_in){
# plot decision boundary  for df <- Default[, c("income", "balance", "default")]
require(MASS)
require(caret)
require(ggplot2)
require(gridExtra)
# Paso 1: crear un grid de valores desde min a max de ambos predictores
pl = seq(min(train_df_in$balance), max(train_df_in$balance), length.out = 80)
pw = seq(min(train_df_in$income), max(train_df_in$income), length.out = 80)
lgrid <- expand.grid(balance=pl, income=pw)
# Paso 2: obtener las predicciones tanto para el grid como para el test
modelPredGrid <- predict(model_in, newdata=lgrid)
train_df_in$Pred.Class <- predict(model_in, newdata = train_df_in)
test_df_in$Pred.Class <- predict(model_in, newdata = test_df_in)
# Paso 3: ggplot con la funcion contour
gg1 <- ggplot(data=lgrid) +
stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) +
geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) +
labs(colour = "Clases") + ggtitle("Train") +
geom_point(data=train_df_in,
aes(x=balance, y=income,
colour=default), size=5, shape=1) +
theme_light()
gg2 <- ggplot(data=lgrid) +
stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) +
geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) +
labs(colour = "Clases") + ggtitle("Test") +
geom_point(data=test_df_in,
aes(x=balance, y=income,
colour=default), size=5, shape=1) +
theme_light()
grid.arrange(gg1, gg2, ncol=1, nrow=2)
}
decision_bound(train_df, test_df, model_lda_def)
View(train_df)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control)
model_qda_def
model_qda_def$finalModel
# hagamos las predicciones del conjunto de prueba
prediction_qda_def <- predict(model_qda_def, newdata = test_df)
confusionMatrix(prediction_qda_def, reference = test_df$default)
# extraemos el Accuracy o Precisión
confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.qda <- 1-confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
names(tasa.error.qda) <- "Error QDA"
tasa.error.qda
set.seed(123)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control)
model_qda_def
model_qda_def$finalModel
# hagamos las predicciones del conjunto de prueba
prediction_qda_def <- predict(model_qda_def, newdata = test_df)
confusionMatrix(prediction_qda_def, reference = test_df$default)
# extraemos el Accuracy o Precisión
confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.qda <- 1-confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
names(tasa.error.qda) <- "Error QDA"
tasa.error.qda
decision_bound(train_df, test_df, model_qda_def)
# El grid se puede definir tambien "a mano"
mi.grid <- data.frame(lambda = c(0.1, 0.5, 0.9) ,
gamma = c(0))
fitControl <- trainControl(method = "cv",
number = 10)
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneGrid = mi.grid,
trControl = fitControl)
## en este caso el ggplot nos da información sobre los hiperparametros y su correspondiente Accuracy
ggplot(model_rda_def) + theme_light()
fitControl <- trainControl(method = "cv",
number = 10)
model_rda_def <- train(default ~.,
data = train_df[sample(1:nrow(train_df), 0.2 * nrow(train_df)),],
method = "rda",
tuneLength = 2,
trControl = fitControl)
model_rda_def
model_rda_def$finalModel
# este método demora mucho más... por razones prácticas solo se ajusta a una porción de la muestra
model_rda_def <- train(default ~.,
data = train_df[sample(1:nrow(train_df), 0.2 * nrow(train_df)),],
method = "rda",
tuneLength = 2,
trControl = fitControl)
model_rda_def
model_rda_def$finalModel
# El grid se puede definir tambien "a mano"
mi.grid <- data.frame(lambda = c(0, 0.5, 0.9) ,
gamma = c(0,1))
# El grid se puede definir tambien "a mano"
mi.grid <- data.frame(lambda = c(0, 0, 1, 1) ,
gamma = c(0, 1, 0, 1))
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneGrid = mi.grid,
trControl = fitControl)
## en este caso el ggplot nos da información sobre los hiperparametros y su correspondiente Accuracy
ggplot(model_rda_def) + theme_light()
model_rda_def
model_rda_def$finalModel
model_rda_def
model_rda_def$finalModel
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneLength = 2,
trControl = fitControl)
# # este método demora mucho más... por razones prácticas solo se ajusta a una porción de la muestra
# model_rda_def <- train(default ~.,
#                        data = train_df[sample(1:nrow(train_df), 0.2 * nrow(train_df)),],
#                        method = "rda",
#                        tuneLength = 2,
#                        trControl = fitControl)
model_rda_def
model_rda_def$finalModel
library(caret)
library(ISLR)
df <- Default[, c("income", "balance", "default")]
set.seed(123)
train.ID <- createDataPartition(df$default, p = 0.8, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# definimos como control una validación cruzada con 10 hojas, sin repeticiones
fit_control <- trainControl(method='cv', number = 10)
# LDA
set.seed(321)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control)
# hagamos las predicciones del conjunto de prueba
prediction_lda_def <- predict(model_lda_def, newdata = test_df)
# QDA
set.seed(321)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control)
# hagamos las predicciones del conjunto de prueba
prediction_qda_def <- predict(model_qda_def, newdata = test_df)
# RDA
mi.grid <- data.frame(lambda = c(0) ,
gamma = c(0))
set.seed(321)
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneGrid = mi.grid,
trControl = fit_control)
# hagamos las predicciones del conjunto de prueba
prediction_rda_def <- predict(model_qda_def, newdata = test_df)
model_knn_def <- train(default ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 10)
# hagamos las predicciones del conjunto de prueba
prediction_knn_def <- predict(model_knn_def, newdata = test_df)
resamps <- resamples(list(LDA = model_lda_def,
QDA = model_qda_def,
RDA = model_rda_def,
KNN = model_knn_def))
resamps
summary(resamps)
# box plots
bwplot(resamps, layout = c(2, 1))
set.seed(321)
model_knn_def <- train(default ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 10)
# hagamos las predicciones del conjunto de prueba
prediction_knn_def <- predict(model_knn_def, newdata = test_df)
library(caret)
library(ISLR)
df <- Default[, c("income", "balance", "default")]
set.seed(123)
train.ID <- createDataPartition(df$default, p = 0.8, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# definimos como control una validación cruzada con 10 hojas y repeticiones
fit_control <- trainControl(method='repeatedcv', number = 10, repeats = 5)
# LDA
set.seed(321)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control)
# hagamos las predicciones del conjunto de prueba
prediction_lda_def <- predict(model_lda_def, newdata = test_df)
# QDA
set.seed(321)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control)
# hagamos las predicciones del conjunto de prueba
prediction_qda_def <- predict(model_qda_def, newdata = test_df)
# RDA
mi.grid <- data.frame(lambda = c(0) ,
gamma = c(0))
set.seed(321)
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneGrid = mi.grid,
trControl = fit_control)
# hagamos las predicciones del conjunto de prueba
prediction_rda_def <- predict(model_qda_def, newdata = test_df)
set.seed(321)
model_knn_def <- train(default ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 5)
# hagamos las predicciones del conjunto de prueba
prediction_knn_def <- predict(model_knn_def, newdata = test_df)
resamps <- resamples(list(LDA = model_lda_def,
QDA = model_qda_def,
RDA = model_rda_def,
KNN = model_knn_def))
resamps
summary(resamps)
# box plots
bwplot(resamps, layout = c(2, 1))
# box plots
bwplot(resamps, layout = c(1,2))
View(resamps)
# box plots
bwplot(resamps, metric = "Accuracy")
difValues <- diff(resamps)
difValues
# box plots
bwplot(difValues, metric = "Accuracy")
# intervalos de confianza para las diferencias
dotplot(difValues)
difValues <- diff(resamps)
difValues
summary(difValues)
# intervalos de confianza para las diferencias
dotplot(difValues)
library(ROCR)
library(dplyr)
prob.pred <- prediction(pred_prob[,2], test_df$default)
library(caret)
library(ISLR)
df <- Default[, c("income", "balance", "default")]
set.seed(123)
train.ID <- createDataPartition(df$default, p = 0.8, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# definimos como control una validación cruzada con 10 hojas y repeticiones
fit_control <- trainControl(method='repeatedcv', number = 10, repeats = 5)
# LDA
set.seed(321)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control)
# QDA
set.seed(321)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control)
# RDA
mi.grid <- data.frame(lambda = c(0) ,
gamma = c(0))
set.seed(321)
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneGrid = mi.grid,
trControl = fit_control)
set.seed(321)
model_knn_def <- train(default ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 5)
resamps <- resamples(list(LDA = model_lda_def,
QDA = model_qda_def,
RDA = model_rda_def,
KNN = model_knn_def))
resamps
summary(resamps)
# box plots
bwplot(resamps, metric = "Accuracy")
difValues <- diff(resamps)
difValues
summary(difValues)
# intervalos de confianza para las diferencias
dotplot(difValues)
# hagamos las predicciones del conjunto de prueba
pred_prob <- predict(model_lda_def, newdata = test_df, type = "prob")
library(ROCR)
library(dplyr)
prob.pred <- prediction(pred_prob[,2], test_df$default)
# ROC
prob.pred %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC: mientras mas cercano a 1, mejor predicciones
auc.lda <- performance(prob.pred, measure = "auc")@y.values[[1]]
library(ROCR)
library(dplyr)
prob.pred <- prediction(pred_prob[,2], test_df$default)
# ROC
prob.pred %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC: mientras mas cercano a 1, mejor predicciones
auc.lda <- performance(prob.pred, measure = "auc")@y.values[[1]]
auc.lda
prob.pred %>%
performance("err") %>%
plot()
prob.pred %>%
performance("tpr") %>%
plot()
prob.pred %>%
performance("fpr") %>%
plot()
prob.pred %>%
performance("fnr") %>%
plot()
# Podemos combinar las 3 ultimas en un mismo grafico
df_perfor <- data.frame(Error.Rate = performance(prob.pred, "err")@y.values[[1]],
FNR = performance(prob.pred, "fnr")@y.values[[1]],
FPR = performance(prob.pred, "fpr")@y.values[[1]],
CutOffs = performance(prob.pred, "err")@x.values[[1]])
# plot tasas de error
errores.lda <- ggplot(df_perfor, aes(x = CutOffs)) +
geom_line(aes(y = Error.Rate, colour = "Tasa Error General")) +
geom_line(aes(y = FNR, colour = "FNR")) +
geom_line(aes(y = FPR, colour = "FPR")) +
scale_colour_discrete(name = "Medidas" ) +
xlab("Puntos de corte") + ylab("Tasas de Error") +
theme_light()
errores.lda
# plot de la curva ROC
roc.lda <- ggplot(df_perfor, aes(x = FPR, y = TPR)) +
geom_line() +
xlab("FPR: 1- especificidad") + ylab("TPR: sensibilidad") +
ggtitle(paste0("Curva ROC - LDA (Area Under Curve = ", round(auc.lda, digits = 3),")")) +
theme_light()
roc.lda
# Podemos combinar las 3 ultimas en un mismo grafico
df_perfor <- data.frame(Error.Rate = performance(prob.pred, "err")@y.values[[1]],
FNR = performance(prob.pred, "fnr")@y.values[[1]],
FPR = performance(prob.pred, "fpr")@y.values[[1]],
TPR = performance(prob.pred, "tpr")@y.values[[1]],
CutOffs = performance(prob.pred, "err")@x.values[[1]])
# plot tasas de error
errores.lda <- ggplot(df_perfor, aes(x = CutOffs)) +
geom_line(aes(y = Error.Rate, colour = "Tasa Error General")) +
geom_line(aes(y = FNR, colour = "FNR")) +
geom_line(aes(y = FPR, colour = "FPR")) +
scale_colour_discrete(name = "Medidas" ) +
xlab("Puntos de corte") + ylab("Tasas de Error") +
theme_light()
errores.lda
# plot de la curva ROC
roc.lda <- ggplot(df_perfor, aes(x = FPR, y = TPR)) +
geom_line() +
xlab("FPR: 1- especificidad") + ylab("TPR: sensibilidad") +
ggtitle(paste0("Curva ROC - LDA (Area Under Curve = ", round(auc.lda, digits = 3),")")) +
theme_light()
roc.lda
setwd("~/OneDrive - Universidad Carlos III de Madrid/Big_Analytics_Ed4/14_15_Aprendizaje_Supervisado_Harold_A_Hdez_Roig/codes_practica")
setwd("~/OneDrive - Universidad Carlos III de Madrid/Big_Analytics_Ed4/14_15_Aprendizaje_Supervisado_Harold_A_Hdez_Roig/codes_practica")
setwd("~/OneDrive - Universidad Carlos III de Madrid/Big_Analytics_Ed4/probar_wine_concurso")
train_df <- read_csv("train_data.csv", col_names = T)
library(readr)
train_df <- read_csv("train_data.csv", col_names = T)
head(train_df)
head(train_df)
train_df <- read_csv("train_data.csv", col_names = T)
test_df <- read_csv("test_data.csv", col_names = T)
head(train_df)
View(test_df)
library(caret)
# creamos la particion correspondiente para entrenar
df2 <- df
train.ID <- createDataPartition(df2$Origin, p = 0.7, list = FALSE)
fit_control <- trainControl(## 10-fold CV
method = "cv",
number = 5)
model_knn <- train(Origin ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 2)
model_knn
View(train_df)
train_df$Origin <-as.factor(train_df$Origin)
fit_control <- trainControl(## 10-fold CV
method = "cv",
number = 5)
model_knn <- train(Origin ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 2)
model_knn
plot(model_knn)
pred <- predict(model_knn, newdata = test_df)
my_submission <- data.frame(pred = pred)
write.csv(my_submission, file = "pruebita1.csv", row.names = FALSE)
fit_control <- trainControl(## 10-fold CV
method = "cv",
number = 10,
repeats = 5)
fit_control <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
repeats = 5)
model_knn <- train(Origin ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 30)
model_knn
pred <- predict(model_knn, newdata = test_df)
my_submission <- data.frame(pred = pred)
write.csv(my_submission, file = "pruebita1.csv", row.names = FALSE)
