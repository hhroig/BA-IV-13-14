plot()
prob.pred %>%
performance("tpr") %>%
plot()
prob.pred %>%
performance("fpr") %>%
plot()
prob.pred %>%
performance("fnr") %>%
plot()
# Podemos combinar las 3 ultimas en un mismo grafico
df_perfor <- data.frame(Error.Rate = performance(prob.pred, "err")@y.values[[1]],
FNR = performance(prob.pred, "fnr")@y.values[[1]],
FPR = performance(prob.pred, "fpr")@y.values[[1]],
CutOffs = performance(prob.pred, "err")@x.values[[1]])
# plot tasas de error
errores.lda <- ggplot(df_perfor, aes(x = CutOffs)) +
geom_line(aes(y = Error.Rate, colour = "Tasa Error General")) +
geom_line(aes(y = FNR, colour = "FNR")) +
geom_line(aes(y = FPR, colour = "FPR")) +
scale_colour_discrete(name = "Medidas" ) +
xlab("Puntos de corte") + ylab("Tasas de Error") +
theme_light()
errores.lda
# plot de la curva ROC
roc.lda <- ggplot(df_perfor, aes(x = FPR, y = TPR)) +
geom_line() +
xlab("FPR: 1- especificidad") + ylab("TPR: sensibilidad") +
ggtitle(paste0("Curva ROC - LDA (Area Under Curve = ", round(auc.lda, digits = 3),")")) +
theme_light()
roc.lda
# Podemos combinar las 3 ultimas en un mismo grafico
df_perfor <- data.frame(Error.Rate = performance(prob.pred, "err")@y.values[[1]],
FNR = performance(prob.pred, "fnr")@y.values[[1]],
FPR = performance(prob.pred, "fpr")@y.values[[1]],
TPR = performance(prob.pred, "tpr")@y.values[[1]],
CutOffs = performance(prob.pred, "err")@x.values[[1]])
# plot tasas de error
errores.lda <- ggplot(df_perfor, aes(x = CutOffs)) +
geom_line(aes(y = Error.Rate, colour = "Tasa Error General")) +
geom_line(aes(y = FNR, colour = "FNR")) +
geom_line(aes(y = FPR, colour = "FPR")) +
scale_colour_discrete(name = "Medidas" ) +
xlab("Puntos de corte") + ylab("Tasas de Error") +
theme_light()
errores.lda
# plot de la curva ROC
roc.lda <- ggplot(df_perfor, aes(x = FPR, y = TPR)) +
geom_line() +
xlab("FPR: 1- especificidad") + ylab("TPR: sensibilidad") +
ggtitle(paste0("Curva ROC - LDA (Area Under Curve = ", round(auc.lda, digits = 3),")")) +
theme_light()
roc.lda
setwd("~/OneDrive - Universidad Carlos III de Madrid/Big_Analytics_Ed4/14_15_Aprendizaje_Supervisado_Harold_A_Hdez_Roig/codes_practica")
setwd("~/OneDrive - Universidad Carlos III de Madrid/Big_Analytics_Ed4/14_15_Aprendizaje_Supervisado_Harold_A_Hdez_Roig/codes_practica")
setwd("~/OneDrive - Universidad Carlos III de Madrid/Big_Analytics_Ed4/probar_wine_concurso")
train_df <- read_csv("train_data.csv", col_names = T)
library(readr)
train_df <- read_csv("train_data.csv", col_names = T)
head(train_df)
head(train_df)
train_df <- read_csv("train_data.csv", col_names = T)
test_df <- read_csv("test_data.csv", col_names = T)
head(train_df)
View(test_df)
library(caret)
# creamos la particion correspondiente para entrenar
df2 <- df
train.ID <- createDataPartition(df2$Origin, p = 0.7, list = FALSE)
fit_control <- trainControl(## 10-fold CV
method = "cv",
number = 5)
model_knn <- train(Origin ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 2)
model_knn
View(train_df)
train_df$Origin <-as.factor(train_df$Origin)
fit_control <- trainControl(## 10-fold CV
method = "cv",
number = 5)
model_knn <- train(Origin ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 2)
model_knn
plot(model_knn)
pred <- predict(model_knn, newdata = test_df)
my_submission <- data.frame(pred = pred)
write.csv(my_submission, file = "pruebita1.csv", row.names = FALSE)
fit_control <- trainControl(## 10-fold CV
method = "cv",
number = 10,
repeats = 5)
fit_control <- trainControl(## 10-fold CV
method = "repeatedcv",
number = 10,
repeats = 5)
model_knn <- train(Origin ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 30)
model_knn
pred <- predict(model_knn, newdata = test_df)
my_submission <- data.frame(pred = pred)
write.csv(my_submission, file = "pruebita1.csv", row.names = FALSE)
library(caret)
library(ISLR)
df <- Default[, c("income", "balance", "default")]
set.seed(123)
train.ID <- createDataPartition(df$default, p = 0.8, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# definimos como control una validación cruzada con 10 hojas y repeticiones
fit_control <- trainControl(method='repeatedcv', number = 10, repeats = 5)
# LDA
set.seed(321)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control)
# QDA
set.seed(321)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control)
# RDA
mi.grid <- data.frame(lambda = c(0) ,
gamma = c(0))
set.seed(321)
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneGrid = mi.grid,
trControl = fit_control)
set.seed(321)
model_knn_def <- train(default ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 5)
resamps <- resamples(list(LDA = model_lda_def,
QDA = model_qda_def,
RDA = model_rda_def,
KNN = model_knn_def))
resamps
summary(resamps)
# box plots
bwplot(resamps, metric = "Accuracy")
difValues <- diff(resamps)
difValues
summary(difValues)
# intervalos de confianza para las diferencias
dotplot(difValues)
# hagamos las predicciones del conjunto de prueba
pred_prob <- predict(model_lda_def, newdata = test_df, type = "prob")
library(ROCR)
library(dplyr)
prob.pred <- prediction(pred_prob[,2], test_df$default)
# ROC
prob.pred %>%
performance(measure = "tpr", x.measure = "fpr") %>%
plot()
# AUC: mientras mas cercano a 1, mejor predicciones
auc.lda <- performance(prob.pred, measure = "auc")@y.values[[1]]
auc.lda
prob.pred %>%
performance("err") %>%
plot()
prob.pred %>%
performance("tpr") %>%
plot()
prob.pred %>%
performance("fpr") %>%
plot()
prob.pred %>%
performance("fnr") %>%
plot()
# Podemos combinar las 3 ultimas en un mismo grafico
df_perfor <- data.frame(Error.Rate = performance(prob.pred, "err")@y.values[[1]],
FNR = performance(prob.pred, "fnr")@y.values[[1]],
FPR = performance(prob.pred, "fpr")@y.values[[1]],
TPR = performance(prob.pred, "tpr")@y.values[[1]],
CutOffs = performance(prob.pred, "err")@x.values[[1]])
# plot tasas de error
errores.lda <- ggplot(df_perfor, aes(x = CutOffs)) +
geom_line(aes(y = Error.Rate, colour = "Tasa Error General")) +
geom_line(aes(y = FNR, colour = "FNR")) +
geom_line(aes(y = FPR, colour = "FPR")) +
scale_colour_discrete(name = "Medidas" ) +
xlab("Puntos de corte") + ylab("Tasas de Error") +
theme_light()
errores.lda
# plot de la curva ROC
roc.lda <- ggplot(df_perfor, aes(x = FPR, y = TPR)) +
geom_line() +
xlab("FPR: 1- especificidad") + ylab("TPR: sensibilidad") +
ggtitle(paste0("Curva ROC - LDA (Area Under Curve = ", round(auc.lda, digits = 3),")")) +
theme_light()
roc.lda
# definimos como control una validación cruzada con 10 hojas y repeticiones
fit_control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 5,
## Estimar las probabilidades:
classProbs = TRUE,
## Evaluar rendimiento del modelo:
summaryFunction = twoClassSummary)
# LDA
set.seed(321)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control,
## Especificamos la métrica para optimizar:
metric = "ROC")
# QDA
set.seed(321)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control,
## Especificamos la métrica para optimizar:
metric = "ROC")
# RDA
mi.grid <- data.frame(lambda = c(0) ,
gamma = c(0))
set.seed(321)
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneGrid = mi.grid,
trControl = fit_control,
## Especificamos la métrica para optimizar:
metric = "ROC")
model_knn_def <- train(default ~.,
data = train_df,
method = "knn",
trControl = fit_control,
preProcess = c("center", "scale"),
tuneLength = 5,
## Especificamos la métrica para optimizar:
metric = "ROC")
resamps <- resamples(list(LDA = model_lda_def,
QDA = model_qda_def,
RDA = model_rda_def,
KNN = model_knn_def))
resamps
summary(resamps)
# box plots
bwplot(resamps, metric = "Accuracy")
# box plots
bwplot(resamps, metric = "ROC")
difValues <- diff(resamps)
difValues
summary(difValues)
# intervalos de confianza para las diferencias
dotplot(difValues)
model_knn_def
unlink('index_cache', recursive = TRUE)
setwd("~/OneDrive - Universidad Carlos III de Madrid/Big_Analytics_Ed4/14_15_Aprendizaje_Supervisado_Harold_A_Hdez_Roig/bookdown-Aprendizaje-Supervisado")
library(caret)
library(ggplot2)
library(readr)
library(dplyr)
library(gridExtra)
library(ROCR)
## Cargar datos ----
winsc <- read_csv("data_breast_cancer_Winsconsin.csv")
## Cargar datos ----
winsc <- read_csv("data_breast_cancer_Winsconsin.csv")
View(winsc)
# entender los datos
summary(winsc)
df <- as.data.frame(winsc)
## *** LDA -----
set.seed(123)
train.ID <- createDataPartition(df$diagnosis, p = 0.8, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# damos nuestros primeros pasos en la validacion cruzada...
# (podemos probar varias opciones)
# en este caso no hay parametros para tunear
fit_control <- trainControl(method='cv', number = 10)
model_lda_def <- train(diagnosis ~.,
data = train_df,
method = "lda",
trControl = fit_control)
# la respuesta es diagnosis: B = benign, M = malignant
winsc$diagnosis <- as.factor(winsc$diagnosis)
library(caret)
library(ggplot2)
library(readr)
library(dplyr)
library(gridExtra)
library(ROCR)
## Cargar datos ----
winsc <- read_csv("data_breast_cancer_Winsconsin.csv")
# no nos interesan los ID, y la última columna no se ha cargado bien
winsc <- winsc[, 2:32]
# la respuesta es diagnosis: B = benign, M = malignant
winsc$diagnosis <- as.factor(winsc$diagnosis)
library(caret)
library(ggplot2)
library(readr)
library(dplyr)
library(gridExtra)
library(ROCR)
## Cargar datos ----
wiscon <- read_csv("data_breast_cancer_wisconsin.csv")
# no nos interesan los ID, y la última columna no se ha cargado bien
wiscon <- wiscon[, 2:32]
# la respuesta es diagnosis: B = benign, M = malignant
wiscon$diagnosis <- as.factor(wiscon$diagnosis)
str(wiscon)
library(caret)
library(ggplot2)
library(readr)
library(dplyr)
library(gridExtra)
library(ROCR)
## Cargar datos ----
wiscon <- read_csv("data_breast_cancer_wisconsin.csv")
# no nos interesan los ID, y la última columna no se ha cargado bien
wiscon <- wiscon[, 2:32]
# la respuesta es diagnosis: B = benign, M = malignant
wiscon$diagnosis <- as.factor(wiscon$diagnosis)
str(wiscon)
library(caret)
library(ggplot2)
library(readr)
library(dplyr)
library(gridExtra)
library(ROCR)
## Cargar datos ----
wiscon <- read_csv("data_breast_cancer_wisconsin.csv")
# no nos interesan los ID, y la última columna no se ha cargado bien
wiscon <- wiscon[, 2:32]
# la respuesta es diagnosis: B = benign, M = malignant
wiscon$diagnosis <- as.factor(wiscon$diagnosis)
str(wiscon)
library(caret)
library(ggplot2)
library(readr)
library(dplyr)
library(gridExtra)
library(ROCR)
## Cargar datos ----
wiscon <- read_csv("data_breast_cancer_wisconsin.csv")
# no nos interesan los ID, y la última columna no se ha cargado bien
wiscon <- wiscon[, 2:32]
# la respuesta es diagnosis: B = benign, M = malignant
wiscon$diagnosis <- as.factor(wiscon$diagnosis)
df <- as.data.frame(wiscon)
str(df)
set.seed(666)
train.ID <- createDataPartition(df$diagnosis, p = 0.7, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# en este caso estamos reduciendo la cantidad de variables iniciales
# a solamente ¡2!
preProc.res <- preProcess(df, method = c('pca'), pcaComp = 2)
df.pca <- predict(preProc.res, df)
head(df.pca, 7)
ggplot(df.pca,  aes(x = PC1, y = PC2, group = diagnosis)) +
geom_point(aes(color = diagnosis ), alpha = 0.8) +
theme_light()
ggplot(df.pca,  aes(x = PC1, y = PC2, group = diagnosis)) +
geom_point(aes(color = diagnosis ), alpha = 0.8) +
theme_light()
# Ajustemos nuestros modelos con los datos transformados:
train_df <- df.pca[train.ID, ]
test_df <- df.pca[-train.ID, ]
unlink('index_cache', recursive = TRUE)
# automatically create a bib database for R packages
knitr::write_bib(c(
.packages(), 'class', 'caret', 'ISLR', 'ROCR'
), 'packages.bib')
knitr::opts_chunk$set(cache = T)
unlink('index_cache', recursive = TRUE)
install.packages("ISLR")
library(caret)
library(ISLR)
data("Default")
head(Default, 10)
str(Default)
summary(Default)
# ver el balance de la muestra
prop.table(table(Default$default))
library(ggplot2)
library(gridExtra)
## Scatter plot con densidades ----
plot.2d <- ggplot(Default, aes(x = balance, y = income, group = default)) +
geom_point(aes(shape = default, color = default), alpha = 0.5) +
theme_light()
# Empty plot
empty <- ggplot()+geom_point(aes(1,1), color="white") +
theme(
plot.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_blank(),
axis.text.y = element_blank(),
axis.ticks = element_blank()
)
# arriba
dens.balance <- ggplot(Default, aes(x = balance, group = default)) +
geom_density(aes(color = default, fill = default), alpha = 0.2) +
theme_light()+
theme(legend.position = "none")
# derecha
dens.income <- ggplot(Default, aes(x = income, group = default)) +
geom_density(aes(color = default, fill = default), alpha = 0.2) +
theme_light() + coord_flip() +
theme(legend.position = "none")
grid.arrange(dens.balance, empty, plot.2d, dens.income, ncol=2, nrow=2, widths=c(4, 1), heights=c(1, 4))
df <- Default[, c("income", "balance", "default")]
set.seed(123)
train.ID <- createDataPartition(df$default, p = 0.8, list = FALSE)
train_df <- df[train.ID, ]
test_df <- df[-train.ID, ]
# definimos como control una validación cruzada con 10 hojas, sin repeticiones
fit_control <- trainControl(method='cv', number = 10)
set.seed(123)
model_lda_def <- train(default ~.,
data = train_df,
method = "lda",
trControl = fit_control)
model_lda_def
model_lda_def$finalModel
varImp(model_lda_def)
# hagamos las predicciones del conjunto de prueba
prediction_lda_def <- predict(model_lda_def, newdata = test_df)
confusionMatrix(prediction_lda_def, reference = test_df$default)
# extraemos el Accuracy o Precisión
confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.lda <- 1-confusionMatrix(prediction_lda_def, reference = test_df$default)$overall[1]
names(tasa.error.lda) <- "Error LDA"
tasa.error.lda
decision_bound = function(train_df_in, test_df_in, model_in){
# plot decision boundary  for df <- Default[, c("income", "balance", "default")]
require(MASS)
require(caret)
require(ggplot2)
require(gridExtra)
# Paso 1: crear un grid de valores desde min a max de ambos predictores
pl = seq(min(train_df_in$balance), max(train_df_in$balance), length.out = 80)
pw = seq(min(train_df_in$income), max(train_df_in$income), length.out = 80)
lgrid <- expand.grid(balance=pl, income=pw)
# Paso 2: obtener las predicciones tanto para el grid como para el test
modelPredGrid <- predict(model_in, newdata=lgrid)
train_df_in$Pred.Class <- predict(model_in, newdata = train_df_in)
test_df_in$Pred.Class <- predict(model_in, newdata = test_df_in)
# Paso 3: ggplot con la funcion contour
gg1 <- ggplot(data=lgrid) +
stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) +
geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) +
labs(colour = "Clases") + ggtitle("Train") +
geom_point(data=train_df_in,
aes(x=balance, y=income,
colour=default), size=5, shape=1) +
theme_light()
gg2 <- ggplot(data=lgrid) +
stat_contour(aes(x=balance, y=income, z=as.numeric(modelPredGrid)), bins=2) +
geom_point(aes(x=balance, y=income, colour=modelPredGrid), alpha=0.1) +
labs(colour = "Clases") + ggtitle("Test") +
geom_point(data=test_df_in,
aes(x=balance, y=income,
colour=default), size=5, shape=1) +
theme_light()
grid.arrange(gg1, gg2, ncol=1, nrow=2)
}
decision_bound(train_df, test_df, model_lda_def)
set.seed(123)
model_qda_def <- train(default ~.,
data = train_df,
method = "qda",
trControl = fit_control)
model_qda_def
model_qda_def$finalModel
# hagamos las predicciones del conjunto de prueba
prediction_qda_def <- predict(model_qda_def, newdata = test_df)
confusionMatrix(prediction_qda_def, reference = test_df$default)
# extraemos el Accuracy o Precisión
confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
# la tasa de error
tasa.error.qda <- 1-confusionMatrix(prediction_qda_def, reference = test_df$default)$overall[1]
names(tasa.error.qda) <- "Error QDA"
tasa.error.qda
decision_bound(train_df, test_df, model_qda_def)
set.seed(123)
model_rda_def <- train(default ~.,
data = train_df,
method = "rda",
tuneLength = 2,
trControl = fit_control)
model_rda_def
model_rda_def$finalModel
